{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from konlpy.tag import Mecab\n",
    "import MeCab\n",
    "import re\n",
    "\n",
    "mecab = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대한 정제를 많이 해주는 게 성능을 높일 수 있다.\n",
    "\n",
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 함수.\n",
    "    doc = re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', doc) #띄어쓰기도 꼭 넣어준다\n",
    "    # [^ ] : ~를 제외하고 ^[ ] : ~로 시작하고\n",
    "\n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set() # 겹칠까봐 set으로\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # SW.add('있다')\n",
    "    \n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가하기 \n",
    "    # 원래는 with open(path) as f: 였는데\n",
    "    # 에러가 나므로 with open(path, 'r', encoding='utf-8') as f: 로 바꿔준다.\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f: #path로 파일 경로가 들어온다. with open : 파일을 자동으로 불러준다.\n",
    "        for word in f: # 라인하나당 워드 하나\n",
    "            SW.add(word) # 추가\n",
    "            \n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(doc):\n",
    "    \n",
    "    # list comprehension을 풀어서 쓴 코드.\n",
    "    # tokenized_doc =[]\n",
    "\n",
    "    # for word in mecab.morphs(doc):\n",
    "    #     if word not in SW and len(word) > 1:\n",
    "    #         tokenized_doc.append(word)\n",
    "    \n",
    "    # return tokenized_doc\n",
    "    \n",
    "    return [word for word in mecab_morphs(doc) if word not in SW and len(word) > 1] \n",
    "    # 리스트의 원소가 word. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
