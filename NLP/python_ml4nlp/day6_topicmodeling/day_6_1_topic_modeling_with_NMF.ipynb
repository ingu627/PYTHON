{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 만들어 둔, 문서 - 단어 행렬을 이용하여 NMF 를 이용한 토픽모델링을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.49\n",
      "added lovit_textmining_dataset\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from lovit_textmining_dataset.navernews_10days import get_bow\n",
    "\n",
    "x, idx_to_vocab, vocab_to_idx = get_bow(date='2016-10-20', tokenize='noun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300,91 개의 문서가 9,774 개의 단어로 표현되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30091, 9774)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용법은 sklearn.decomposition.TruncatedSVD 와 같습니다.\n",
    "\n",
    "하지만 학습 시간이 조금 긴 편입니다. SVD 가 16.2 초 만에 학습되었지만, NMF 는 5 분의 학습 시간이 필요합니다. 학습이 오래 걸리니 한 번 학습한 뒤, 파일을 저장해둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 1.29 s, total: 1.32 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pickle\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    nmf = NMF(n_components=100)\n",
    "    y = nmf.fit_transform(x)\n",
    "\n",
    "    with open('./2016-10-20-nmf.pkl', 'wb') as f:\n",
    "        pickle.dump(nmf, f)\n",
    "    with open('./2016-10-20-nmf_y.pkl', 'wb') as f:\n",
    "        pickle.dump(y, f)\n",
    "\n",
    "else:\n",
    "    # load trained model\n",
    "    with open('./2016-10-20-nmf.pkl', 'rb') as f:\n",
    "        nmf = pickle.load(f)\n",
    "    with open('./2016-10-20-nmf_y.pkl', 'rb') as f:\n",
    "        y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF 가 차원을 축소한 대상은 Bag of Words 로 표현된 문서벡터 입니다. 9,774 차원으로 표현되는 문서가 100 차원으로 표현됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30091, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9,774 개의 단어 역시 100 차원으로 표현됩니다. SVD 처럼 components_ 에 정보가 저장되어 있습니다.\n",
    "\n",
    "이를 반대로 해석하면 100 개의 components 들이 각각 9,774 개의 term weight vector 로 표현된 것과 같습니다. 30091 개의 문서를 표현할 수 있는 100 개의 components 입니다. 이는 마치 topic vector 의 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9774)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "components vector 를 topic_vector 로 복사해둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_vector = nmf.components_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 component 에 대하여 weight 의 크기가 큰 순서대로 component keyword 를 선택할 수 있습니다. 일종의 labeling 입니다.\n",
    "\n",
    "71 번 component 는 아이돌 방송 관련 토픽들 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('불독', 16.011034310171343),\n",
       " ('데뷔', 7.627838309186585),\n",
       " ('걸그룹', 6.566420812556478),\n",
       " ('쇼케이스', 4.602740865694784),\n",
       " ('키미', 4.3269350387987675),\n",
       " ('형은', 4.119217675175327),\n",
       " ('무대', 3.9953000852499865),\n",
       " ('소라', 3.451016450726792),\n",
       " ('롤링', 3.2674144688507427),\n",
       " ('세이', 3.146422564026698)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_important_terms(topic_idx, topn=10):\n",
    "    \n",
    "    # sort by weight in decreasing order\n",
    "    term_idxs = topic_vector[topic_idx,:].argsort()[::-1]\n",
    "\n",
    "    # select top n terms\n",
    "    if topn > 0:\n",
    "        term_idxs = term_idxs[:topn]\n",
    "\n",
    "    # form of [(idx, weight), ... ]\n",
    "    weights = topic_vector[topic_idx, term_idxs]\n",
    "    term_and_weights = [(t,w) for t,w in zip(term_idxs, weights)]\n",
    "\n",
    "    # decode\n",
    "    term_and_weights = [(idx_to_vocab[t], w) for t,w in term_and_weights]\n",
    "\n",
    "    return term_and_weights\n",
    "\n",
    "most_important_terms(71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 번 components 는 미국 대선 관련 성분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('트럼프', 24.060825373206004),\n",
       " ('클린턴', 16.330708437344487),\n",
       " ('토론', 10.376507974552318),\n",
       " ('대선', 7.041041755201666),\n",
       " ('후보', 5.324686093570161),\n",
       " ('힐러리', 4.617175691684361),\n",
       " ('주장', 4.14022124957622),\n",
       " ('공화당', 3.4868372768982976),\n",
       " ('3차', 3.4362118631731096),\n",
       " ('선거', 2.988377886302627)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_important_terms(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 단어에 대하여 weight 가 큰 components 를 찾을 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_relavant_topics(term, n_topics=5, n_terms=20):\n",
    "    term_idx = vocab_to_idx.get(term, -1)\n",
    "    if term_idx < 0:\n",
    "        return []\n",
    "\n",
    "    # slice topic weight vector\n",
    "    relavant_topics = topic_vector[:,term_idx].argsort()[::-1]\n",
    "\n",
    "    # select top topics\n",
    "    if n_topics > 0:\n",
    "        relavant_topics = relavant_topics[:n_topics]\n",
    "\n",
    "    # select important terms for each topic (component)\n",
    "    topic_terms = []\n",
    "    for topic_idx in relavant_topics:\n",
    "        # idx, score, terms\n",
    "        topic_terms.append(\n",
    "            (topic_idx,\n",
    "             topic_vector[topic_idx, term_idx],\n",
    "             most_important_terms(topic_idx, n_terms))\n",
    "        )\n",
    "    \n",
    "    return topic_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 빈도수를 확인하기 위하여 term frequency matrix 에 row sum 을 하여 frequency vector 를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9774,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "term_frequency = np.asarray(x.sum(axis=0)).reshape(-1)\n",
    "term_frequency.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`아이오아이` 라는 단어의 weight 가 큰 components 들의 top weighted words 입니다. `아이오아이` 라는 단어와 관련된 토픽 (components) 은 크게 3 개 정도로 학습되었지만, 해당 component 들에서의 weight 는 크지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query term = 아이오아이, frequency = 270\n",
      "\n",
      "component#71 (1.406): 불독 데뷔 걸그룹 쇼케이스 키미 형은 무대 소라 롤링 세이 마포구 출연 매력 오전 멤버들 101 싱글 프로듀스 강렬 표현\n",
      "component#11 (0.9662): 공개 모습 드라마 촬영 배우 방송 캐릭터 연기 도깨비 공유 출연 선보 매력 화보 한편 현장 연출 변신 기대 특히\n",
      "component#79 (0.2516): 트와이스 1위 기록 스트리밍 방탄소년단 차트 누적 발표 뮤직비디오 올해 가온차트 2016년 데뷔 유튜브 최고 공개 미니앨범 조회수 20일 차지\n",
      "component#78 (0.04374): 신화 앨범 발매 13집 팬들 정규 컴백 공개 활동 11월 이번 그룹 콘서트 데뷔 예정 멤버들 곡들 19년 기대 29일\n",
      "component#34 (0.02382): 사진 이미지 역사 가치 인스타그램 글과 제공 자료 제주 예술 마이데일리 기사 모습 의미 카메라 평가 가운데 자신 대상 일어\n"
     ]
    }
   ],
   "source": [
    "query = '아이오아이'\n",
    "query_idx = vocab_to_idx[query]\n",
    "print('query term = {}, frequency = {}\\n'.format(query, term_frequency[query_idx]))\n",
    "\n",
    "relevant_topics = most_relavant_topics(query)\n",
    "\n",
    "for topic_id, relavant_score, important_terms in relevant_topics:\n",
    "    terms = ' '.join([term for term, _ in important_terms])\n",
    "    print('component#{} ({:.4}): {}'.format(topic_id, relavant_score, terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`트와이스` 가 해당 시기에 더 자주 등장한 단어였기 때문에 weight 가 훨씬 크게 학습되었습니다. component#79 는 아이돌 관련 component 임을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query term = 트와이스, frequency = 655\n",
      "\n",
      "component#79 (5.795): 트와이스 1위 기록 스트리밍 방탄소년단 차트 누적 발표 뮤직비디오 올해 가온차트 2016년 데뷔 유튜브 최고 공개 미니앨범 조회수 20일 차지\n",
      "component#19 (0.1202): 네이버 의장 서비스 이사회 유럽 한성숙 이해진 글로벌 부사장 3월 내년 대표이사 김상헌 대표 라인 내정자 의장직 업계 내정 인터넷\n",
      "component#13 (0.01512): 차태현 김유정 영화 박보검 서현진 연기 사랑 때문 성동일 김윤혜 박근형 구르미 달빛 주지홍 호흡 많이 출연 배우 드라마 기억\n",
      "component#99 (0.0): 모바일 삼성전자 10나노 기술 출시 제공 메모리 기존 용량 기기 글로벌 고객들 스마트폰 제품 세계 2배 공급 16 솔루션 교환\n",
      "component#25 (0.0): 디자이너 브랜드 마이데일리 진행 2017 동대문디자인플라자 기사 유진 헤라서울패션위크 참석 컬렉션 22일 실시간 보도자료 구매 뉴미디어 신진 제보 참가 6개\n"
     ]
    }
   ],
   "source": [
    "query = '트와이스'\n",
    "query_idx = vocab_to_idx[query]\n",
    "print('query term = {}, frequency = {}\\n'.format(query, term_frequency[query_idx]))\n",
    "\n",
    "relevant_topics = most_relavant_topics(query)\n",
    "\n",
    "for topic_id, relavant_score, important_terms in relevant_topics:\n",
    "    terms = ' '.join([term for term, _ in important_terms])\n",
    "    print('component#{} ({:.4}): {}'.format(topic_id, relavant_score, terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'트럼프'라는 단어의 weight 가 큰 components 들의 top weighted words 입니다. component#1 과 component#39 에 집중되어 있으며, term frequency 가 크기 때문에 weight 도 큽니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query term = 트럼프, frequency = 3540\n",
      "\n",
      "component#1 (24.06): 트럼프 클린턴 토론 대선 후보 힐러리 주장 공화당 3차 선거 민주당 도널드 대선후보 미국 결과 라스베이거스 이날 지지 발언 불복\n",
      "component#39 (10.0): 후보 지금 힐러리 말씀 있습니다 트럼프 이런 때문 제가 사실 클린턴 미국 생각합니다 생각 발언 경제 대통령 우리 국가 도널드\n",
      "component#57 (0.06952): 여성 남성 남성들 여성들 혐오 자신 생각 사회 임신 말하는 운동 표현 이상 남자 낙태 사건 차별 비율 여자 사용\n",
      "component#14 (0.04523): 중국 필리핀 두테르테 양국 남중국해 베이징 주석 정상회담 분쟁 협력 영유권 시진핑 방문 성장률 투자 갈등 수출 달러 경제 로드\n",
      "component#74 (0.04333): 한국 세계 국내 최초 국가 이번 우리나라 영국 아시아 주제 문화재 해외 프랑스 독일 선생 태국 기증 독립 처음 언어\n"
     ]
    }
   ],
   "source": [
    "query = '트럼프'\n",
    "query_idx = vocab_to_idx[query]\n",
    "print('query term = {}, frequency = {}\\n'.format(query, term_frequency[query_idx]))\n",
    "\n",
    "relevant_topics = most_relavant_topics(query)\n",
    "\n",
    "for topic_id, relavant_score, important_terms in relevant_topics:\n",
    "    terms = ' '.join([term for term, _ in important_terms])\n",
    "    print('component#{} ({:.4}): {}'.format(topic_id, relavant_score, terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NMF 는 SVD 와 활용하는 방법은 비슷합니다. component 를 학습하는 방식이 다릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
