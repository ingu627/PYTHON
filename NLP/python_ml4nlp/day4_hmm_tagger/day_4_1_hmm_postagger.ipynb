{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hidden Markov Model (HMM)\n",
    "\n",
    "Hidden Markov Model (HMM) 은 길이가 $n$ 인 sequence $x_{1:n} = [x_1, x_2, \\dots, x_n]$ 에 대하여 $P(y_{1:n} \\vert x_{1:n})$ 가 가장 큰 $y_{1:n}$ 를 찾습니다. 품사 판별의 문제에서는 $n$ 개의 단어로 구성된 단어열에 대하여 각 단어의 품사 $y$ 를 부여하는 것입니다. 이 과정을 HMM 의 decoding 이라 합니다. 그리고 우리가 찾아야 하는 label, $y$ 를 HMM 에서는 state 라 합니다.\n",
    "\n",
    "이 때 $P(y_{1:n} \\vert x_{1:n})$ 는 다음처럼 계산됩니다.\n",
    "\n",
    "$P(y_{1:n} \\vert x_{1:n}) = P(x_1 \\vert y_1) \\times P(y_1 \\vert BOS) \\times P(y_2 \\vert y_1) \\times P(x_2 \\vert y_2) \\cdots$\n",
    "\n",
    "위의 식은 현재 시점 $i$ 의 state 인 $y_i$ 를 판별 (classification, or labeling) 하기 위하여 이전 시점의 state 인 $y_{i-1}$ 이 이용됩니다. 이처럼 이전의 한 단계 전의 state 정보를 이용하는 모델을 first-order Markov Model 이라 합니다. 만약 이전 두 단계의 states 정보를 모두 이용한다면 $P(y_i \\vert y_{i-2}, y_{i-1})$ 이 학습되어야 하며, 이는 second-order Markov Model 이라 합니다. 그 외의 멀리 떨어진 state 정보는 이용하지 않습니다.\n",
    "\n",
    "이처럼 state 간의 변화 확률을 transition probability (전이 확률) 라 합니다. HMM 은 각 state 에서 우리가 관측 가능한 값 (observation) 이 발생할 확률이 있다고 가정합니다. 이를 emission probability, $P(x_i \\vert y_i)$ 라 합니다. 품사 판별에서는 명사 집합에서 '아이오아이'라는 단어가 존재할 확률 입니다.\n",
    "\n",
    "숫자 계산에서 곱셈은 덧셈보다 비싼 작업입니다. 그렇기 때문에 확률을 곱하는 작업들은 주로 log 를 씌워 덧셈으로 변환합니다. 위 수식은 아래처럼 변환됩니다.\n",
    "\n",
    "$log P(y_{1:n} \\vert x_{1:n}) = log P(x_1 \\vert y_1)+ log P(y_1 \\vert S) + log P(y_2 \\vert y_1) + log P(x_2 \\vert y_2) \\cdots$\n",
    "\n",
    "$log P(x_i \\vert y_i)$ 나 $log P(y_i \\vert y_{i-1})$ 은 마치 score($x_i, y_i$), score($y_i, y_{i-1}$) 형태와 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Supervised training\n",
    "\n",
    "학습 말뭉치를 이용하여 품사 판별이나 형태소 분석용 tagger 를 학습하기도 합니다. 학습 말뭉치는 다음과 같은 데이터로 구성되어 있습니다.\n",
    "\n",
    "아래는 세종 말뭉치의 예시입니다. 세종 말뭉치는 한국어 형태소 분석을 위하여 국립국어원에서 배포한 말뭉치입니다. 한 문장이 (morpheme, tag) 형식으로 기술되어 있습니다. 아래는 네 개의 문장의 스냅샷입니다.\n",
    "\n",
    "    [['뭐', 'NP'], ['타', 'VV'], ['고', 'EC'], ['가', 'VV'], ['ㅏ', 'EF']]\n",
    "    [['지하철', 'NNG']]\n",
    "    [['기차', 'NNG']]\n",
    "    [['아침', 'NNG'], ['에', 'JKB'], ['몇', 'MM'], ['시', 'NNB'], ['에', 'JKB'], ['타', 'VV'], ['고', 'EC'], ['가', 'VV'], ['는데', 'EF']]\n",
    "\n",
    "우리는 first-order HMM tagger 를 만들어봅니다. 이는 state 관점에서 bigram 을 이용하기 때문에 bigram HMM tagger 라 불려도 괜찮습니다.\n",
    "\n",
    "bigram HMM tagger 를 학습하려면 transition probability $P(y_i \\vert y_{i-1})$ 와 emittion probability $P(x_i \\vert y_i)$ 를 학습해야 합니다. 직관적으로 다음처럼 확률을 정의할 수 있습니다.\n",
    "\n",
    "- transition prob: $P(y_i \\vert y_{i-1}) = \\frac{f(y_{i-1}, y_i)}{f(y_{i-1})}$\n",
    "- emittion prob:   $P(x_i \\vert y_i) = \\frac{f(x_i, y_i)}{f(y_i)}$\n",
    "\n",
    "그런데 위의 식이 직관적일 뿐 아니라 Maximum Likelihood Estimation (MLE) 관점에서도 확률을 학습하는 solution 입니다. 즉, 학습 말뭉치에서의 빈도수를 계산하는 것만으로 학습을 할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "corpus = [\n",
    "    [('이것', 'Noun'), ('은', 'Josa'), ('예시', 'Noun'), ('이', 'Adjective'), ('었다', 'Eomi')]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 corpus 가 nested list 구조라 가정합니다. 각 문장 sent 는 [(word, tag), (word, tag), ... ] 형식입니다. 우리가 해야 할 일은 각 태그 별로 단어가 발생한 횟수와 $tag_{i-1}, tag_{i}$ 의 횟수를 세는 것 뿐입니다.\n",
    "\n",
    "문장의 시작을 BOS 로, 끝을 EOS 로 구분하기 위하여 tag sequence 에 BOS 와 EOS 를 추가하여 bigram counting 을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOS = 'BOS'\n",
    "EOS = 'EOS'\n",
    "\n",
    "def count(corpus):\n",
    "    emissions = defaultdict(lambda: defaultdict(int))\n",
    "    transitions = defaultdict(int)\n",
    "\n",
    "    for sent in corpus:\n",
    "        # emission prob\n",
    "        for word, pos in sent:\n",
    "            emissions[pos][word] += 1\n",
    "\n",
    "        words, tags = zip(*sent)\n",
    "        tags = [BOS] + list(tags) + [EOS]\n",
    "\n",
    "        # transition prob\n",
    "        for t0, t1 in zip(tags, tags[1:]):\n",
    "            transitions[(t0, t1)] += 1\n",
    "\n",
    "    return {k:dict(v) for k,v in emissions.items()}, dict(transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 계산한 것은 빈도수 입니다. 이를 확률로 변형합니다. Emissions probability 는 각 품사의 빈도수로 (단어, 품사) 발생 횟수를 나눠주면 됩니다. Transitions probability 역시 이전 품사의 빈도수인 base[tag0] 으로 (tag0, tag1) 의 빈도수를 나눠주면 됩니다.\n",
    "\n",
    "그리고 float 는 곱셈보다 덧셈이 계산이 빠르기 때문에, 미리 math.log 를 이용하여 확률값을 log probability 로 변환하여 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def _to_log_prob(emissions, transitions):\n",
    "\n",
    "    # emission\n",
    "    base = {tag:sum(words.values()) for tag, words in emissions.items()}\n",
    "    emissions_ = {tag:{word:math.log(count/base[tag]) for word, count in words.items()}\n",
    "                  for tag, words in emissions.items()}\n",
    "\n",
    "    # transition\n",
    "    base = defaultdict(int)\n",
    "    for (tag0, tag1), count in transitions.items():\n",
    "        base[tag0] += count\n",
    "    transitions_ = {tags:math.log(count/base[tags[0]]) for tags, count in transitions.items()}\n",
    "\n",
    "    return emissions_, transitions_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential labeling 을 위한 HMM model 의 supervised learning 은 모두 끝났습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HMM 기반 형태소 분석기 만들기\n",
    "\n",
    "일단은 클래스로 정리하지 않고, 기능별로 함수를 만들어봅니다.\n",
    "\n",
    "이를 위해 다음의 데이터를 이용합니다. max word len, min emission, min transition 은 모델이 이용할 parameters 입니다. 미등록 단어에 대한 emission score 와 transition score 입니다. 또한 학습된 사전에서 어느 길이까지 단어를 찾아볼지 정의하기 위하여, 사전에 등록된 단어의 길이 중 가장 긴 길이를 max word len 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    [('이것', 'Noun'), ('은', 'Josa'), ('예시', 'Noun'), ('이', 'Adjective'), ('다', 'Eomi')],\n",
    "    [('이것', 'Noun'), ('도', 'Josa'), ('예시', 'Noun'), ('이', 'Adjective'), ('었다', 'Eomi')],\n",
    "    [('아이오아이', 'Noun'), ('를', 'Josa'), ('예시', 'Noun'), ('로', 'Josa'), ('이용', 'Noun'), ('하', 'Verb'), ('ㄴ다', 'Eomi')],\n",
    "    [('공연', 'Noun'), ('이', 'Josa'), ('시작', 'Noun'), ('하', 'Verb'), ('ㄴ다', 'Eomi')],\n",
    "    [('이것', 'Noun'), ('은', 'Josa'), ('아이', 'Noun'), ('의', 'Josa'), ('예시', 'Noun'), ('이', 'Adjective'), ('다', 'Eomi')],\n",
    "    [('이', 'Noun'), ('는', 'Josa'), ('숫자', 'Noun'), ('이', 'Adjective'), ('다', 'Eomi')],\n",
    "    [('예', 'Noun'), ('도', 'Josa'), ('단어', 'Noun'), ('이', 'Adjective'), ('다', 'Eomi')],\n",
    "]\n",
    "\n",
    "emissions_freq, transitions_freq = count(corpus)\n",
    "emissions, transitions = _to_log_prob(emissions_freq, transitions_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "-2.822588722239781\n",
      "-2.1294415416798356\n"
     ]
    }
   ],
   "source": [
    "max_word_len = max(len(w) for words in emissions.values() for w in words)\n",
    "min_emission = min(s for words in emissions.values() for s in words.values()) - 0.05\n",
    "min_transition = min(transitions.values()) - 0.05\n",
    "\n",
    "print(max_word_len)\n",
    "print(min_emission)\n",
    "print(min_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Lookup\n",
    "\n",
    "앞서 만든 Lemmatizer 를 가져옵니다. 동사, 형용사, 어미 사전은 학습 말뭉치에 있는 단어를 이용하고, lemma_rules 는 실습을 위한 demo set 을 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('하', 'Verb'), ('ㄴ다', 'Eomi'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./korean_lemmatizer/')\n",
    "from soylemma import Lemmatizer\n",
    "\n",
    "rules = {\n",
    "    '한': {('하', 'ㄴ')},\n",
    "    '였': {('이', '었')}\n",
    "}\n",
    "\n",
    "lemmatizer = Lemmatizer(\n",
    "    verbs = emissions.get('Verb', {}),\n",
    "    adjectives = emissions.get('Adjective', {}),\n",
    "    eomis = emissions.get('Eomi', {}),\n",
    "    lemma_rules = rules\n",
    ")\n",
    "\n",
    "lemmatizer.analyze('한다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'하'}\n",
      "{'이'}\n",
      "{'다', '었다', 'ㄴ다'}\n",
      "{'한': {('하', 'ㄴ')}, '였': {('이', '었')}}\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.verbs)\n",
    "print(lemmatizer.adjectives)\n",
    "print(lemmatizer.eomis)\n",
    "print(lemmatizer.lemma_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장이 입력되면 띄어쓰기 기준으로 이를 분리합니다. 어절 단위로 우리가 알고 있는 단어들이 존재하는지 확인합니다. offset 은 현재까지 탐색한 문장에서의 글자수 입니다. 문장에서의 다음 어절의 시작점을 알려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_lookup(sentence, emissions, lemmatizer, max_word_len):\n",
    "    sent = []\n",
    "    for eojeol in sentence.split():\n",
    "        sent += eojeol_lookup(\n",
    "            eojeol,\n",
    "            emissions,\n",
    "            lemmatizer,\n",
    "            max_word_len,\n",
    "            offset = len(sent)\n",
    "        )\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 (morph, tag, morph, tag, begin, end) 형식으로 저장합니다. 이는 용언처럼 형태가 변하는 (활용, conjugation) 단어의 정보를 손쉽게 저장하기 위함입니다. 이를 namedtuple 로 만들어 가독성을 좋게 합니다.\n",
    "\n",
    "또한 용언이 아니라 하더라도 tag1 에 tag0 을 함께 입력합니다. 이는 이후에 그래프를 만드는데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word(시작/Noun, 1, 3)\n",
      "Word(하/Verb + ㄴ다/Eomi, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "class Word(namedtuple('Word', 'morph0 tag0 morph1 tag1 begin end is_compound')):    \n",
    "    def __repr__(self):\n",
    "        if not self.is_compound:\n",
    "            return 'Word({}/{}, {}, {})'.format(\n",
    "                self.morph0, self.tag0, self.begin, self.end)\n",
    "        else:\n",
    "            return 'Word({}/{} + {}/{}, {}, {})'.format(\n",
    "                self.morph0, self.tag0, self.morph1, self.tag1, self.begin, self.end)\n",
    "\n",
    "word0 = Word('시작', 'Noun', None, 'Noun', 1, 3, False)\n",
    "word1 = Word('하', 'Verb', 'ㄴ다', 'Eomi', 3, 5, True)\n",
    "\n",
    "print(word0)\n",
    "print(word1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어절에서의 단어 시작점 b 로부터 사전에 등록된 단어의 최대 길이 max_word_len 까지 확장하며 substring 을 surface 에 잘라둡니다. 이 substring 이 사전에 존재하는지 get_pos 함수를 이용하여 확인합니다. 품사가 존재한다면 이는 우리가 학습한 단어라는 의미입니다.\n",
    "\n",
    "get_pos 함수에서 동사, 형용사, 어미는 string match 를 하지 않습니다.  이들은 lemmatizer 가 확인할 것이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eojeol_lookup(eojeol, emissions, lemmatizer, max_word_len, offset=0):\n",
    "    n = len(eojeol)\n",
    "    pos = [[] for _ in range(n)]\n",
    "    for b in range(n):\n",
    "        for r in range(1, max_word_len+1):\n",
    "            e = b+r\n",
    "            if e > n:\n",
    "                continue\n",
    "            sub = eojeol[b:e]\n",
    "\n",
    "            # string match\n",
    "            for tag in get_pos(sub, emissions, max_word_len):\n",
    "                pos[b].append(Word(sub, tag, None, tag, b + offset, e + offset, False))\n",
    "\n",
    "            # lemmatizer\n",
    "            for (w0, t0), (w1, t1) in lemmatizer.analyze(sub):\n",
    "                pos[b].append(Word(w0, t0, w1, t1, b + offset, e + offset, True))\n",
    "    return pos\n",
    "\n",
    "def get_pos(sub, emissions, max_word_len):\n",
    "    tags = []\n",
    "    for tag, words in emissions.items():\n",
    "        if tag == 'Verb' or tag == 'Adjective' or tag == 'Eomi':\n",
    "            continue\n",
    "        if sub in words:\n",
    "            tags.append(tag)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Word(시작/Noun, 0, 2)], [], [Word(하/Verb + ㄴ다/Eomi, 2, 4)], []]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eojeol_lookup('시작한다', emissions, lemmatizer, max_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eojeol_lookup 함수를 이용하면 어절의 길이만큼의 nested list 가 return 됩니다. list 의 각 위치는 어절에서의 글자의 시작지점입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Word(예/Noun, 0, 1), Word(예시/Noun, 0, 2)],\n",
       " [],\n",
       " [Word(이/Noun, 2, 3), Word(이/Josa, 2, 3), Word(이/Adjective + 다/Eomi, 2, 4)],\n",
       " []]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eojeol_lookup('예시이다', emissions, lemmatizer, max_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장에 대한 lookup 을 수행한 결과 입니다. '예시'는 문장에서 네번째로 등장하는 단어이기 때문에 begin index 가 3 임을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Word(이/Noun, 0, 1), Word(이/Josa, 0, 1), Word(이것/Noun, 0, 2)],\n",
       " [],\n",
       " [Word(은/Josa, 2, 3)],\n",
       " [Word(예/Noun, 3, 4), Word(예시/Noun, 3, 5)],\n",
       " [],\n",
       " [Word(이/Noun, 5, 6), Word(이/Josa, 5, 6), Word(이/Adjective + 다/Eomi, 5, 7)],\n",
       " []]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lookup('이것은 예시이다', emissions, lemmatizer, max_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 그래프 만들기\n",
    "\n",
    "앞서 어절과 문장에서 알려진 단어를 찾는 과정을 구현하였습니다. 이 단어들 중에서 앞의 단어의 end index 와 뒤의 단어의 begin index 가 같은 경우, 이 둘을 연결할 수 있습니다.\n",
    "\n",
    "이는 다음의 과정으로 구현할 수 있습니다.\n",
    "\n",
    "    links = []\n",
    "    for words in sent[:-1]:\n",
    "        for word in words:\n",
    "            for adjacent in sent[word.end]:\n",
    "                links.append((word, adjacent))\n",
    "\n",
    "그러나 한 단어의 end index 에서부터 시작하는 단어가 없을 경우에는 링크가 만들어지지 않습니다. 이 때에는 가능한 가장 가까운 다음 단어의 begin index 를 찾아야 합니다. 이를 위하여 다음의 함수를 구현합니다. offset 이후의 지점에서 sent[i] 가 empty 가 아닌 가장 빠른 지점을 return 합니다.\n",
    "\n",
    "    def get_nonempty_first(sent, end, offset=0):\n",
    "        for i in range(offset, end):\n",
    "            if sent[i]:\n",
    "                return i\n",
    "        return offset\n",
    "\n",
    "만약 문장의 끝부분까지 아는 단어가 존재하지 않을수도 있습니다. 이를 방지하기 위해 문장의 끝을 표시하는 eos 를 sent 에 추가합니다.\n",
    "\n",
    "    sent = sentence_lookup(sentence)\n",
    "    n_char = len(sent) + 1\n",
    "\n",
    "    eos = Word('EOS', 'EOS', 'EOS', 'EOS', n_char-1, n_char, False)\n",
    "    sent.append([eos])\n",
    "\n",
    "그리고 end index 로부터 시작하는 단어가 없을 경우, 가장 가까운 단어의 시작지점까지 부분어절을 잘라 'Unk' 태그를 부여합니다. \n",
    "\n",
    "    links = []\n",
    "    for words in sent[:-1]:\n",
    "        for word in words:\n",
    "            if not sent[word.end]:\n",
    "                b = get_nonempty_first(sent, n_char, word.end)\n",
    "                unk = Word(chars[end:b], 'Unk', 'Unk', word.end, b)\n",
    "                links.append((word, unk))\n",
    "            for adjacent in sent[word.end]:\n",
    "                links.append((word, adjacent))\n",
    "\n",
    "하지만 모르는 단어로부터 아는 단어까지의 링크가 아직 만들어지지 않았습니다. 품사가 'Unk' 인 단어들을 찾아 그 단어의 end index 로부터 시작하는 단어들과 링크를 만듭니다.\n",
    "\n",
    "    unks = {to_node for _, to_node in links if to_node.tag0 == 'Unk'}\n",
    "    for unk in unks:\n",
    "        for adjacent in sent[unk.end]:\n",
    "            links.append((unk, adjacent))\n",
    "\n",
    "마지막으로 그래프의 시작점에 bos 를 추가합니다. 문장의 맨 앞에 있는 단어들과 bos 간에 링크를 추가합니다.\n",
    "\n",
    "    bos = Word('BOS', 'BOS', 'BOS', 'BOS', 0, 0, False)\n",
    "    for word in sent[0]:\n",
    "        links.append((bos, word))\n",
    "\n",
    "Shortest path 에서 bos 와 eos 는 시작 마디와 끝 마디로 이용됩니다. 이 값을 함께 return 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_link(sentence, emissions, lemmatizer, max_word_len):\n",
    "\n",
    "    def get_nonempty_first(sent, end, offset=0):\n",
    "        for i in range(offset, end):\n",
    "            if sent[i]:\n",
    "                return i\n",
    "        return offset\n",
    "\n",
    "    chars = sentence.replace(' ','')\n",
    "    sent = sentence_lookup(\n",
    "        sentence,\n",
    "        emissions,\n",
    "        lemmatizer,\n",
    "        max_word_len\n",
    "    )\n",
    "    n_char = len(sent) + 1\n",
    "\n",
    "    eos = Word('EOS', 'EOS', 'EOS', 'EOS', n_char-1, n_char, False)\n",
    "    sent.append([eos])\n",
    "\n",
    "    i = get_nonempty_first(sent, n_char)\n",
    "    if i > 0:\n",
    "        sent[0].append(Word(chars[:i], 'Unk', None, 'Unk', 0, i, False))\n",
    "\n",
    "    links = []\n",
    "    for words in sent[:-1]:\n",
    "        for word in words:\n",
    "            if not sent[word.end]:\n",
    "                b = get_nonempty_first(sent, n_char, word.end)\n",
    "                unk = Word(chars[word.end:b], 'Unk', None, 'Unk', word.end, b, False)\n",
    "                links.append((word, unk))\n",
    "            for adjacent in sent[word.end]:\n",
    "                links.append((word, adjacent))\n",
    "\n",
    "    unks = {to_node for _, to_node in links if to_node.tag0 == 'Unk'}\n",
    "    for unk in unks:\n",
    "        for adjacent in sent[unk.end]:\n",
    "            links.append((unk, adjacent))\n",
    "\n",
    "    bos = Word('BOS', 'BOS', 'BOS', 'BOS', 0, 0, False)\n",
    "    for word in sent[0]:\n",
    "        links.append((bos, word))\n",
    "    links = sorted(links, key=lambda x:(x[0].begin, x[1].end))\n",
    "\n",
    "    return links, bos, eos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수를 적용하여 만든 링크입니다. '것'이라는 단어는 알려지지 않았기 때문에 'Unk' 품사로 인식되었습니다. 또한 '였다'는 '이/Adjective + 었다/Eomi' 로 인식되어 하나의 마디를 이루고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Word(BOS/BOS, 0, 0), Word(이/Noun, 0, 1)),\n",
      " (Word(BOS/BOS, 0, 0), Word(이/Josa, 0, 1)),\n",
      " (Word(이/Noun, 0, 1), Word(것/Unk, 1, 2)),\n",
      " (Word(이/Josa, 0, 1), Word(것/Unk, 1, 2)),\n",
      " (Word(BOS/BOS, 0, 0), Word(이것/Noun, 0, 2)),\n",
      " (Word(이것/Noun, 0, 2), Word(은/Josa, 2, 3)),\n",
      " (Word(것/Unk, 1, 2), Word(은/Josa, 2, 3)),\n",
      " (Word(은/Josa, 2, 3), Word(예/Noun, 3, 4)),\n",
      " (Word(은/Josa, 2, 3), Word(예시/Noun, 3, 5)),\n",
      " (Word(예/Noun, 3, 4), Word(시/Unk, 4, 5)),\n",
      " (Word(예시/Noun, 3, 5), Word(이/Adjective + 었다/Eomi, 5, 7)),\n",
      " (Word(시/Unk, 4, 5), Word(이/Adjective + 었다/Eomi, 5, 7)),\n",
      " (Word(이/Adjective + 었다/Eomi, 5, 7), Word(EOS/EOS, 7, 8))]\n"
     ]
    }
   ],
   "source": [
    "links, bos, eos = generate_link(\n",
    "    '이것은 예시였다',\n",
    "    emissions,\n",
    "    lemmatizer,\n",
    "    max_word_len\n",
    ")\n",
    "\n",
    "pprint(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만든 링크에 가중치를 부여합니다. HMM 의 모델을 그래프로 표현하기 위해서는 현재 마디로 유입되는 앞 마디의 단어로부터 지금 단어로 이동하는 transition probability 와 현재 마디의 단어, 품사가 발생할 emission probability 의 곱, 혹은 두 확률의 log 값의 합을 가중치로 이용하면 됩니다. HMM 학습 시 보지 못했던 단어와 transition 일 수 있기 때문에 min_emission 과 min_transition 을 이용하여 Key Error 를 방지합니다.\n",
    "\n",
    "    w = emissions.get(to_node.tag0, {}).get(to_node.morph0, min_emission)\n",
    "    w += transitions.get((from_node.tag1, to_node.tag0), min_transition)\n",
    "\n",
    "단, '였다 = 이/Adjective + 었다/Eomi' 의 경우, 하나의 마디에 두 개의 형태소가 포함된 구조이므\n",
    "로, 이 때에는 마디 안에서의 transition 도 고려해야 합니다. tuple 로 표현된 마디의 첫번째 요소를 ' + '로 나눕니다. 이 길이가 2 라면 이는 '이 + 었다'처럼 두 형태소가 하나의 마디를 이루는 경우입니다.\n",
    "\n",
    "    if to_node.is_compound:\n",
    "        w += emissions.get(to_node.tag1, {}).get(to_node.morph0, min_emission)\n",
    "        w += transitions.get(to_node.tag1, {}).get(to_node.tag0, min_transition)\n",
    "\n",
    "그리고 이 과정에서 다른 품사보다 명사를 더 선호한다거나, 길이가 짧은 단어에 페널티를 부여할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_weight(links, emissions, transitions, min_emission, min_transition):\n",
    "\n",
    "    def weight(from_node, to_node):\n",
    "\n",
    "        # score of first word\n",
    "        w = emissions.get(to_node.tag0, {}).get(to_node.morph0, min_emission)\n",
    "        w += transitions.get((from_node.tag1, to_node.tag0), min_transition)\n",
    "\n",
    "        # penalty example\n",
    "        if to_node.tag0 == 'Noun' and len(to_node.morph0) == 1:\n",
    "            w = w / 2\n",
    "\n",
    "        # score of second word\n",
    "        if to_node.is_compound:\n",
    "            w += emissions.get(to_node.tag1, {}).get(to_node.morph1, min_emission)\n",
    "            w += transitions.get(to_node.tag1, {}).get(to_node.tag0, min_transition)\n",
    "        return w\n",
    "\n",
    "    graph = []\n",
    "    for from_node, to_node in links:\n",
    "        edge = (from_node, to_node, weight(from_node, to_node))\n",
    "        graph.append(edge)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "링크에 가중치를 더한 결과입니다. 미등록 단어인 '것'과 연결된 경우에는 가장 작은 가중치가 부여되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word(BOS/BOS, 0, 0)    \t->    Word(이/Noun, 0, 1): -1.386\n",
      "Word(BOS/BOS, 0, 0)    \t->    Word(이/Josa, 0, 1): -4.327\n",
      "Word(이/Noun, 0, 1)    \t->    Word(것/Unk, 1, 2): -4.952\n",
      "Word(이/Josa, 0, 1)    \t->    Word(것/Unk, 1, 2): -4.952\n",
      "Word(BOS/BOS, 0, 0)    \t->    Word(이것/Noun, 0, 2): -1.674\n",
      "Word(이것/Noun, 0, 2)    \t->    Word(은/Josa, 2, 3): -2.079\n",
      "Word(것/Unk, 1, 2)    \t->    Word(은/Josa, 2, 3): -3.634\n",
      "Word(은/Josa, 2, 3)    \t->    Word(예/Noun, 3, 4): -1.386\n",
      "Word(은/Josa, 2, 3)    \t->    Word(예시/Noun, 3, 5): -1.386\n",
      "Word(예/Noun, 3, 4)    \t->    Word(시/Unk, 4, 5): -4.952\n",
      "Word(예시/Noun, 3, 5)    \t->    Word(이/Adjective + 었다/Eomi, 5, 7): -5.239\n",
      "Word(시/Unk, 4, 5)    \t->    Word(이/Adjective + 었다/Eomi, 5, 7): -6.205\n",
      "Word(이/Adjective + 었다/Eomi, 5, 7)    \t->    Word(EOS/EOS, 7, 8): -2.823\n"
     ]
    }
   ],
   "source": [
    "graph = add_weight(links, emissions, transitions, min_emission, min_transition)\n",
    "\n",
    "for node_0, node_1, weight in graph:\n",
    "    print('{}    \\t->    {}: {:.4}'.format(node_0, node_1, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 포드 알고리즘을 이용한 최단경로 찾기\n",
    "\n",
    "최단경로를 찾는 포드 알고리즘은는 d[u] + w(u,v) < d[v] 이면 d[v] = d[u] + w(u,v) 로 대체합니다. 이를 d[u] + w(u,v) > d[v] 이면 d[v] = d[u] + w(u,v) 이 되도록 변경하면 최장경로를 구하는 함수로 바꿀 수 있습니다. HMM 은 log 확률 합의 최대값을 지니는 sequence 를 찾습니다. 이를 위하여 아래의 ford_list 함수는 최장경로를 찾도록 식을 변형하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shortestpath import ford_list\n",
    "\n",
    "nodes = {node for edge in graph for node in edge[:2]}\n",
    "\n",
    "# choose optimal sequence\n",
    "path, cost = ford_list(graph, nodes, bos, eos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 생성한 그래프를 ford_list 함수에 입력하면 품사 판별이 된 문장이 선택됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Word(BOS/BOS, 0, 0),\n",
      " Word(이것/Noun, 0, 2),\n",
      " Word(은/Josa, 2, 3),\n",
      " Word(예시/Noun, 3, 5),\n",
      " Word(이/Adjective + 었다/Eomi, 5, 7),\n",
      " Word(EOS/EOS, 7, 8)]\n"
     ]
    }
   ],
   "source": [
    "pprint(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. 형태소 형식으로 만들기\n",
    "\n",
    "위 결과에서 '이 + 었다'를 각각 하나의 형태소로 분해하면 형태소 분석의 결과가 됩니다. 반대로 '이 + 었다' 를 '였다/Adjective' 로 인식하면 품사 판별이 됩니다. 형태소 분석 결과로 만들기 위해서 ' + ' 를 기준으로 단어를 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BOS', 'BOS'),\n",
      " ('이것', 'Noun'),\n",
      " ('은', 'Josa'),\n",
      " ('예시', 'Noun'),\n",
      " ('이', 'Adjective'),\n",
      " ('었다', 'Eomi'),\n",
      " ('EOS', 'EOS')]\n"
     ]
    }
   ],
   "source": [
    "def flatten(path):\n",
    "    pos = []\n",
    "    for word in path:        \n",
    "        pos.append((word.morph0, word.tag0))\n",
    "        if word.is_compound:\n",
    "            pos.append((word.morph1, word.tag1))\n",
    "    return pos\n",
    "\n",
    "pos = flatten(path)\n",
    "pprint(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 미등록 단어 'tt' 가 포함된 문장을 넣어 형태소 분석을 한 결과까지 한 번에 얻어봅니다. 'tt' 가 미등록 단어로 인식되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BOS', 'BOS'),\n",
      " ('tt', 'Unk'),\n",
      " ('도', 'Josa'),\n",
      " ('예시', 'Noun'),\n",
      " ('이', 'Adjective'),\n",
      " ('었다', 'Eomi'),\n",
      " ('EOS', 'EOS')]\n"
     ]
    }
   ],
   "source": [
    "links, bos, eos = generate_link('tt도예시였다', emissions, lemmatizer, max_word_len)\n",
    "graph = add_weight(links, emissions, transitions, min_emission, min_transition)\n",
    "nodes = {node for edge in graph for node in edge[:2]}\n",
    "path, cost = ford_list(graph, nodes, bos, eos)\n",
    "pos = flatten(path)\n",
    "pprint(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. 미등록단어의 품사 추정\n",
    "\n",
    "이번에는 tt 의 품사를 추정하는 함수를 만듭니다. 'tt' 는 문장의 맨 앞에 시작했으며 뒤에 '-도/Josa'가 위치하기 때문에 명사일 가능성이 높습니다. 이를 HMM 의 식으로 표현하면 앞 단어의 품사에서 tt 가 가질 수 있는 품사로의 transtion probability 와 tt 가 가질 수 있는 품사에서 다음 단어로의 transition probability 의 곱이 가장 큰 품사를 찾으면 'Noun' 이라는 의미입니다.\n",
    "\n",
    "    for i, pos_i in enumerate(pos[:-1]):\n",
    "\n",
    "        ...\n",
    "        # previous -> current transition\n",
    "        if i == 1:\n",
    "            tag_prob = {tag:prob for tag, prob in begin.items()}\n",
    "        else:\n",
    "            tag_prob = {\n",
    "                tag:prob for (prev_tag, tag), prob in transition.items()\n",
    "                if prev_tag == pos[i-1][1]\n",
    "            }\n",
    "\n",
    "        # current -> next transition\n",
    "        for (tag, next_tag), prob in transition.items():\n",
    "            if next_tag == pos[i+1][1]:\n",
    "                tag_prob[tag] = tag_prob.get(tag, 0) + prob\n",
    "\n",
    "만약 앞, 뒤 단어의 품사들조차 이용할 수 없는 상황이라면 이를 명사로 추정합니다. 한국어 단어 중 명사가 가장 많은 단어를 보유하고 있기 때문에 이러한 추정은 자연스럽습니다.\n",
    "\n",
    "    for i, pos_i in enumerate(pos[:-1]):\n",
    "        if not tag_prob:\n",
    "            infered_tag = 'Noun'\n",
    "        else:\n",
    "            infered_tag = sorted(tag_prob, key=lambda x:-tag_prob[x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_unknown(pos, transitions):\n",
    "    pos_ = []\n",
    "    for i, pos_i in enumerate(pos[:-1]):\n",
    "        if not (pos_i[1] == 'Unk'):\n",
    "            pos_.append(pos_i)\n",
    "            continue\n",
    "\n",
    "        # previous -> current transition\n",
    "        tag_prob = {\n",
    "            tag:prob for (prev_tag, tag), prob in transitions.items()\n",
    "            if prev_tag == pos[i-1][1]\n",
    "        }\n",
    "\n",
    "        # current -> next transition\n",
    "        for (tag, next_tag), prob in transitions.items():\n",
    "            if next_tag == pos[i+1][1]:\n",
    "                tag_prob[tag] = tag_prob.get(tag, 0) + prob\n",
    "\n",
    "        # prevent BOS, EOS\n",
    "        tag_prob = {tag:prob for tag, prob in tag_prob.items() if tag != 'BOS' and tag != 'EOS'}\n",
    "\n",
    "        if not tag_prob:\n",
    "            infered_tag = 'Noun'\n",
    "        else:\n",
    "            infered_tag = sorted(tag_prob, key=lambda x:-tag_prob[x])[0]\n",
    "        pos_.append((pos_i[0], infered_tag))\n",
    "\n",
    "    return pos_ + pos[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추정 함수를 거친 결과 'tt' 는 명사로 인식됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BOS', 'BOS'),\n",
      " ('tt', 'Noun'),\n",
      " ('도', 'Josa'),\n",
      " ('예시', 'Noun'),\n",
      " ('이', 'Adjective'),\n",
      " ('었다', 'Eomi'),\n",
      " ('EOS', 'EOS')]\n"
     ]
    }
   ],
   "source": [
    "pos_ = inference_unknown(pos, transitions)\n",
    "pprint(pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. 후처리\n",
    "\n",
    "위 결과의 앞/뒤에 존재하는 BOS 와 EOS 를 제거하여 형태소 분석 결과를 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tt', 'Noun'),\n",
       " ('도', 'Josa'),\n",
       " ('예시', 'Noun'),\n",
       " ('이', 'Adjective'),\n",
       " ('었다', 'Eomi')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def postprocessing(pos):\n",
    "    return pos[1:-1]\n",
    "\n",
    "postprocessing(pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. 사용자 사전 추가\n",
    "\n",
    "HMM 기반 모델은 사용자 사전을 추가하기가 쉽습니다. 추가할 (단어, 품사)를 emission probability 에 추가만 하여도 되기 때문입니다.\n",
    "\n",
    "혹은 존재하는 단어라 하더라도 사용자가 원하는 선호도를 score 로 업데이트 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_user_dictionary(word, tag, score, emissions):\n",
    "    if not (tag in emission):\n",
    "        emissions[tag] = {word: score}\n",
    "    else:\n",
    "        emissions[tag][word] = score\n",
    "    return emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. 클래스 화 하기\n",
    "\n",
    "위 과정들을 클래스로 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from soylemma import Lemmatizer\n",
    "from shortestpath import ford_list\n",
    "\n",
    "class HMMTagger:\n",
    "    def __init__(self, emissions, transitions, lemmatize):\n",
    "        self.emissions = emissions\n",
    "        self.transitions = transitions\n",
    "        self.lemmatize = lemmatize\n",
    "\n",
    "        self._max_word_len = max(\n",
    "            len(w) for words in emissions.values() for w in words)\n",
    "        self._min_emission = min(\n",
    "            s for words in emissions.values() for s in words.values()) - 0.05\n",
    "        self._min_transition = min(transitions.values()) - 0.05\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        # lookup & generate graph\n",
    "        links, bos, eos = self._generate_link(sentence)\n",
    "        graph = self._add_weight(links)\n",
    "\n",
    "        # find optimal path\n",
    "        nodes = {node for edge in graph for node in edge[:2]}\n",
    "        path, cost = ford_list(graph, nodes, bos, eos)\n",
    "        pos = self._flatten(path)\n",
    "\n",
    "        # infering tag of unknown words\n",
    "        pos = self._inference_unknown(pos)\n",
    "\n",
    "        # post processing\n",
    "        pos = self._postprocessing(pos)\n",
    "\n",
    "        return pos\n",
    "\n",
    "    def _sentence_lookup(self, sentence):\n",
    "        sent = []\n",
    "        for eojeol in sentence.split():\n",
    "            sent += self._eojeol_lookup(eojeol, offset=len(sent))\n",
    "        return sent\n",
    "\n",
    "    def _eojeol_lookup(self, eojeol, offset=0):\n",
    "        n = len(eojeol)\n",
    "        pos = [[] for _ in range(n)]\n",
    "        for b in range(n):\n",
    "            for r in range(1, self._max_word_len+1):\n",
    "                e = b+r\n",
    "                if e > n:\n",
    "                    continue\n",
    "                sub = eojeol[b:e]\n",
    "\n",
    "                # string match\n",
    "                for tag in self._get_pos(sub):\n",
    "                    pos[b].append(Word(sub, tag, None, tag, b + offset, e + offset, False))\n",
    "\n",
    "                # lemmatizer\n",
    "                for (w0, t0), (w1, t1) in self.lemmatize(sub):\n",
    "                    pos[b].append(Word(w0, t0, w1, t1, b + offset, e + offset, True))\n",
    "        return pos\n",
    "\n",
    "    def _get_pos(self, sub):\n",
    "        tags = []\n",
    "        for tag, words in self.emissions.items():\n",
    "            if tag == 'Verb' or tag == 'Adjective' or tag == 'Eomi':\n",
    "                continue\n",
    "            if sub in words:\n",
    "                tags.append(tag)\n",
    "        return tags\n",
    "\n",
    "    def _generate_link(self, sentence):\n",
    "\n",
    "        def get_nonempty_first(sent, end, offset=0):\n",
    "            for i in range(offset, end):\n",
    "                if sent[i]:\n",
    "                    return i\n",
    "            return offset\n",
    "\n",
    "        chars = sentence.replace(' ','')\n",
    "        sent = self._sentence_lookup(sentence)\n",
    "        n_char = len(sent) + 1\n",
    "\n",
    "        eos = Word('EOS', 'EOS', None, 'EOS', n_char-1, n_char, False)\n",
    "        sent.append([eos])\n",
    "\n",
    "        i = get_nonempty_first(sent, n_char)\n",
    "        if i > 0:\n",
    "            sent[0].append(Word(chars[:i], 'Unk', None, 'Unk', 0, i, False))\n",
    "\n",
    "        links = []\n",
    "        for words in sent[:-1]:\n",
    "            for word in words:\n",
    "                if not sent[word.end]:\n",
    "                    b = get_nonempty_first(sent, n_char, word.end)\n",
    "                    unk = Word(chars[word.end:b], 'Unk', None, 'Unk', word.end, b, False)\n",
    "                    links.append((word, unk))\n",
    "                for adjacent in sent[word.end]:\n",
    "                    links.append((word, adjacent))\n",
    "\n",
    "        unks = {to_node for _, to_node in links if to_node[1] == 'Unk'}\n",
    "        for unk in unks:\n",
    "            for adjacent in sent[unk.end]:\n",
    "                links.append((unk, adjacent))\n",
    "\n",
    "        bos = Word('BOS', 'BOS', None, 'BOS', 0, 0, False)\n",
    "        for word in sent[0]:\n",
    "            links.append((bos, word))\n",
    "        links = sorted(links, key=lambda x:(x[0][3], x[1][4]))\n",
    "\n",
    "        return links, bos, eos\n",
    "\n",
    "    def _add_weight(self, links):\n",
    "\n",
    "        def weight(from_node, to_node):\n",
    "\n",
    "            # score of first word\n",
    "            w = self.emissions.get(to_node.tag0, {}).get(to_node.morph0, self._min_emission)\n",
    "            w += self.transitions.get((from_node.tag1, to_node.tag0), self._min_transition)\n",
    "\n",
    "            # score of second word\n",
    "            if to_node.is_compound:\n",
    "                w += self.emissions.get(to_node.tag1, {}).get(to_node.morph1, self._min_emission)\n",
    "                w += self.transitions.get(to_node.tag0, {}).get(to_node.tag1, self._min_transition)\n",
    "            return w\n",
    "\n",
    "        graph = []\n",
    "        for from_node, to_node in links:\n",
    "            edge = (from_node, to_node, weight(from_node, to_node))\n",
    "            graph.append(edge)\n",
    "        return graph\n",
    "\n",
    "    def _flatten(self, path):\n",
    "        pos = []\n",
    "        for word in path:\n",
    "            pos.append((word.morph0, word.tag0))\n",
    "            if word.is_compound:\n",
    "                pos.append((word.morph1, word.tag1))\n",
    "        return pos\n",
    "\n",
    "    def _inference_unknown(self, pos):\n",
    "        pos_ = []\n",
    "        for i, pos_i in enumerate(pos[:-1]):\n",
    "            if not (pos_i[1] == 'Unk'):\n",
    "                pos_.append(pos_i)\n",
    "                continue\n",
    "\n",
    "            # previous -> current transition\n",
    "            tag_prob = {\n",
    "                tag:prob for (prev_tag, tag), prob in self.transitions.items()\n",
    "                if prev_tag == pos[i-1][1]\n",
    "            }\n",
    "\n",
    "            # current -> next transition\n",
    "            for (tag, next_tag), prob in self.transitions.items():\n",
    "                if next_tag == pos[i+1][1]:\n",
    "                    tag_prob[tag] = tag_prob.get(tag, 0) + prob\n",
    "\n",
    "            # prevent BOS, EOS\n",
    "            tag_prob = {tag:prob for tag, prob in tag_prob.items() if tag != 'BOS' and tag != 'EOS'}\n",
    "\n",
    "            if not tag_prob:\n",
    "                infered_tag = 'Noun'\n",
    "            else:\n",
    "                infered_tag = sorted(tag_prob, key=lambda x:-tag_prob[x])[0]\n",
    "            pos_.append((pos_i[0], infered_tag))\n",
    "\n",
    "        return pos_ + pos[-1:]\n",
    "\n",
    "    def _postprocessing(self, pos):\n",
    "        return pos[1:-1]\n",
    "\n",
    "    def add_user_dictionary(self, word, tag, score):\n",
    "        if not (tag in self.emissions):\n",
    "            self.emissions[tag] = {word: score}\n",
    "        else:\n",
    "            self.emissions[tag][word] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리한 품사 판별기를 이용하여 앞의 예시 문장의 형태소 분석 결과를 다시 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tt', 'Noun'),\n",
       " ('도', 'Josa'),\n",
       " ('예시', 'Noun'),\n",
       " ('이', 'Adjective'),\n",
       " ('었다', 'Eomi')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_tagger = HMMTagger(emissions, transitions, lemmatizer.analyze)\n",
    "hmm_tagger.tag('tt도예시였다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('공연', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('단어', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('예시', 'Noun'),\n",
       " ('이', 'Adjective'),\n",
       " ('었다', 'Eomi')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_tagger.tag('공연은단어의예시였다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아이오아이', 'Noun'),\n",
       " ('와', 'Josa'),\n",
       " ('공연', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('단어', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('예시', 'Noun'),\n",
       " ('이', 'Adjective'),\n",
       " ('었다', 'Eomi'),\n",
       " ('tt', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('이것', 'Noun'),\n",
       " ('을', 'Josa'),\n",
       " ('이용', 'Noun'),\n",
       " ('하', 'Verb'),\n",
       " ('ㄴ다', 'Eomi')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_tagger.tag('아이오아이와공연은단어의예시였다tt는이것을이용한다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
