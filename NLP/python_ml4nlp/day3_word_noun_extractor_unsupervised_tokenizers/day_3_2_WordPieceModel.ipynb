{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Piece Model 은 논문에 subword unit 을 만드는 python 코드가 적혀있습니다. 편의를 위해서 일부분을 수정하였으며, tokenize 함수를 따로 만들어 두었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating 10 / 10 was done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_': 2,\n",
       " 'd': 3,\n",
       " 'e': 2,\n",
       " 'est_': 3,\n",
       " 'low': 2,\n",
       " 'low_': 5,\n",
       " 'newest_': 6,\n",
       " 'r': 2,\n",
       " 'wi': 3}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "class WordPieceModel:\n",
    "    def __init__(self, max_iters=10):\n",
    "        self.max_iters = max_iters if max_iters > 0 else 10\n",
    "        self.units = {}\n",
    "        self.max_length = 0\n",
    "\n",
    "    def train(self, sents):\n",
    "        vocabs = self._sent_to_vocabs(sents)\n",
    "        self.units = self._build_subword_units(vocabs)\n",
    "\n",
    "    def _sent_to_vocabs(self, sents):\n",
    "        vocabs = Counter((eojeol.replace('_', '') for sent in sents for eojeol in sent.split() if eojeol))\n",
    "        return {' '.join(w) + ' _': c for w,c in vocabs.items() if w}\n",
    "\n",
    "    def _build_subword_units(self, vocabs):\n",
    "        def get_stats(vocabs):\n",
    "            pairs = defaultdict(int)\n",
    "            for word, freq in vocabs.items():\n",
    "                symbols = word.split()\n",
    "                for i in range(len(symbols)-1):\n",
    "                    pairs[(symbols[i],symbols[i+1])] += freq\n",
    "            return pairs\n",
    "\n",
    "        def merge_vocab(pair, v_in):\n",
    "            v_out = {}\n",
    "            bigram = ' '.join(pair)\n",
    "            replacer = ''.join(pair)\n",
    "            for word, freq in v_in.items():\n",
    "                w_out = word.replace(bigram, replacer)\n",
    "                v_out[w_out] = freq\n",
    "            return v_out\n",
    "\n",
    "        for i in range(1, self.max_iters + 1):\n",
    "            print('\\riterating {} / {} ...'.format(i, self.max_iters), end='', flush=True)\n",
    "            pairs = get_stats(vocabs)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocabs = merge_vocab(best, vocabs)\n",
    "        print('\\riterating {} / {} was done'.format(i, self.max_iters))\n",
    "\n",
    "        units = {}\n",
    "        for word, freq in vocabs.items():\n",
    "            for unit in word.split():\n",
    "                units[unit] = units.get(unit, 0) + freq\n",
    "        self.max_length = max((len(w) for w in units))\n",
    "        return dict(sorted(units.items(), key=lambda x:-x[1])[:self.max_iters])\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        return ' '.join([self._tokenize(w) for w in s.split()])\n",
    "\n",
    "    def _tokenize(self, w):\n",
    "        def initialize(w):\n",
    "            w += '_'\n",
    "            subwords = []\n",
    "            n = len(w)\n",
    "            for b in range(n):\n",
    "                for e in range(b+1, min(n, self.max_length)+1):\n",
    "                    subword = w[b:e]\n",
    "                    if not subword in self.units:\n",
    "                        continue\n",
    "                    subwords.append((subword, b, e, e-b))\n",
    "            return subwords\n",
    "\n",
    "        def longest_match(subwords):\n",
    "            matched = []\n",
    "            subwords = sorted(subwords, key=lambda x:(-x[3], x[1]))\n",
    "            while subwords:\n",
    "                s, b, e, l = subwords.pop(0) # str, begin, end, length\n",
    "                matched.append((s, b, e, l))\n",
    "                removals = []\n",
    "                for i, (_, b_, e_, _) in enumerate(subwords):\n",
    "                    if (b_ < e and b < e_) or (b_ < e and e_ > b):\n",
    "                        removals.append(i)\n",
    "                for i in reversed(removals):\n",
    "                    del subwords[i]\n",
    "            return sorted(matched, key=lambda x:x[1])\n",
    "\n",
    "        subwords = initialize(w)\n",
    "        subwords = longest_match(subwords)\n",
    "        subwords = ' '.join([s for s, _, _, _ in subwords])\n",
    "        return subwords\n",
    "\n",
    "\n",
    "vocab = {'l o w _' : 5,\n",
    "         'l o w e r _' : 2,\n",
    "         'n e w e s t _':6,\n",
    "         'w i d e s t _':3\n",
    "        }\n",
    "\n",
    "encoder = WordPieceModel()\n",
    "encoder.units = encoder._build_subword_units(vocab)\n",
    "encoder.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'low e r _'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder._tokenize('lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'low e r _ newest_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.tokenize('lower newest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.49\n",
      "Dataset version\n",
      "[navermovie_comments.data] is latest (0.0.1)\n",
      "[navermovie_comments.models] is latest (0.0.1)\n",
      "[navernews_10days.data] is latest (0.0.1)\n",
      "[navernews_10days.models] is latest (0.0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from config import dataset_dir\n",
    "sys.path.append('{}/lovit_textmining_dataset'.format(dataset_dir))\n",
    "\n",
    "from navernews_10days import get_news_paths\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "corpus_path = get_news_paths(date='2016-10-20')\n",
    "corpus = DoublespaceLineCorpus(corpus_path, iter_sent=True, num_sent = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating 1000 / 1000 was done\n"
     ]
    }
   ],
   "source": [
    "news_encoder = WordPieceModel(max_iters=1000)\n",
    "news_encoder.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014년_', 14),\n",
       " ('연합뉴스_', 165),\n",
       " ('현지시간_', 47),\n",
       " ('확장억제_', 31),\n",
       " ('2016_', 28),\n",
       " ('자료사진_', 27),\n",
       " ('설명했다_', 24),\n",
       " ('국방장관_', 24),\n",
       " ('관계자는_', 17),\n",
       " ('강조했다_', 17),\n",
       " ('알려졌다_', 14),\n",
       " ('대통령이_', 14),\n",
       " ('인근에서_', 12),\n",
       " ('것이라고_', 12),\n",
       " ('탄자니아_', 12),\n",
       " ('예정이다_', 11),\n",
       " ('것이라는_', 11),\n",
       " ('것으로_', 90),\n",
       " ('19일_', 80),\n",
       " ('말했다_', 58)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by length\n",
    "sorted(news_encoder.units.items(), key=lambda x:-len(x[0]))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', 1184),\n",
       " ('을_', 578),\n",
       " ('이_', 547),\n",
       " ('에_', 505),\n",
       " ('의_', 489),\n",
       " ('한_', 366),\n",
       " ('를_', 334),\n",
       " ('은_', 314),\n",
       " ('는_', 290),\n",
       " ('고_', 286),\n",
       " ('이', 283),\n",
       " ('에서_', 276),\n",
       " ('가_', 260),\n",
       " ('했다_', 219),\n",
       " ('대', 215),\n",
       " ('지', 210),\n",
       " ('로_', 196),\n",
       " ('시', 176),\n",
       " ('사', 175),\n",
       " ('도_', 174)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by frequency\n",
    "sorted(news_encoder.units.items(), key=lambda x:-x[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent: 19\n",
      "tokenized: 19 _\n",
      "\n",
      "sent: 1990\n",
      "tokenized: 19 9 0_\n",
      "\n",
      "sent: 52 1 22\n",
      "tokenized: 5 2_ 1_ 2 2_\n",
      "\n",
      "sent: 오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스\n",
      "tokenized: 오 패 산 터널_ 총격 전_ 용의자_ 검거 _ 서울_ 연합뉴스_ 경찰 _ 관계 자 들이_ 19일_ 오후_ 서울_ 강북 구_ 오 패 산_ 터널_ 인근에서_ 사 제_ 총 기를_ 발 사 해_ 경찰 을_ 살 해 한_ 용의자_ 성 모 씨를_ 검거 하고_ 있다_ 성씨는_ 검거 _ 당시_ 서 바 이 벌 _ 게 임 에서_ 쓰 는_ 방 탄 조 에_ 까지_ 착 용 한_ 상태 였다_ 독 자 제공_ 영 상_ 처_ 연합뉴스_\n",
      "\n",
      "sent: 서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다\n",
      "tokenized: 서울_ 연합뉴스_ 김 경_ 기자_ 사 제_ 총 기로_ 경찰 을_ 살 해 한_ 범 인_ 성 모_ 4 6_ 씨는_ 주 도 면 밀 했다_\n",
      "\n",
      "sent: 경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다\n",
      "tokenized: 경찰 에_ 따르면_ 성씨는_ 19일_ 오후_ 강북 경찰 서_ 인 근 _ 부 동 산_ 업 소_ 밖 에서_ 부 동 산 업 자_ 이 모_ 6 7_ 씨가_ 나 오 기를_ 기 다 렸다_ 이 씨 는_ 평 소 에도_ 말 다 을_ 자 주_ 한_ 것으로_ 알려졌다_\n",
      "\n",
      "sent: 이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다\n",
      "tokenized: 이 씨가_ 나 와_ 기_ 시작 하 자_ 성씨는_ 따 라 가 면서_ 미 리_ 준비 해 온_ 사 제_ 총 기를_ 이 씨 에게_ 발 사 했다_ 총 알 이_ 나 가 면서_ 이 씨는_ 도 갔다_ 그_ 나 간_ 총 알 은_ 지 나 가 던_ 행 인_ 7 1_ 씨 의_ 배 를_ 스 쳤 다_\n",
      "\n",
      "sent: 성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다\n",
      "tokenized: 성씨는_ 강북 서_ 인 근 _ 치 집 까지_ 이 씨 _ 뒤 를_ 으며_ 실 랑 이 하다_ 쓰 러 린_ 후_ 총 기 와_ 함께_ 가 온_ 치 로_ 이 씨 _ 머 리를_ 때 렸다_\n",
      "\n",
      "sent: 이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다\n",
      "tokenized: 이_ 과정 에서_ 오후_ 6 시_ 20 분 _ 강북 구_ 번 동_ 길 _ 위 에서_ 사람 들이_ 우 고_ 있다_ 총 소 리가_ 다_ 는_ 등의_ 신 고가_ 여 러 건_ 들어 왔다_\n",
      "\n",
      "sent: 5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다\n",
      "tokenized: 5 분_ 후 에_ 성 씨 의_ 전자 발 가_ 손 됐 다는_ 신 고가_ 보 호 관 소_ 시 스 을_ 통해_ 들어 왔다_ 성 범 자 로_ 전자 발 를_ 차 고_ 있 던_ 성씨는_ 부 로_ 직 접 _ 자신의_ 발 를_ 었다_\n",
      "\n",
      "sent: 용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기\n",
      "tokenized: 용의자_ 소 지_ 사제총 기_ 2 정_ 서울_ 연합뉴스_ 임 정_ 기자_ 서울_ 시 내 에서_ 폭행_ 용 의 자 가_ 현장 _ 조 사를_ 벌 이 던_ 경찰관 에게_ 사제총 기를_ 발 사 해_ 경찰관 이_ 숨 졌다_ 19일_ 오후_ 6 시 2 8 분_ 강북 구_ 번 동 에서_ 둔 기로_ 맞 았다_ 는_ 폭행_ 피 해_ 신 고가_ 접 수 돼_ 현장 에서_ 조사 하던_ 강북 경찰 서_ 번 동 파 출 소_ 소 속_ 김 모_ 5 4_ 경위 가_ 폭행_ 용의자_ 성 모_ 4 5_ 씨가_ _ 사제총 기에_ 맞 고_ 쓰 러 진_ 뒤_ 원에_ 겨 으나_ 숨 졌다_ 사진은_ 용 의 자 가_ 소 지 한_ 사제총 기_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(corpus):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print('sent: {}'.format(sent))\n",
    "    print('tokenized: {}'.format(news_encoder.tokenize(sent)), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
