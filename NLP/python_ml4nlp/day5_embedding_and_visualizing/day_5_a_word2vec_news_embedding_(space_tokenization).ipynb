{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 띄어쓰기 만으로 토크나이징을 할 수도 있습니다. 데이터의 숫자가 정말로 풍부하고, 모델이 수많은 어절을 그대로 학습할 수 있을만큼의 하드웨어라면, 어절을 그대로 Word2Vec 학습하여도 그 결과는 충분히 쓸만합니다만, 그 목적이 다를 수 있습니다. 아래에서 그 예시를 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import config\n",
    "from navernews_10days import get_news_paths\n",
    "\n",
    "path = get_news_paths(date='2016-10-20', tokenize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soynlp.utils.DoublespaceLineCorpus 를 이용하여 list of str 이 yield 되는 input data 를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19']\n",
      "['1990']\n",
      "['52', '1', '22']\n",
      "['오패산터널', '총격전', '용의자', '검거', '서울', '연합뉴스', '경찰', '관계자들이', '19일', '오후']\n",
      "['서울', '연합뉴스', '김은경', '기자', '사제', '총기로', '경찰을', '살해한', '범인', '성모']\n",
      "['경찰에', '따르면', '성씨는', '19일', '오후', '강북경찰서', '인근', '부동산', '업소', '밖에서']\n"
     ]
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "    \n",
    "class Word2VecCorpus:\n",
    "    def __init__(self, fname):\n",
    "        self.corpus = DoublespaceLineCorpus(fname, iter_sent=True)\n",
    "    def __iter__(self):\n",
    "        for sent in self.corpus:\n",
    "            yield sent.split()\n",
    "                \n",
    "word2vec_corpus = Word2VecCorpus(path)\n",
    "for num_sent, sent in enumerate(word2vec_corpus):\n",
    "    if num_sent > 5:\n",
    "        break\n",
    "    print(sent[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(word2vec_corpus, min_count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries = '김무성 박근혜 문재인 국방부 정부 국정원 대통령 축구 외교 정책 군대 미국 일본 중국 아이오아이'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어절만 이용하여 학습했음에도 불구하고, 김무성의 유사어로 정치인들이 등장함을 볼 수 있습니다. 이는 뉴스 코퍼스에서는 정치인의 이름만 적고 띄어쓰기를 하는 경우도 충분하기 때문입니다. \n",
    "\n",
    "하지만 '김무성 - 대표도'와 같은 유사 어절이 등장한 것은, 뉴스에서 '김무성'으로만 적는 경우와 '김무성 대표도'로 적는 경우가 혼재되어 있어서, '대표도'의 문맥이 '김무성'의 문맥과 유사하였기 때문입니다. \n",
    "\n",
    "비슷한 의미로, 박근혜 - 박 (6251)의 경우에는 '박근혜 대통령', '박 대통령'이 번갈아가며 이용되었기 때문입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나, 국정원의 경우에는 총 42번 나왔으며, min_count=30으로 하였기 때문에 18000여개의 단어 중에서는 infrequent 한 편에 속합니다. 그리고 이 때의 유사 어절들은 문맥상 잘 어울리지 않는 어절들입니다. \n",
    "\n",
    "    Vocab(count:42, index:12945, sample_int:4294967296)\n",
    "    국정원 - 단국대 (33) (0.774)\n",
    "    국정원 - 대우학원 (34) (0.726)\n",
    "    국정원 - 김형수 (92) (0.722)\n",
    "    국정원 - 인터뷰가 (36) (0.717)\n",
    "    국정원 - 입 (59) (0.711)\n",
    "    국정원 - 정동구 (31) (0.709)\n",
    "    국정원 - 정유라씨가 (41) (0.705)\n",
    "    국정원 - 취재진이 (33) (0.703)\n",
    "    국정원 - 크리스 (33) (0.696)\n",
    "    국정원 - 이사장을 (81) (0.696)\n",
    "    \n",
    "Word2Vec에서는 infrequent 한 단어(어절)들의 유사 단어(어절)들은 infrequent 한 경향이 있습니다. 빈도수가 충분하지 않을 경우에는 학습이 잘 되지 않는 것이라고 해석할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어절을 그대로 학습할 경우에는, 아래와 같이 대통령의 유사어절로 '대통령 + 조사'의 어절들이 자주 등장합니다. \n",
    "    \n",
    "    Seed word = 대통령\n",
    "    Vocab(count:3411, index:89, sample_int:4294967296)\n",
    "    대통령 - 대통령의 (3074) (0.809)\n",
    "    대통령 - 대통령을 (287) (0.784)\n",
    "    대통령 - 대통령과 (437) (0.784)\n",
    "    대통령 - 대통령에 (255) (0.723)\n",
    "    대통령 - 대통령은 (2852) (0.716)\n",
    "    대통령 - 대통령에게 (203) (0.708)\n",
    "    대통령 - 대통령도 (125) (0.704)\n",
    "    대통령 - 대통령이다 (54) (0.698)\n",
    "    대통령 - 대통령이 (5054) (0.698)\n",
    "    대통령 - 정부에서 (76) (0.687)\n",
    "    \n",
    "우리가 원하는 것이 Language model을 학습하는 것이라면, 이 결과는 유용합니다. 하지만, 우리가 문맥적으로 유사한 단어들을 찾기 위하여 Word2Vec을 학습하는 것이 목적이었다면, 어절을 그대로 학습하는 것보다 토크나이징을 한 뒤, 이를 이용하여 학습해야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Query = 김무성\n",
      "Vocab(count:40, index:13718, sample_int:4294967296)\n",
      "김무성 - 김종인 (87, 0.882)\n",
      "김무성 - 측근인 (31, 0.875)\n",
      "김무성 - 대표와 (232, 0.866)\n",
      "김무성 - 오세훈 (34, 0.864)\n",
      "김무성 - 정의화 (33, 0.863)\n",
      "김무성 - 행장 (35, 0.856)\n",
      "김무성 - 김문수 (69, 0.850)\n",
      "김무성 - 이재오 (32, 0.849)\n",
      "김무성 - 이재정 (102, 0.842)\n",
      "김무성 - 박명재 (76, 0.841)\n",
      "\n",
      "\n",
      "Query = 박근혜\n",
      "Vocab(count:1412, index:261, sample_int:4294967296)\n",
      "박근혜 - 박 (2276, 0.771)\n",
      "박근혜 - 올랑드 (53, 0.764)\n",
      "박근혜 - 두테르테 (334, 0.762)\n",
      "박근혜 - 푸틴 (124, 0.733)\n",
      "박근혜 - 오바마 (229, 0.713)\n",
      "박근혜 - 노 (190, 0.703)\n",
      "박근혜 - 정권 (183, 0.686)\n",
      "박근혜 - 최순실이 (39, 0.673)\n",
      "박근혜 - 대통령이 (1454, 0.670)\n",
      "박근혜 - 블라디미르 (73, 0.665)\n",
      "\n",
      "\n",
      "Query = 문재인\n",
      "Vocab(count:933, index:411, sample_int:4294967296)\n",
      "문재인 - 문 (742, 0.899)\n",
      "문재인 - 안철수 (193, 0.861)\n",
      "문재인 - 반기문 (136, 0.827)\n",
      "문재인 - 야권 (134, 0.811)\n",
      "문재인 - 사무총장 (104, 0.804)\n",
      "문재인 - 당내 (73, 0.803)\n",
      "문재인 - 김종인 (87, 0.801)\n",
      "문재인 - 송민순 (814, 0.797)\n",
      "문재인 - 손학규 (1217, 0.787)\n",
      "문재인 - 추미애 (42, 0.786)\n",
      "\n",
      "\n",
      "Query = 국방부\n",
      "Vocab(count:327, index:1515, sample_int:4294967296)\n",
      "국방부 - 카터 (207, 0.908)\n",
      "국방부 - 한민구 (143, 0.906)\n",
      "국방부 - 애슈턴 (130, 0.880)\n",
      "국방부 - 윤병세 (297, 0.875)\n",
      "국방부 - 케리 (401, 0.866)\n",
      "국방부 - 국방장관이 (83, 0.862)\n",
      "국방부 - 외교부 (418, 0.855)\n",
      "국방부 - 국무장관과 (40, 0.836)\n",
      "국방부 - 존 (345, 0.835)\n",
      "국방부 - 미 (1264, 0.819)\n",
      "\n",
      "\n",
      "Query = 정부\n",
      "Vocab(count:1005, index:380, sample_int:4294967296)\n",
      "정부 - 정부의 (677, 0.890)\n",
      "정부 - 예산 (305, 0.797)\n",
      "정부 - 정부와 (203, 0.775)\n",
      "정부 - 누리과정 (45, 0.768)\n",
      "정부 - 국가 (496, 0.761)\n",
      "정부 - 집권 (37, 0.755)\n",
      "정부 - 정부는 (726, 0.755)\n",
      "정부 - 구조조정 (166, 0.748)\n",
      "정부 - 저출산 (106, 0.738)\n",
      "정부 - 현 (689, 0.733)\n",
      "\n",
      "\n",
      "Query = 국정원\n",
      "Vocab(count:284, index:1794, sample_int:4294967296)\n",
      "국정원 - 국감에서 (136, 0.801)\n",
      "국정원 - 이병호 (366, 0.799)\n",
      "국정원 - 국정원장 (127, 0.776)\n",
      "국정원 - 국가정보원 (41, 0.770)\n",
      "국정원 - 정보위 (292, 0.770)\n",
      "국정원 - 국정원장의 (162, 0.763)\n",
      "국정원 - 황 (106, 0.751)\n",
      "국정원 - 정보위원회 (121, 0.738)\n",
      "국정원 - 통화에서 (130, 0.737)\n",
      "국정원 - 김만복 (235, 0.734)\n",
      "\n",
      "\n",
      "Query = 대통령\n",
      "Vocab(count:1187, index:309, sample_int:4294967296)\n",
      "대통령 - 대통령과 (197, 0.855)\n",
      "대통령 - 대통령도 (53, 0.850)\n",
      "대통령 - 대통령의 (856, 0.850)\n",
      "대통령 - 대통령을 (180, 0.791)\n",
      "대통령 - 버락 (113, 0.789)\n",
      "대통령 - 이명박 (70, 0.787)\n",
      "대통령 - 노무현 (266, 0.780)\n",
      "대통령 - 대통령이 (1454, 0.778)\n",
      "대통령 - 박정희 (48, 0.771)\n",
      "대통령 - 오바마 (229, 0.770)\n",
      "\n",
      "\n",
      "Query = 축구\n",
      "Vocab(count:36, index:15179, sample_int:4294967296)\n",
      "축구 - 연주 (43, 0.790)\n",
      "축구 - 1세대 (39, 0.782)\n",
      "축구 - 악기 (35, 0.759)\n",
      "축구 - 문학 (62, 0.739)\n",
      "축구 - 한국사 (47, 0.729)\n",
      "축구 - 미술관 (40, 0.724)\n",
      "축구 - 봉사활동 (36, 0.724)\n",
      "축구 - 민족 (36, 0.723)\n",
      "축구 - 발해태조건국지 (39, 0.722)\n",
      "축구 - 태양 (33, 0.720)\n",
      "\n",
      "\n",
      "Query = 외교\n",
      "Vocab(count:757, index:542, sample_int:4294967296)\n",
      "외교 - 국방 (297, 0.955)\n",
      "외교 - 외교와 (32, 0.918)\n",
      "외교 - 차관급 (50, 0.918)\n",
      "외교 - 한미 (864, 0.911)\n",
      "외교 - 고위급 (165, 0.894)\n",
      "외교 - 국방장관 (310, 0.868)\n",
      "외교 - 양국은 (219, 0.862)\n",
      "외교 - 양국 (487, 0.848)\n",
      "외교 - 양국이 (120, 0.837)\n",
      "외교 - 북한인권 (113, 0.830)\n",
      "\n",
      "\n",
      "Query = 정책\n",
      "Vocab(count:441, index:1063, sample_int:4294967296)\n",
      "정책 - 정책의 (76, 0.874)\n",
      "정책 - 안보 (183, 0.835)\n",
      "정책 - 정책과 (62, 0.820)\n",
      "정책 - 정책을 (280, 0.820)\n",
      "정책 - 차원의 (217, 0.812)\n",
      "정책 - 대응 (174, 0.805)\n",
      "정책 - 문제 (668, 0.800)\n",
      "정책 - 저출산 (106, 0.795)\n",
      "정책 - 정부의 (677, 0.794)\n",
      "정책 - 전략적 (150, 0.793)\n",
      "\n",
      "\n",
      "Query = 미국\n",
      "Vocab(count:4057, index:75, sample_int:4294967296)\n",
      "미국 - 미국의 (951, 0.738)\n",
      "미국 - 한국 (1681, 0.737)\n",
      "미국 - 일본 (1615, 0.737)\n",
      "미국 - 러시아 (395, 0.721)\n",
      "미국 - 해군 (170, 0.716)\n",
      "미국 - 인도 (198, 0.712)\n",
      "미국 - 영국 (611, 0.708)\n",
      "미국 - 일본과 (69, 0.707)\n",
      "미국 - 한국과 (319, 0.706)\n",
      "미국 - 호주 (236, 0.698)\n",
      "\n",
      "\n",
      "Query = 일본\n",
      "Vocab(count:1615, index:227, sample_int:4294967296)\n",
      "일본 - 호주 (236, 0.848)\n",
      "일본 - 대만 (186, 0.805)\n",
      "일본 - 영국 (611, 0.804)\n",
      "일본 - 한국 (1681, 0.799)\n",
      "일본 - 중국 (2523, 0.797)\n",
      "일본 - 홍콩 (207, 0.790)\n",
      "일본 - 태국 (127, 0.789)\n",
      "일본 - 인도 (198, 0.788)\n",
      "일본 - 한국과 (319, 0.775)\n",
      "일본 - 독일 (587, 0.755)\n",
      "\n",
      "\n",
      "Query = 중국\n",
      "Vocab(count:2523, index:132, sample_int:4294967296)\n",
      "중국 - 일본 (1615, 0.797)\n",
      "중국 - 인도 (198, 0.770)\n",
      "중국 - 인도네시아 (194, 0.753)\n",
      "중국 - 중국의 (365, 0.740)\n",
      "중국 - 호주 (236, 0.739)\n",
      "중국 - 한국 (1681, 0.737)\n",
      "중국 - 유럽 (540, 0.715)\n",
      "중국 - 필리핀 (561, 0.712)\n",
      "중국 - 대만 (186, 0.704)\n",
      "중국 - 영국 (611, 0.693)\n",
      "\n",
      "\n",
      "Query = 아이오아이\n",
      "Vocab(count:123, index:4400, sample_int:4294967296)\n",
      "아이오아이 - 방탄소년단 (200, 0.928)\n",
      "아이오아이 - 몬스타엑스 (69, 0.926)\n",
      "아이오아이 - 샤이니 (173, 0.895)\n",
      "아이오아이 - 듀오 (89, 0.881)\n",
      "아이오아이 - 갓세븐 (32, 0.878)\n",
      "아이오아이 - 코드 (41, 0.871)\n",
      "아이오아이 - 신용재 (33, 0.867)\n",
      "아이오아이 - 에이핑크 (159, 0.859)\n",
      "아이오아이 - 다비치 (51, 0.856)\n",
      "아이오아이 - 엑스 (44, 0.855)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda2/envs/scrapper/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "/home/lovit/anaconda2/envs/scrapper/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "\n",
    "    if not (query in word2vec_model.wv.vocab):\n",
    "        continue\n",
    "\n",
    "    print('\\n\\nQuery = {}\\n{}'.format(query, word2vec_model.wv.vocab[query]))\n",
    "\n",
    "    for sim_word, sim in word2vec_model.most_similar(query):\n",
    "        sim_count = word2vec_model.wv.vocab[sim_word].count\n",
    "        print('%s - %s (%d, %.3f)' % (query, sim_word, sim_count, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
