{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.49\n",
      "added lovit_textmining_dataset\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "\n",
    "from navernews_10days import get_comments_paths\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "path = get_comments_paths(date='2016-10-20')\n",
    "corpus = DoublespaceLineCorpus(path, iter_sent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word Extractor and Tokenizer\n",
    "\n",
    "명사 추출기를 이용하여 명사 점수를 단어 점수로 이용합니다. 뉴스 기사는 띄어쓰기가 잘 되어 있기 때문에 L-Tokenizer 를 이용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=1260, neg=1173, common=12\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 687560 from 228630 sents. mem=0.181 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2775027, mem=1.653 Gb\n",
      "[Noun Extractor] batch prediction was completed for 244896 words\n",
      "[Noun Extractor] checked compounds. discovered 183564 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 211088 -> 159349\n",
      "[Noun Extractor] postprocessing ignore_features : 159349 -> 159153\n",
      "[Noun Extractor] postprocessing ignore_NJ : 159153 -> 157566\n",
      "[Noun Extractor] 157566 nouns (183564 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=2.005 Gb                    \n",
      "[Noun Extractor] 74.53 % eojeols are covered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['뉴스', '기사', '를', '이용', '하여', '학습', '한', '모델', '입니다']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_extractor = LRNounExtractor_v2()\n",
    "noun_scores = noun_extractor.train_extract(corpus)\n",
    "\n",
    "word_scores = {noun:score.score for noun, score in noun_scores.items()}\n",
    "tokenizer = LTokenizer(scores = word_scores)\n",
    "\n",
    "tokenizer.tokenize('뉴스 기사를 이용하여 학습한 모델입니다', tolerance=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build co-occurrence matrix\n",
    "\n",
    "L parts 에서 명사 점수의 최대값과 0.3 점 차이가 나지 않는다면 그 중 가장 긴 subword 를 단어로 추출하도록 tolerance 를 이용하였습니다. min_tf 를 10 으로 설정하여 10 번 이하로 등장한 단어에 대해서는 co-occurrence 를 계산하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create (word, contexts) matrix\n",
      "  - counting word frequency from 228629 sents, mem=2.156 Gb\n",
      "  - scanning (word, context) pairs from 228629 sents, mem=2.156 Gb\n",
      "  - (word, context) matrix was constructed. shape = (30810, 30810)                    \n",
      "  - done\n"
     ]
    }
   ],
   "source": [
    "from soynlp.vectorizer import sent_to_word_contexts_matrix\n",
    "\n",
    "def custom_tokenizer(sent):\n",
    "    return tokenizer.tokenize(sent, tolerance=0.3)\n",
    "\n",
    "x, idx2vocab = sent_to_word_contexts_matrix(\n",
    "    corpus,\n",
    "    windows = 2,\n",
    "    min_tf = 10,\n",
    "    tokenizer = custom_tokenizer,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 후 30,810 개의 단어가 학습되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30810, 30810)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similar words using context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idx2vocab 을 vocab2idx 로 만듭니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302\n",
      "1892\n"
     ]
    }
   ],
   "source": [
    "vocab2idx = {vocab:idx for idx, vocab in enumerate(idx2vocab)}\n",
    "\n",
    "print(vocab2idx['이화여대'])\n",
    "print(vocab2idx['아이오아이'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.metrics.pairwise_distances 를 이용하여 '이화여대'와 context vector 가 비슷한 다른 단어를 찾습니다. \n",
    "\n",
    "numpy.ndarray 형식인 distance matrix (dist) 에 대하여 argsort() 를 하면, 거리 순서대로 정렬됩니다. sort() 를 하면 값이 정렬되며, argsort() 를 하면 각 값의 index 가 return 됩니다. \n",
    "\n",
    "여러 번 쓸 수 있도록 함수로도 만들어둡니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def most_similar_words(query, wv, topk=10):\n",
    "    \"\"\"\n",
    "    query : str\n",
    "    wv : word representation\n",
    "    topk : num of similar terms\n",
    "    \"\"\"\n",
    "    \n",
    "    if not (query in vocab2idx):\n",
    "        return []\n",
    "\n",
    "    query_idx = vocab2idx[query]\n",
    "    query_vec = wv[query_idx,:].reshape(1,-1)\n",
    "    dist = pairwise_distances(query_vec, wv, metric='cosine')[0]\n",
    "    similars = []\n",
    "\n",
    "    # sorting\n",
    "    for similar_idx in dist.argsort():\n",
    "        \n",
    "        # filtering query term\n",
    "        if similar_idx == query_idx:\n",
    "            continue\n",
    "\n",
    "        if len(similars) >= topk:\n",
    "            break\n",
    "\n",
    "        # decoding index to word\n",
    "        similar_word = idx2vocab[similar_idx]\n",
    "        similars.append((similar_word, 1 - dist[similar_idx]))\n",
    "\n",
    "    return similars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cooccurrence matrix 만을 이용해도 문맥이 **매우** 확실한 단어 `아이오아이`는 다른 아이돌 이름이나 `아오아` 같은 약어가 유사어로 검색됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('블랙핑크', 0.6647859272131573),\n",
       " ('트와이스', 0.6262975246594952),\n",
       " ('엑소', 0.5901119271712834),\n",
       " ('여자친구', 0.5623464164823095),\n",
       " ('노래', 0.5616676134078805),\n",
       " ('우리나라', 0.5489489482770429),\n",
       " ('에이핑크', 0.5366051901750828),\n",
       " ('아오아', 0.5336581153448168),\n",
       " ('우리', 0.5334947525137911),\n",
       " ('가수', 0.5308252449873944)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 단어 `아프리카`의 유사어는 잘 검색되지 않습니다. Co-occurrence frequency 를 representation 으로 이용하면 어느 문맥에나 등장하는 단어들에 영향을 받기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('회사', 0.815166536698036),\n",
       " ('우리나라', 0.7957352100985269),\n",
       " ('머리', 0.7956636660982553),\n",
       " ('학교', 0.7854806600688846),\n",
       " ('우리', 0.7806264137345261),\n",
       " ('정부', 0.772928310373537),\n",
       " ('사회', 0.7697541171327323),\n",
       " ('이나라', 0.7691019000466983),\n",
       " ('스크린도어', 0.7584292635755353),\n",
       " ('뉴스', 0.756708891308895)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI 를 이용한 term weighting\n",
    "\n",
    "soynlp 의 pmi 를 이용하여 co-occurrence matrix 에 PMI 를 적용합니다. `min_pmi` 를 0 으로 설정하여 Positive PMI 로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing pmi was done                              \n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import pmi\n",
    "\n",
    "x_pmi = pmi(x, min_pmi=0.0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPMI 가 어느 문맥에나 등장하는 단어의 weight 를 0 으로 만들기 때문에 단어의 문맥이 뚜렷해집니다. `아이오아이`의 유사어는 크게 달라지지 않았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('트와이스', 0.26833507616667673),\n",
       " ('블랙핑크', 0.2330310888485071),\n",
       " ('ioi', 0.20614506232316532),\n",
       " ('블핑', 0.20043581143858202),\n",
       " ('엑소', 0.1956895594920276),\n",
       " ('빅뱅', 0.19290733919589997),\n",
       " ('아오아', 0.1923566732868287),\n",
       " ('방탄', 0.18676463532029342),\n",
       " ('컴백', 0.18369205928881527),\n",
       " ('트와', 0.17785742618855838)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 `아프리카`의 유사어는 크게 바뀌었습니다. 다른 나라의 이름도 검색되며, `아프리카tv` 관련 단어인 `bj` 나 `유튜브`가 검색됩니다. 이날 대륙 `아프리카`에 관련된 기사와 `아프리카tv` 관련 기사가 모두 있었기 때문에 두 문맥이 모두 반영되어 있습니다.\n",
    "\n",
    "그러나 representation vector 의 차원은 3 만 차원보다 큽니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bj', 0.12834827693122852),\n",
       " ('유튜브', 0.096835574394464),\n",
       " ('아프리카tv', 0.09196082471525624),\n",
       " ('남미', 0.08615314705604948),\n",
       " ('유투브', 0.08609361437533758),\n",
       " ('삼성', 0.08402667987562407),\n",
       " ('수수료', 0.0818229514991633),\n",
       " ('우리나라', 0.0808034827606553),\n",
       " ('일본', 0.07906642731209024),\n",
       " ('수준', 0.07747639800665107)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI + SVD 를 이용한 차원 축소\n",
    "\n",
    "Singular Vector Decomposition (SVD) 를 이용하여 차원을 축소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "x_pmisvd = svd.fit_transform(x_pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD 로 차원 축소를 하면 noise 가 어느 정도 제거됩니다. Co-occurrence matrix 에서의 noise 는 문맥이 특이하거나 infrequent 한 단어입니다. 그 외에는 PPMI 를 적용했을 때와 유사어의 관계는 비슷합니다. 하지만 cosine 유사도의 값은 커집니다. PPMI 만 적용하면 sparse vector 이기 때문에 평균적인 유사도가 작습니다. 하지만 SVD 를 적용하면 평균적인 유사도 scale 이 커집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('트와이스', 0.8312918236557398),\n",
       " ('블랙핑크', 0.7997067755304503),\n",
       " ('엑소', 0.7867454808727841),\n",
       " ('빅뱅', 0.7706317217383628),\n",
       " ('블핑', 0.7672525339203083),\n",
       " ('방탄', 0.74302486441824),\n",
       " ('음원', 0.704504341079977),\n",
       " ('여자친구', 0.691471618537713),\n",
       " ('컴백', 0.6913749814622887),\n",
       " ('트와', 0.6748600420141303)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아이오아이', x_pmisvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('유튜브', 0.5146891539203406),\n",
       " ('티비', 0.5080156207242567),\n",
       " ('유투브', 0.49143872809457734),\n",
       " ('수준', 0.48707990949387203),\n",
       " ('국내', 0.4826523614881042),\n",
       " ('우리나라', 0.47600365296728875),\n",
       " ('방송', 0.4716885486745739),\n",
       " ('필리핀', 0.4646173406329134),\n",
       " ('후진국', 0.46366026290804196),\n",
       " ('회사', 0.4624947071755334)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words('아프리카', x_pmisvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
