{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 연습한 LSTM 품사 판별기의 코드에서 forward 함수만 변화하여 마지막 hidden vector 를 sentence representation 으로 이용하는 sentence classification 용 모델을 만들어봅니다. 데이터를 모두 이용하면 느리기 때문에 10k 개의 문장만 이용한 간단한 모델을 만들어봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version = 1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print('pytorch version = {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from navermovie_comments import load_comments_image_without_padding\n",
    "from navermovie_comments import load_trained_embedding\n",
    "import numpy as np\n",
    "\n",
    "n_data = 1000\n",
    "sents, labels, idx_to_vocab = load_comments_image_without_padding(\n",
    "    large=True, tokenize='soynlp_unsup', n_data=n_data)\n",
    "\n",
    "# transform list of int to torch.tensor\n",
    "X = [torch.LongTensor(sent) for sent in sents]\n",
    "\n",
    "# transform label to torch.tensor\n",
    "def encode_label(y):\n",
    "    return y <= 6\n",
    "\n",
    "Y = torch.LongTensor([encode_label(y) for y in labels])\n",
    "\n",
    "word2vec_model = load_trained_embedding(data_name='large',\n",
    "    tokenize='soynlp_unsup', embedding='word2vec')\n",
    "\n",
    "wv = word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 235, 0: 765})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(Y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained 된 word embedding vector 를 이용할 것이기 때문에 CNN 을 이용한 sentence classification 의 예시처럼 nn.Embedding layer 를 만든 뒤, 입력된 embedding vectors 를 복사합니다. \n",
    "\n",
    "```python\n",
    "class Model:\n",
    "    def __init__(self, ... ):\n",
    "        # ...\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pretrained_wordvec is not None:\n",
    "            self.word_embeddings.weight.data.copy_(torch.from_numpy(pretrained_wordvec))\n",
    "```\n",
    "\n",
    "마지막 output vector 를 sentence vector 로 이용하기 위하여 lstm_out[-1] 을 선택합니다. 이를 linear layer 인 hidden2label 에 넣어 y, prediction vector 를 얻습니다.\n",
    "\n",
    "```python\n",
    "class Model:\n",
    "    def __init__(self, ...):\n",
    "        # ...\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # ...\n",
    "        y = self.hidden2label(lstm_out[-1])\n",
    "        return y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, label_size, pretrained_wordvec):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # prepare word embedding vector\n",
    "        vocab_size, embedding_dim = pretrained_wordvec.shape\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight.data.copy_(torch.from_numpy(pretrained_wordvec))\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        hidden, cell = self.init_hidden()\n",
    "        lstm_out, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        # Use only last output\n",
    "        y = self.hidden2label(lstm_out[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습하기 위한 패러매터를 정의합니다. 단어의 개수가 많기 떄문에 hidden dimension 을 128 로 크게 잡아줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "label_size = np.unique(Y.numpy()).shape[0]\n",
    "max_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model, loss function, optimizer 를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(hidden_dim, label_size, wv)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정은 앞선 모델들과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, Y, optimizer, loss_func, epoch, checkpoints=200):\n",
    "    def status(loss_sum, i):\n",
    "        loss_tmp = loss_sum / (i+1)\n",
    "        args = (epoch, i+1, loss_tmp)\n",
    "        print('\\repoch = {}, batch = {}, training loss = {:.3}'.format(*args))\n",
    "\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(zip(X, Y)):\n",
    "        model.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y.view(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if i % checkpoints == checkpoints-1:\n",
    "            status(loss_sum, i)\n",
    "    status(loss_sum, i)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, batch = 200, training loss = 0.629\n",
      "epoch = 1, batch = 400, training loss = 0.538\n",
      "epoch = 1, batch = 600, training loss = 0.478\n",
      "epoch = 1, batch = 800, training loss = 0.423\n",
      "epoch = 1, batch = 1000, training loss = 0.378\n",
      "epoch = 1, batch = 1000, training loss = 0.378\n",
      "\n",
      "epoch = 2, batch = 200, training loss = 0.558\n",
      "epoch = 2, batch = 400, training loss = 0.443\n",
      "epoch = 2, batch = 600, training loss = 0.385\n",
      "epoch = 2, batch = 800, training loss = 0.34\n",
      "epoch = 2, batch = 1000, training loss = 0.306\n",
      "epoch = 2, batch = 1000, training loss = 0.306\n",
      "\n",
      "epoch = 3, batch = 200, training loss = 0.438\n",
      "epoch = 3, batch = 400, training loss = 0.348\n",
      "epoch = 3, batch = 600, training loss = 0.295\n",
      "epoch = 3, batch = 800, training loss = 0.263\n",
      "epoch = 3, batch = 1000, training loss = 0.237\n",
      "epoch = 3, batch = 1000, training loss = 0.237\n",
      "\n",
      "epoch = 4, batch = 200, training loss = 0.337\n",
      "epoch = 4, batch = 400, training loss = 0.264\n",
      "epoch = 4, batch = 600, training loss = 0.216\n",
      "epoch = 4, batch = 800, training loss = 0.193\n",
      "epoch = 4, batch = 1000, training loss = 0.175\n",
      "epoch = 4, batch = 1000, training loss = 0.175\n",
      "\n",
      "epoch = 5, batch = 200, training loss = 0.238\n",
      "epoch = 5, batch = 400, training loss = 0.188\n",
      "epoch = 5, batch = 600, training loss = 0.149\n",
      "epoch = 5, batch = 800, training loss = 0.134\n",
      "epoch = 5, batch = 1000, training loss = 0.121\n",
      "epoch = 5, batch = 1000, training loss = 0.121\n",
      "\n",
      "epoch = 6, batch = 200, training loss = 0.165\n",
      "epoch = 6, batch = 400, training loss = 0.127\n",
      "epoch = 6, batch = 600, training loss = 0.1\n",
      "epoch = 6, batch = 800, training loss = 0.0893\n",
      "epoch = 6, batch = 1000, training loss = 0.081\n",
      "epoch = 6, batch = 1000, training loss = 0.081\n",
      "\n",
      "epoch = 7, batch = 200, training loss = 0.113\n",
      "epoch = 7, batch = 400, training loss = 0.0866\n",
      "epoch = 7, batch = 600, training loss = 0.0682\n",
      "epoch = 7, batch = 800, training loss = 0.0591\n",
      "epoch = 7, batch = 1000, training loss = 0.0535\n",
      "epoch = 7, batch = 1000, training loss = 0.0535\n",
      "\n",
      "epoch = 8, batch = 200, training loss = 0.0764\n",
      "epoch = 8, batch = 400, training loss = 0.0586\n",
      "epoch = 8, batch = 600, training loss = 0.047\n",
      "epoch = 8, batch = 800, training loss = 0.0402\n",
      "epoch = 8, batch = 1000, training loss = 0.0358\n",
      "epoch = 8, batch = 1000, training loss = 0.0358\n",
      "\n",
      "epoch = 9, batch = 200, training loss = 0.0543\n",
      "epoch = 9, batch = 400, training loss = 0.0408\n",
      "epoch = 9, batch = 600, training loss = 0.033\n",
      "epoch = 9, batch = 800, training loss = 0.0283\n",
      "epoch = 9, batch = 1000, training loss = 0.0249\n",
      "epoch = 9, batch = 1000, training loss = 0.0249\n",
      "\n",
      "epoch = 10, batch = 200, training loss = 0.0397\n",
      "epoch = 10, batch = 400, training loss = 0.0292\n",
      "epoch = 10, batch = 600, training loss = 0.0234\n",
      "epoch = 10, batch = 800, training loss = 0.0202\n",
      "epoch = 10, batch = 1000, training loss = 0.0178\n",
      "epoch = 10, batch = 1000, training loss = 0.0178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(hidden_dim, label_size, wv)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.5)\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model = train(model, X, Y, optimizer, loss_fun, epoch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.998\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        y_pred = model(x)\n",
    "        score, predicted = torch.max(y_pred.data, dim=1)\n",
    "        n_correct += (int(predicted) == int(y))\n",
    "    accuracy = n_correct / len(labels)\n",
    "\n",
    "print('accuracy = {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
