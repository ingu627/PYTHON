{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version check\n",
    "\n",
    "우리는 scikit learn 에서 제공하는 20NewsGroups data 를 이용하여 document classification 을 하는 multi-layer feed-foward neural network 를 만들어봅니다.\n",
    "\n",
    "현재 실습의 torch 버전은 1.0.1 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "20NewsGroups 의 모든 데이터를 이용해도 되지만, 빠른 확인을 위하여 네 개의 카테고리만 이용합니다. 20NewsGroups 은 20 개의 카테고리로 분류된 뉴스 문서 집합입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set and test set\n",
    "categories = [\n",
    "    'rec.sport.baseball',\n",
    "    'soc.religion.christian',\n",
    "    'comp.windows.x',\n",
    "    'sci.space'\n",
    "]\n",
    "\n",
    "removals = ('headers', 'footers', 'quotes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove 에 'headers', 'footers', 'quotes' 를 넣으면 뉴스의 header 들이 제거 된 text 만 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train', remove=removals, categories=categories)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test', remove=removals, categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 text 와 category label 을 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = newsgroups_train.data\n",
    "y_train = newsgroups_train.target\n",
    "data_test = newsgroups_test.data\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "\n",
    "TF-IDF vectorizer 를 이용하여 vectorizing 을 합니다. 이 때 학습 데이터 기준에서 word index 가 학습되도록 학습데이터에는 fit_transform 을 적용하고, 테스트 데이터에는 transform 을 적용합니다. 학습 데이터에 존재하지 않는 단어는 테스트 단어에서 벡터화 되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x = (2382, 6535), y = (2382,)\n",
      "test  x = (1584, 6535), y = (1584,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "x_train = vectorizer.fit_transform(data_train)\n",
    "x_test = vectorizer.transform(data_test)\n",
    "\n",
    "print('train x = {}, y = {}'.format(x_train.shape, y_train.shape))\n",
    "print('test  x = {}, y = {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Feed forward neural netowrk 를 만듭니다. torch.nn.Module 을 상속하여 init, forward 함수만 구현하면 됩니다.\n",
    "\n",
    "init 을 구현할 때에는 Python 의 상속처럼 super().\\_\\_init\\_\\_() 을 실행해야 합니다. 우리는 3 개의 hidden layer 로 이뤄진 feed forward 를 만들 것입니다. 각각은 (input, hidden 1), (hidden 1, hidden 2), (hidden 2, classes) 의 크기로 이뤄진 weight matrix 를 지닙니다. bias 역시 학습하도록 설정합니다.\n",
    "\n",
    "```python\n",
    "self.fc_1 = nn.Linear(in_features = input_dim, out_features = hidden_1_dim, bias=True)\n",
    "self.fc_2 = nn.Linear(in_features = hidden_1_dim, out_features = hidden_2_dim, bias=True)\n",
    "self.fc_3 = nn.Linear(in_features = hidden_2_dim, out_features = n_classes, bias = True)\n",
    "```\n",
    "\n",
    "우리는 Linear layer 만을 만들었을 뿐, activation function 은 아직 만들지 않았습니다. hidden layer 1, 2 의 output 에 대하여 ReLU 를 적용합니다.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    out = F.relu(self.fc_1(x))\n",
    "    out = F.relu(self.fc_2(out))\n",
    "    return self.fc_3(out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__ (self, input_dim, hidden_1_dim, hidden_2_dim, n_classes):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_1_dim, bias=True)\n",
    "        self.fc_2 = nn.Linear(hidden_1_dim, hidden_2_dim, bias=True)\n",
    "        self.fc_3 = nn.Linear(hidden_2_dim, n_classes, bias = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc_1(x))\n",
    "        out = F.relu(self.fc_2(out))\n",
    "        return self.fc_3(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "numpy.unique 는 numpy.ndarray 의 값의 set 입니다. 네 개의 카테고리가 각각 0, 1, 2, 3 으로 encoding 되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 이용할 패러매터를 정의합니다. epochs, mini batch size, hidden 1, hidden 2 의 크기를 정의합니다.\n",
    "\n",
    "우리가 정의한 네트워크의 구조는 아래와 같습니다. 6,535 개의 단어로 표현된 문서가 128, 32 차원을 거쳐 4 개의 클래스로 분류됩니다.\n",
    "\n",
    "    6535 - 128 - 32 - 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer = 6535 - 128 - 32 - 4\n",
      "n data = 2382\n",
      "num mini-batch = 37\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "hidden_1_dim = 128\n",
    "hidden_2_dim = 32\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "n_data = x_train.shape[0]\n",
    "\n",
    "print('layer = {} - {} - {} - {}'.format(\n",
    "    input_dim, hidden_1_dim, hidden_2_dim, n_classes))\n",
    "print('n data = {}'.format(n_data))\n",
    "print('num mini-batch = {}'.format(n_data // batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, loss function, optimizer\n",
    "\n",
    "실제로 모델을 만듭니다. loss function (criterion) 과 optimizer 는 regression 과 비슷합니다. 단, classification 이기 때문에 Cross Entropy loss 를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = FeedForwardNN(\n",
    "    input_dim,\n",
    "    hidden_1_dim,\n",
    "    hidden_2_dim,\n",
    "    n_classes\n",
    ")\n",
    "\n",
    "# Parameter for the optimizer\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의한 바와 같이 6535 - 128 - 32 구조의 hidden layer 를 지닌, 4 개의 클래스를 분류하는 neural network 가 만들어졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardNN(\n",
      "  (fc_1): Linear(in_features=6535, out_features=128, bias=True)\n",
      "  (fc_2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc_3): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function\n",
    "\n",
    "epochs 만큼 iteration 을 돕니다. 각 epoch 마다의 누적 loss 를 저장하기 위한 준비를 합니다.\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0\n",
    "    # TODO\n",
    "```\n",
    "\n",
    "mini batch 는 데이터의 일부로 loss 를 계산한 다음, 이를 반영하여 weight parameter 를 학습하는 것입니다. 데이터의 크기가 100 일 때, 한 번에 5 개의 데이터를 이용한다면 총 20 번의 mini batch 를 이용한 학습이 됩니다. 이는 다음처럼 구현합니다. begin (b), end (e) index 를 만들고, x 와 y 에서 b 부터 e 까지 slicing 을 합니다. 이 때 x 의 형식은 scipy.sparse 입니다. \n",
    "\n",
    "```python\n",
    "for i in range(n_data // batch_size):\n",
    "\n",
    "    b = i * batch_size\n",
    "    e = min(n_data, (i+1) * batch_size)\n",
    "\n",
    "    x_batch = x_train[b:e] # type : sparse matrix\n",
    "    y_batch = y_train[b:e] # type : numpy.ndarray\n",
    "```\n",
    "\n",
    "이를 먼저 numpy.ndarray 로 변환한 뒤, torch.Tensor 로 변환합니다. 한 번에 x_train 을 scipy.sparse 에서 numpy.ndarray 로 변환하면 sparse matrix 가 dense matrix 로 변하면서 지나치게 많은 메모리를 소모하게 됩니다. \n",
    "\n",
    "x 는 TF-IDF 값이기 때문에 소수값을 포함합니다. 이는 torch.FloatTensor 로 구현합니다. label 은 정수이기 때문에 torch.LongTensor 로 감쌉니다.\n",
    "\n",
    "```python\n",
    "for i in range(n_data // batch_size):\n",
    "    # ...\n",
    "    x_batch = torch.FloatTensor(x_batch.todense())\n",
    "    y_batch = torch.LongTensor(y_batch)\n",
    "```\n",
    "\n",
    "항상 optimizer 에 이전 step 에서 이용한 gradient 를 지운 뒤, forwarding, back-propagation 을 합니다.\n",
    "\n",
    "```python\n",
    "for i in range(n_data // batch_size):\n",
    "    # ...\n",
    "    optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "prediction, loss 계산, 그리고 back propagation 을 수행합니다.\n",
    "\n",
    "```python\n",
    "for i in range(n_data // batch_size):\n",
    "    # ...\n",
    "    x_pred = model(x_batch)\n",
    "    loss = criterion(x_pred, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "loss 의 data 에는 loss 값이 저장되어 있습니다. 이는 torch.Tensor 이니 numpy() 를 이용하여 숫자로 변환하여 loss_sum 에 누적합니다.\n",
    "\n",
    "```python\n",
    "for i in range(n_data // batch_size):\n",
    "    # ...\n",
    "    loss_sum += loss.data.numpy()\n",
    "```\n",
    "\n",
    "매 epoch 마다 loss 가 줄어듦을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## epoch = 0, training loss = 1.340s = 1.357\n",
      "## epoch = 1, training loss = 0.878s = 0.934\n",
      "## epoch = 2, training loss = 0.295s = 0.310\n",
      "## epoch = 3, training loss = 0.121s = 0.121\n",
      "## epoch = 4, training loss = 0.072s = 0.069\n",
      "## epoch = 5, training loss = 0.053s = 0.050\n",
      "## epoch = 6, training loss = 0.043s = 0.040\n",
      "## epoch = 7, training loss = 0.038s = 0.035\n",
      "## epoch = 8, training loss = 0.035s = 0.032\n",
      "## epoch = 9, training loss = 0.033s = 0.030\n"
     ]
    }
   ],
   "source": [
    "def train(x_train, y_train, model, loss_func, optimizer, batch_size):\n",
    "\n",
    "    n_data = x_train.shape[0]\n",
    "\n",
    "    # Loop over all epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss_sum = 0\n",
    "\n",
    "        for i in range(n_data // batch_size):\n",
    "\n",
    "            # select mini-batch data\n",
    "            b = i * batch_size\n",
    "            e = min(n_data, (i+1) * batch_size)\n",
    "            x_batch = x_train[b:e] # type : sparse matrix\n",
    "            y_batch = y_train[b:e] # type : numpy.ndarray\n",
    "\n",
    "            # scipy.sparse -> numpy.ndarray -> torch.Tensor\n",
    "            x_batch = torch.FloatTensor(x_batch.todense())\n",
    "            y_batch = torch.LongTensor(y_batch)\n",
    "\n",
    "            # Forward -> Backward -> Optimize\n",
    "            optimizer.zero_grad() # Make the gradient buffer zero\n",
    "            x_pred = model(x_batch)\n",
    "            loss = loss_func(x_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # cumulate loss\n",
    "            loss_sum += loss.data.numpy()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                loss_tmp = loss_sum / (i+1)\n",
    "                print('\\repoch = {}, batch = {}, training loss = {}'.format(\n",
    "                    epoch, i+1, '%.3f' % loss_tmp), end='')\n",
    "\n",
    "        print('\\r## epoch = {}, training loss = {}'.format(epoch, '%.3f' % (loss_sum / (i+1)) ))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train(x_train, y_train, model, loss_func, optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training accuracy test\n",
    "\n",
    "테스트 데이터에 대하여 정확도를 측정합니다. 데이터의 크기가 얼마 크지 않기 때문에 한 번에 dense matrix 로 변환하였습니다. 그리고 torch.Tensor 로 변환하여 모델에 입력합니다.\n",
    "\n",
    "```python\n",
    "x_test_tensor = torch.FloatTensor(x_test.todense())\n",
    "```\n",
    "\n",
    "forward 함수가 call 함수로 지정되어 있기 때문에 아래 두 구문은 같은 기능을 수행합니다.\n",
    "\n",
    "```python\n",
    "y_test_pred = model(x_test_tensor)\n",
    "y_test_pred = model.forward(x_test_tensor)\n",
    "```\n",
    "\n",
    "그 결과 1584 개의 데이터에 대하여 각각 4 개의 클래스와의 softmax 값이 계산됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1584, 6535)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1584, 4])\n"
     ]
    }
   ],
   "source": [
    "# Make test batch data\n",
    "x_test_tensor = torch.FloatTensor(x_test.todense())\n",
    "y_test_pred = model(x_test_tensor)\n",
    "\n",
    "print(y_test_pred.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.max 는 torch.Tensor 에 대하여 dim 기준으로 최대값과 그 값을 지니는 index 를 출력합니다. dim=1 이기 때문에 [1584, 4] size 에서 4 인, column 기준으로 max 값을 찾습니다. return 은 max value 와 index 로 나뉘어져 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1584])\n",
      "torch.Size([1584])\n"
     ]
    }
   ],
   "source": [
    "score, predicted = torch.max(y_test_pred.data, dim=1)\n",
    "\n",
    "print(score.size())\n",
    "print(predicted.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7198, 9.5568, 8.6373])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584,)\n",
      "(1584,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(predicted.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy.ndarray 를 이용하여 predicted 된 값과 label 이 같은 indices 를 찾습니다. 그 길이만큼 제대로된 prediction 을 수행한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9059343434343434\n"
     ]
    }
   ],
   "source": [
    "n_correct = np.where(y_test == predicted.numpy())[0].shape[0]\n",
    "accuracy = n_correct / y_test.shape[0]\n",
    "\n",
    "print('accuracy = {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
