{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader with MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 는 gray image 이기 때문에 image 로 볼 수 있습니다. matplotlib 을 이용한 이미지 출력 함수 `imshow` 를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    \"\"\"\n",
    "    Notes\n",
    "    -----\n",
    "    plt.imshow input : numpy.ndarray\n",
    "        Shape:\n",
    "            (M, N) : 1 channel image\n",
    "            (M, N, 3) : RBG channel image\n",
    "            (M, N, 4) : RBGA channel image\n",
    "    \"\"\"\n",
    "    npimg = img.squeeze().numpy() # (1, 28, 28) -> (28, 28)\n",
    "    plt.imshow(npimg, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvision 에서 datasets 을 import 합니다. `torch_data_dir` 은 fetch 받은 데이터들의 저장 위치입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from config import torch_data_dir\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset 에서 MNIST 를 불러옵니다. `download=True` 로 설정하면 `torch_data_dir` 에 데이터가 없을 경우 다운로드 받습니다. transform 은 파일을 읽어 torch.Tensor 형태로 변형하는 함수 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_func = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    torch_data_dir, train=True, download=True,\n",
    "    transform=transform_func)\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    torch_data_dir, train=False, download=True,\n",
    "    transform=transform_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 은 `__len__` 과 `__getitem__` 이 구현되어 있습니다. `__len__` 에 의하여 데이터셋의 길이를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__getitem__` 에 의하여 특정 데이터를 가져올 수 있습니다. MNIST data 는 (28, 28) 의 image 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor(2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = train_dataset[5]\n",
    "print(X.size())\n",
    "print(y)\n",
    "\n",
    "imshow(X)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index 가 넘어가면 IndexError 가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 60000 is out of bounds for dimension 0 with size 60000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-275d044d18e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 60000 is out of bounds for dimension 0 with size 60000"
     ]
    }
   ],
   "source": [
    "train_dataset[60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset 은 모든 데이터를 memory 에 올려둔 형태입니다. 만약 데이터의 크기가 매우 크다면 이 부분을 Gensim Word2Vec 의 input 처럼 index 가 입력된 부분만 읽는 형식으로 만들 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.train_data.size())\n",
    "print(train_dataset.train_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader 는 dataset 의 데이터를 shuffle 할 수도 있으며, mini batch 로 yield 합니다. 또한 torch 내부에서 데이터를 읽어들이는 부분과 모델을 학습하는 부분을 병렬로 처리할 수 있도록 도와줍니다.\n",
    "\n",
    "train_loader 의 mini batch size 는 64 이기 때문에 각 batch 별로 (64, 1, 28, 28) 의 tensor (64 장의 images) 가 yield 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 0\n",
      "X: torch.Size([64, 1, 28, 28]), <class 'torch.Tensor'>\n",
      "y: torch.Size([64]), <class 'torch.Tensor'>\n",
      "\n",
      "Batch # 1\n",
      "X: torch.Size([64, 1, 28, 28]), <class 'torch.Tensor'>\n",
      "y: torch.Size([64]), <class 'torch.Tensor'>\n",
      "\n",
      "Batch # 2\n",
      "X: torch.Size([64, 1, 28, 28]), <class 'torch.Tensor'>\n",
      "y: torch.Size([64]), <class 'torch.Tensor'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "use_cuda = False\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "for i_batch, (X, y) in enumerate(train_loader):\n",
    "    if i_batch >= 3:\n",
    "        break\n",
    "    print('Batch # {}'.format(i_batch))\n",
    "    print('X: {}, {}'.format(X.size(), type(X)))\n",
    "    print('y: {}, {}'.format(y.size(), type(y)), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset\n",
    "\n",
    "우리가 이용할 데이터 역시 dataset 으로 만들어 DataLoader 를 이용할 수 있습니다. 토크나이징이 된 네이버 영화평 데이터의 단어를 vocabulary index 로 치환하여 comments images 를 만들었습니다. 데이터의 한 줄은 아래처럼 vocabulary index 가 띄어쓰기로 나뉘어져 있는 형태입니다. 문장마다 길이가 다릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 699 126 188 205 5 126 13\n",
      "493 14 757 65 108 1580 590 699 126 272 8965 154 7 851 575 8517 6 620 953 104 24 153\n",
      "126 1029 263 536 220 245 11327 312 120 8 13 6743 4254\n",
      "224 497 71 2430 10 10558\n"
     ]
    }
   ],
   "source": [
    "from navermovie_comments import get_comments_image_path\n",
    "\n",
    "x_path, y_path, vocab_path = get_comments_image_path(large=False, tokenize='soynlp_unsup')\n",
    "\n",
    "with open(x_path, encoding='utf-8') as f:\n",
    "    for _ in range(4):\n",
    "        print(next(f).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CommentsImage 는 위의 데이터를 numpy.ndarray 형태로 읽어들입니다. 문장의 길이가 다르기 때문에 짧은 문장의 경우에는 <padding> 이 추가됩니다. padding index 는 마지막 vocabulary index 입니다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (294078, 20)\n",
      "<class 'numpy.ndarray'> (294078,)\n",
      "['살아남는것', 'Mar', '농업', '대장님', '<padding>']\n"
     ]
    }
   ],
   "source": [
    "from navermovie_comments import load_comments_image\n",
    "\n",
    "X, y, idx_to_vocab = load_comments_image(\n",
    "    large=False, tokenize='soynlp_unsup', max_len=20)\n",
    "\n",
    "print(type(X), X.shape)\n",
    "print(type(y), y.shape)\n",
    "print(idx_to_vocab[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  590,   699,   126,   188,   205,     5,   126,    13, 26809,\n",
       "        26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809,\n",
       "        26809, 26809],\n",
       "       [  493,    14,   757,    65,   108,  1580,   590,   699,   126,\n",
       "          272,  8965,   154,     7,   851,   575,  8517,     6,   620,\n",
       "          953,   104],\n",
       "       [  126,  1029,   263,   536,   220,   245, 11327,   312,   120,\n",
       "            8,    13,  6743,  4254, 26809, 26809, 26809, 26809, 26809,\n",
       "        26809, 26809]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__getitem__` 과 `__len__` 을 구현하여 torch 의 Dataset 으로 만듭니다. `init` 부분에서 numpy.ndarray 를 torch.Tensor 로 변환하는 부분을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294078\n",
      "tensor([  224,   497,    71,  2430,    10, 10558, 26809, 26809, 26809, 26809,\n",
      "        26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "from navermovie_comments import load_comments_image\n",
    "\n",
    "class CommentsImage(torch.utils.data.Dataset):\n",
    "    def __init__(self, large=False, max_len=20):\n",
    "        super(CommentsImage, self).__init__()\n",
    "        # use only this tokenizer\n",
    "        tokenize = 'soynlp_unsup'\n",
    "\n",
    "        self.X, labels, self.idx_to_vocab = load_comments_image(\n",
    "            large=large, tokenize=tokenize, max_len=max_len)\n",
    "\n",
    "        # {0: negative, 1: neutral, 2: positive}\n",
    "        labels[np.where(labels < 4)[0]] = 0\n",
    "        labels[np.where((labels >= 4) & (labels < 8))] = 1\n",
    "        labels[np.where(8 <= labels)[0]] = 2\n",
    "        self.labels = labels\n",
    "\n",
    "        # transform numpy.ndarray to torch.tensor\n",
    "        self.X = torch.LongTensor(self.X)\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.labels[index]\n",
    "        return X, y\n",
    "\n",
    "comments_images = CommentsImage()\n",
    "\n",
    "print(len(comments_images))\n",
    "\n",
    "X, y = comments_images[3]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comments_images 를 DataLoader 에 입력할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  590,   699,   126,   188,   205,     5,   126,    13, 26809, 26809,\n",
      "         26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809],\n",
      "        [  493,    14,   757,    65,   108,  1580,   590,   699,   126,   272,\n",
      "          8965,   154,     7,   851,   575,  8517,     6,   620,   953,   104],\n",
      "        [  126,  1029,   263,   536,   220,   245, 11327,   312,   120,     8,\n",
      "            13,  6743,  4254, 26809, 26809, 26809, 26809, 26809, 26809, 26809],\n",
      "        [  224,   497,    71,  2430,    10, 10558, 26809, 26809, 26809, 26809,\n",
      "         26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809, 26809]])\n",
      "tensor([2, 2, 2, 2])\n",
      "torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "comments_loader = torch.utils.data.DataLoader(\n",
    "    comments_images, batch_size=4, shuffle=False)\n",
    "\n",
    "for X, y in comments_loader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print(X.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for large size\n",
    "\n",
    "만약 이미지 파일처럼 각각의 instance 가 파일로 나뉘어져 있을 경우에는 index 별 file path 를 저장한 뒤, getitem 에서 각 파일을 loading 하는 함수로 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryFriendlyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_to_file):\n",
    "        super(MemoryFriendlyDataset, self).__init__()\n",
    "        self.index_to_file = index_to_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_to_file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.index_to_file[index]\n",
    "        X, y = self._load_instance(path)\n",
    "\n",
    "    def _load_instance(self, path):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader 에 대한 설명은 다음의 포스트를 참고하세요.\n",
    "\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
