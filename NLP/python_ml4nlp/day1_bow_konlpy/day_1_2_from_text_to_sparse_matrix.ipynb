{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn (sklearn) 은 다양한 머신러닝 알고리즘 및 데이터 처리 모듈들을 가지고 있는 툴킷입니다. sklearn 의 CountVectorizer 를 이용하여 term frequency matrix 를 만드는 법을 연습합니다. \n",
    "\n",
    "이를 위해서 작은 영화 데이터셋을 이용합니다. sys.path 는 각 Ipython notebook 파일마다 추가해야 합니다. 항상 sys.path.append 하는 것이 번거롭다면 [링크](https://wikidocs.net/29)의 내용을 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from config import dataset_dir\n",
    "import sys\n",
    "sys.path.append(dataset_dir)\n",
    "\n",
    "from lovit_textmining_dataset.navermovie_comments import load_movie_comments\n",
    "\n",
    "idxs, texts, rates = load_movie_comments(large=False, tokenize=None, num_doc=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. From corpus to noun term frequency sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter.nouns(sent) 는 주어진 sent 에서 명사를 추출하여 return 합니다. type 은 list of str 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이건', '테스트']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "komoran.nouns('이건 테스트입니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 배운 collections.Counter 를 이용하여 한 번에 명사의 개수를 세어 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nouns: 1469\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "noun_counter = Counter(\n",
    "    [noun for text in texts for noun in komoran.nouns(text)]\n",
    ")\n",
    "\n",
    "print('num of nouns: %d' % len(noun_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term frequency matrix 를 만들 때, 거의 등장하지 않는 단어들은 의미가 없습니다. min count 를 설정하여 각 threshold 별로 몇 개의 명사가 살아남는지 알아봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nouns (min_count = 2): 626\n",
      "num of nouns (min_count = 3): 395\n",
      "num of nouns (min_count = 5): 219\n",
      "num of nouns (min_count = 10): 93\n"
     ]
    }
   ],
   "source": [
    "for min_count in [2, 3, 5, 10]:\n",
    "    _counter = {\n",
    "        word for word, freq in noun_counter.items()\n",
    "        if freq >= min_count\n",
    "    }\n",
    "    print('num of nouns (min_count = %d): %d' % (\n",
    "        min_count, len(_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 2번 이상 나오는 626 개의 명사를 이용하여 term frequency matrix 를 만들어 봅니다. 우리는 전체 문서에서의 빈도수가 2 이상인 명사만을 return 하는 토크나이저를 만들 것입니다.\n",
    "\n",
    "Komoran.nouns() 와 custom_tokenizer의 결과가 달라짐을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인셉션', '크리스토퍼', '감독', '신작', '인터스텔라', '이번', '주', '일요일', '완전', '기대', '중']\n",
      "['인셉션', '크리스토퍼', '감독', '신작', '인터스텔라', '이번', '주', '완전', '기대', '중']\n"
     ]
    }
   ],
   "source": [
    "noun_dict = {\n",
    "    word for word, freq in noun_counter.items()\n",
    "    if freq >= 2\n",
    "}\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    return [word for word in komoran.nouns(doc) if word in noun_dict]\n",
    "\n",
    "print(komoran.nouns(texts[1]))\n",
    "print(custom_tokenizer(texts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noun_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer 는 document frequency 를 기준으로 대부분 문서에 등장하거나 거의 등장하지 않는 극단적인 단어들을 제거할 수 있습니다. min_df 와 max_df 를  이용하면 되며, df 는 비율입니다. [0, 1] 사이의 값을 입력해야 합니다. \n",
    "\n",
    "CountVectorizer 의 argument tokenizer 는 str 형식의 문장이 들어왔을 때 이를 list of str 형식의 단어열로 변형하는 함수입니다. 기본값은 lambda x:x.split()으로 띄어쓰기 기준으로 단어를 나눕니다. 우리가 만든 custom_tokenizer 를 이용하여 빈도수가 2 이상인 명사들만을 return 하는 토크나이저로 이를 대채합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    min_df=0.005,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "x_sparse = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_sparse 는 scipy 의 sparse matrix 입니다. Sparse matrix 는 0 값이 많은 행렬에 대하여 0 이 아닌 (i, j) 의 값만을 저장한 matrix 를 의미합니다. Term frequency matrix 는 very sparse matrix 에 속하므로 dense matrix 보다 sparse matrix 가 훨씬 효율적입니다. dense matrix 는 문서의 양이 조금만 커도 매우 큰 메모리가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_sparse 는 1,000 개의 리뷰가 211 개의 단어로 표현된 형태입니다. 이는 우리가 min_df = 0.005 로 설정하였기 때문입니다. Document frequency 가 4 이하인 단어들도 제거되었기 때문입니다.\n",
    "\n",
    "또한 이 sparse matrix 는 CSR matrix 형식입니다. sparse matrix 의 형식은 이후에 다시 설명하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1000x211 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3319 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(x_sparse))\n",
    "x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CountVectorizer 에서 각 column 에 해당하는 term 알아내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer.vocabulary_ 는 dict 형식으로, {word:index} 입니다. \n",
    "\n",
    "    {'가장': 0,\n",
    "     '가족': 1,\n",
    "     '가치': 2,\n",
    "     '각본': 3,\n",
    "     '간다': 4,\n",
    "     '감': 5,\n",
    "     '감독': 6,\n",
    "     ...\n",
    "    }\n",
    "\n",
    "이를 이용하여 vocab to index, index to vocab 을 만들어 봅니다. 영화라는 단어가 115 에 해당합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2int = vectorizer.vocabulary_\n",
    "vocab2int['영화']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index 는 0 부터 시작합니다. 그러므로 int2vocab 은 dictionary 형태보다 list 형태로 가지고 있어도 좋습니다. 이를 위해 sorted 함수를 이용합니다. 115 번째 단어가 '영화'임을 다시 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'영화'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2vocab = [\n",
    "    word for word,index in sorted(\n",
    "        vocab2int.items(), key=lambda x:x[1])\n",
    "]\n",
    "int2vocab[115]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sparse matrix I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy.io 는 sparse matrix 를 읽고 쓰는 기능을 제공합니다. mmwrite 는 sparse matrix 를 filepath 에 저장합니다. 그런데 filepath 의 경로에 directory 가 존재하지 않는다면 오류가 발생합니다. 이를 방지하기 위해서 path 가 존재하는지 확인하고, directory 가 존재하지 않을 경우 이를 만들어야 합니다.\n",
    "\n",
    "    if not os.path.exists('tmp/'):\n",
    "        os.makedirs('tmp')\n",
    "\n",
    "위의 코드는 tmp 폴더가 있는지 확인하고, 존재하지 않는 경우 tmp 폴더를 만드는 코드입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import mmwrite, mmread\n",
    "\n",
    "if not os.path.exists('tmp/'):\n",
    "    os.makedirs('tmp')\n",
    "\n",
    "mmwrite('./tmp/x.mm', x_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mmread 는 filepath 의 sparse matrix 를 읽습니다. 그런데 mmread 의 형식은 COO matrix 입니다. 이를 CSR matrix 로 바꾸기 위해 tocsr 함수를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x211 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3319 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_x_sparse = mmread('./tmp/x.mm').tocsr()\n",
    "loaded_x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save vectorizer and vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer 는 pickling 으로 저장하여도 좋습니다. vocabulary list 는 따로 list 파일로 저장해도 좋습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./tmp/vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "with open('./tmp/vocabulary.txt', 'w', encoding='utf-8') as f:\n",
    "    for vocab in int2vocab:\n",
    "        f.write('%s\\n' % vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "with open('./tmp/vectorizer.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "\n",
    "print(loaded_vectorizer.vocabulary_['영화'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11월\n",
      "가족\n",
      "가치\n",
      "각본\n",
      "간\n"
     ]
    }
   ],
   "source": [
    "with open('./tmp/vocabulary.txt', encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
