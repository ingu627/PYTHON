{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에 구현해볼 예제는 다음과 같이 키워드를 정의하고, 이를 추출하는 함수입니다. 키워드는 관점이 주어졌을 때, 그 관점에서 더 자주 등장하는 단어로 정의할 수 있습니다. 예를 들어 여름 철 평상시에 뉴스에서 ‘폭우’가 0.1% 등장하는데, 오늘의 뉴스에서 ‘폭우’가 1% 등장하였다면, ‘폭우’는 오늘 뉴스의 키워드입니다. 이를 다음과 같이 공식화 할 수 있습니다. \n",
    "\n",
    "    score(w) = P(w|Dt) / { P(w|Dt) + P(w|Dr) }\n",
    "\n",
    "    P(w|Dt): target document에서 단어 w가 출현한 비율\n",
    "    P(w|Dr): reference document에서 단어 w가 출현한 비율\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016-10-20 일 뉴스에 명사 추출기를 이용하여 명사만을 걸러낸 document - term matrix 를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.49\n",
      "added lovit_textmining_dataset\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from navernews_10days import get_bow\n",
    "\n",
    "x, _idx_to_vocab, _vocab_to_idx = get_bow(tokenize='noun', date='2016-10-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5537"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vocab_to_idx(word):\n",
    "    return _vocab_to_idx.get(word, -1)\n",
    "\n",
    "def idx_to_vocab(idx):\n",
    "    if 0 <= idx < len(_idx_to_vocab):\n",
    "        return _idx_to_vocab[idx]\n",
    "    return None\n",
    "\n",
    "vocab_to_idx('아이오아이')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'아이오아이'라는 단어가 포함된 문서 집합을 positive_documents로, 그렇지 않은 문서 집합을 negative_documents로 둡니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n pos = 97, n neg = 29994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word = '아이오아이'\n",
    "word_idx = vocab_to_idx(word)\n",
    "positive_documents = x[:,word_idx].nonzero()[0]\n",
    "negative_documents = np.asarray(\n",
    "    [i for i in range(x.shape[0]) if not (i in positive_documents)]\n",
    ")\n",
    "\n",
    "print('n pos = {}, n neg = {}'.format(len(positive_documents), len(negative_documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse matrix x에서 sum()을 하면 모든 값의 합이 구해집니다. sum(axis=0)을 하면 rows가 하나의 row로 합쳐지는 sum이며, sum(axis=1)을 하면 columns가 하나의 column으로 합쳐지는 sum입니다. 우리의 x는 (document by term) matrix이기 때문에 row sum을 하면 모든 문서에서의 단어들의 빈도수 합이 구해집니다. 그래서 (30091 by 9774)의 term frequency matrix가 9774 차원의 term frequency vector가 되었음을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30091, 9774)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9774)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy.sparse 의 matrix는 slicing이 가능합니다. positive_documents를 list 형식으로 만들었습니다. 이 list를 x에 넣어서 x[list,:] 을 실행하면 list에 해당하는 모든 row들을 잘라서 submatrix를 만듭니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6884,  6897,  6956,  7338,  7345,  7582,  8011,  8053,  9180,\n",
       "        9228,  9494,  9539,  9876,  9894, 13059, 13231, 13691, 13856,\n",
       "       14117, 15573, 15836, 15868, 15880, 16198, 16485, 16487, 16489,\n",
       "       16490, 16492, 17304], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_documents[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positive_documents, 즉 '아이오아이'라는 단어가 들어간 문서들만을 잘라내어 submatrix를 만든 뒤, 이를 row sum (= sum(axis=0))을 하였습니다. '아이오아이'라는 단어가 들어간 문서의 단어 빈도수가 만들어집니다. \n",
    "\n",
    "    positive_proportion = x[positive_documents,:].sum(axis=0)\n",
    "\n",
    "이를 list로 만든 뒤, 출력해보면 다음과 같이 term frequency list가 만들어졌음을 볼 수 있습니다. 길이는 단어의 개수와 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9774,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_proportion = np.asarray(x[positive_documents,:].sum(axis=0))[0]\n",
    "positive_proportion.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 합을 \\_sum 이라는 변수로 만든 뒤, 모든 빈도수를 이 \\_sum으로 나누어주면 positive documents, 즉 '아이오아이'가 포함된 문서에서의 단어들의 출현 비율이 만들어집니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00010782, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.00010782, 0.00021563,\n",
       "       0.        , 0.00032345, 0.        , 0.00204852, 0.00010782,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_proportion = positive_proportion / positive_proportion.sum()\n",
    "positive_proportion[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 과정을 반복할 것이니 to_proportion(documents_list) 라는 함수로 만들어 둡니다. \n",
    "\n",
    "positive proportion은 '아이오아이'가 포함된 문서에서의 단어 출현 비율, negative proportion은 '아이오아이'가 포함되지 않은 문서에서의 단어 출현 비율입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_proportion(documents_list):\n",
    "    proportion = np.asarray(x[documents_list,:].sum(axis=0))[0]\n",
    "    proportion = proportion / proportion.sum()\n",
    "    return proportion\n",
    "\n",
    "positive_proportion = to_proportion(positive_documents)\n",
    "negative_proportion = to_proportion(negative_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상대적 출현 비율은 모든 단어들에 대하여 p / (p+n) 을 계산하면 됩니다. p는 한 단어의 positive proportion의 값이며, n은 그 단어의 negative proportion의 값입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proportion_ratio(pos, neg):\n",
    "    assert len(pos) == len(neg)\n",
    "    ratio = pos / (pos + neg)\n",
    "    ratio = np.nan_to_num(ratio)\n",
    "    return ratio\n",
    "\n",
    "keyword_score = proportion_ratio(positive_proportion, negative_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.8844969 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.54934511, 0.56280303,\n",
       "       0.        , 0.82732507, 0.        , 0.87481523, 0.54733158,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_score[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 proportion ratio가 높은 단어들을 찾아봅니다. enumerate를 이용하면 점수가 높은 단어의 index와 그 점수를 (단어, 점수) pair로 만들 수 있습니다. \n",
    "\n",
    "    enumerate(keyword_score)\n",
    "\n",
    "이를 점수 기준으로 정렬하면 점수 순 정렬이 됩니다. \n",
    "\n",
    "    sorted(enumerate(keyword_score), key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4309, 1.0),\n",
       " (5537, 1.0),\n",
       " (2308, 0.9999606090273322),\n",
       " (5333, 0.998991194480233),\n",
       " (6145, 0.9989863725521622),\n",
       " (921, 0.9982710816259988),\n",
       " (2466, 0.9981432513884275),\n",
       " (5880, 0.9978307775631691),\n",
       " (4815, 0.9978210421997837),\n",
       " (3682, 0.9975836317984187),\n",
       " (6208, 0.9973594469004617),\n",
       " (4701, 0.9963128215975839),\n",
       " (4441, 0.9958319893090414),\n",
       " (7832, 0.9948644479894773),\n",
       " (9113, 0.9946890725030576),\n",
       " (6037, 0.9938200380735884),\n",
       " (8976, 0.9934422266805437),\n",
       " (7126, 0.9929667382454291),\n",
       " (4546, 0.9909673401797572),\n",
       " (4859, 0.9908127932033489),\n",
       " (5879, 0.9907514986652862),\n",
       " (1103, 0.99017203825805),\n",
       " (6904, 0.9884164745297143),\n",
       " (6584, 0.9881439461828352),\n",
       " (4343, 0.9880894585465715),\n",
       " (4700, 0.9875086307696628),\n",
       " (8721, 0.9869906112674688),\n",
       " (8651, 0.9867835556082788),\n",
       " (4035, 0.98596911773225),\n",
       " (2120, 0.9853881990008125)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(enumerate(keyword_score), key=lambda x:-x[1])[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 term frequency vector를 만들었습니다. 이도 list로 만들어 둡니다. 키워드/연관어를 추출할 때, 최소 빈도수를 설정하기 위해서입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 462,  769,   48,   59,   54,  332,  499,   52,   40, 2683,  195,\n",
       "         40,   49,  144,   55,  323,   45,  222,  246,  466,  289,  190,\n",
       "         37,  831,  248,   91,   78,   53,   57,   49], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequency = np.asarray(x.sum(axis=0))[0]\n",
    "term_frequency[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 과정을 proportion ratio keyword로 감싸서 함수로 만들어 둡니다. min count와 단어를 입력받도록 합니다. \n",
    "\n",
    "term frequency matrix 에 포함되지 않은 단어면 키워드분석을 하지 않습니다. \n",
    "\n",
    "    word_idx = word2int(word)\n",
    "        if word_idx == -1:\n",
    "            return None\n",
    "            \n",
    "min count cutting을 통해서 최소 빈도수 이상인 단어들만 available terms로 만들어 둡니다. \n",
    "\n",
    "    term_frequency = x.sum(axis=0).tolist()[0]\n",
    "    available_terms = {term:count for term, count in enumerate(term_frequency) if count >= min_count}\n",
    "    \n",
    "그 뒤 positive_documents / negative_documents를 선택하고, positive_proportion / negative_proportion 를 계산한 뒤, proportion_ratio를 계산합니다. \n",
    "\n",
    "    positive_documents = x[:,word_idx].nonzero()[0].tolist()\n",
    "    positive_proportion = to_proportion(positive_documents)\n",
    "    ...\n",
    "    keyword_score = proportion_ratio(positive_proportion, negative_proportion)\n",
    "    \n",
    "최소빈도수 이상으로 등장한 단어만을 keyword로 남겨두는 filtering을 합니다. filter 함수를 써도 좋습니다.\n",
    "\n",
    "    keyword_score = [(term, score) for term, score in keyword_score if term in available_terms]\n",
    "\n",
    "word index로 표현되어 있는 keyword_score = [(idx, score), ... ]를 [(word, score), ...]로 바꿔줍니다. \n",
    "\n",
    "    keyword_score = [(int2word(term), score) for term, score in keyword_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proportion_ratio_keyword(word, x, min_count=10):\n",
    "    idx = vocab_to_idx(word)\n",
    "    if idx == -1:\n",
    "        return None\n",
    "    \n",
    "    term_frequency = np.asarray(x.sum(axis=0))[0]\n",
    "    available_terms = set(np.where(term_frequency >= min_count)[0])\n",
    "\n",
    "    positive_documents = x[:,idx].nonzero()[0]\n",
    "    negative_documents = np.asarray([i for i in range(x.shape[0]) if not (i in positive_documents)])\n",
    "\n",
    "    positive_proportion = to_proportion(positive_documents)\n",
    "    negative_proportion = to_proportion(negative_documents)\n",
    "    \n",
    "    keyword_score = proportion_ratio(positive_proportion, negative_proportion)\n",
    "    keyword_score = sorted(enumerate(keyword_score), key=lambda x:-x[1])\n",
    "    keyword_score = [(term, score) for term, score in keyword_score if term in available_terms]\n",
    "    keyword_score = [(idx_to_vocab(term), score) for term, score in keyword_score]\n",
    "\n",
    "    return keyword_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('빅브레인', 1.0),\n",
      " ('아이오아이', 1.0),\n",
      " ('너무너무너무', 0.9999606090273322),\n",
      " ('신용재', 0.998991194480233),\n",
      " ('오블리스', 0.9989863725521622),\n",
      " ('갓세븐', 0.9982710816259988),\n",
      " ('다비치', 0.9981432513884275),\n",
      " ('엠카운트다운', 0.9978307775631691),\n",
      " ('세븐', 0.9978210421997837),\n",
      " ('박진영', 0.9975836317984187),\n",
      " ('완전체', 0.9973594469004617),\n",
      " ('선의', 0.9963128215975839),\n",
      " ('산들', 0.9958319893090414),\n",
      " ('중독성', 0.9948644479894773),\n",
      " ('프로듀스101', 0.9946890725030576),\n",
      " ('열창', 0.9938200380735884),\n",
      " ('펜타곤', 0.9934422266805437),\n",
      " ('잠깐', 0.9929667382454291),\n",
      " ('상큼', 0.9909673401797572),\n",
      " ('소녀들', 0.9908127932033489),\n",
      " ('엠넷', 0.9907514986652862),\n",
      " ('걸크러쉬', 0.99017203825805),\n",
      " ('일산동구', 0.9884164745297143),\n",
      " ('음악방송', 0.9881439461828352),\n",
      " ('사나', 0.9880894585465715),\n",
      " ('선율', 0.9875086307696628),\n",
      " ('타이틀곡', 0.9869906112674688),\n",
      " ('코드', 0.9867835556082788),\n",
      " ('본명', 0.98596911773225),\n",
      " ('깜찍', 0.9853881990008125)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "keywords = proportion_ratio_keyword('아이오아이', x, min_count=30)\n",
    "pprint(keywords[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('최순실', 1.0),\n",
      " ('게이트', 0.9981018054860111),\n",
      " ('정유라', 0.9949748004314443),\n",
      " ('연설문', 0.9900718598746623),\n",
      " ('모녀', 0.9875768099004291),\n",
      " ('승마', 0.9872307511905503),\n",
      " ('개명', 0.986641026457457),\n",
      " ('비선', 0.985018930232134),\n",
      " ('더블루케이', 0.9838995868457685),\n",
      " ('실세', 0.9823312201845503),\n",
      " ('스포츠재단', 0.980984809482314),\n",
      " ('최씨', 0.9802224596736517),\n",
      " ('최경희', 0.980172024643097),\n",
      " ('비덱', 0.9794924174652362),\n",
      " ('이화여대', 0.9792281858488985),\n",
      " ('특혜', 0.9775213977582151),\n",
      " ('미르재단', 0.9774516345256685),\n",
      " ('의혹들', 0.977198367560925),\n",
      " ('학점', 0.976567846725211),\n",
      " ('비선실세', 0.9747618098586102),\n",
      " ('이대', 0.9713049096885505),\n",
      " ('미르', 0.9697354303371427),\n",
      " ('재단', 0.9665692895878129),\n",
      " ('정유라씨', 0.9651208193465403),\n",
      " ('엄정', 0.9635099910913556),\n",
      " ('차은택', 0.9630949366283257),\n",
      " ('이화', 0.962975945484486),\n",
      " ('국정조사', 0.9614360445588696),\n",
      " ('사퇴', 0.961117249105005),\n",
      " ('의혹', 0.9610013059869946)]\n"
     ]
    }
   ],
   "source": [
    "pprint(proportion_ratio_keyword('최순실', x, min_count=100)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
