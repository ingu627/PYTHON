{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로딩\n",
    "\n",
    "샘플로 1000 개의 뉴스 기사만 이용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soynlp=0.0.49\n",
      "added lovit_textmining_dataset\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "\n",
    "from navernews_10days import get_news_paths\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "\n",
    "path = get_news_paths(tokenize='komoran', date='2016-10-20')\n",
    "corpus = DoublespaceLineCorpus(path, iter_sent=True, num_sent=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build bigram dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram_extractor 는 min_count 이상인 bigram 만을 선택하는 extractor 입니다. \n",
    "\n",
    "to_bigram 은 두 개의 list 를 zip 으로 묶어서 bigram list 을 만드는 함수입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bigram_extractor(sents, min_count=10):\n",
    "\n",
    "    def to_bigram(tokens):\n",
    "        bigrams = [(t0, t1) for t0, t1 in zip(tokens, tokens[1:])]\n",
    "        return bigrams\n",
    "\n",
    "    unigram_counter = Counter(word for sent in sents for word in sent.split())\n",
    "    bigram_counter = Counter(\n",
    "        (bigram for sent in sents for bigram in to_bigram(sent.split()))\n",
    "    )\n",
    "\n",
    "    bigram_dictionary = {\n",
    "        bigram:count for bigram, count in bigram_counter.items()\n",
    "        if count >= min_count and '/NN' in bigram[0]\n",
    "    }\n",
    "\n",
    "    def score(bigram, count):\n",
    "        a = unigram_counter[bigram[0]]\n",
    "        b = unigram_counter[bigram[1]]\n",
    "        s = (count - min_count) / (a * b)\n",
    "        return s\n",
    "\n",
    "    bigram_score = {\n",
    "        bigram:score(bigram, count) for bigram, count in bigram_dictionary.items()\n",
    "    }\n",
    "\n",
    "    return bigram_score\n",
    "\n",
    "bigrams = bigram_extractor(corpus)\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram 을 살펴봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('자료/NNG', '사진/NNP'), 0.010714285714285714),\n",
       " (('국제/NNG', '사회/NNG'), 0.008415147265077139),\n",
       " (('확장/NNG', '억제/NNG'), 0.0084090600840906),\n",
       " (('공동/NNG', '성명/NNG'), 0.007918552036199095),\n",
       " (('핵/NNG', '미사일/NNG'), 0.006572295247724975),\n",
       " (('미사일/NNG', '위협/NNG'), 0.006521739130434782),\n",
       " (('외교/NNG', '국방장관/NNG'), 0.005853994490358127),\n",
       " (('외교/NNG', '국방/NNP'), 0.004662004662004662),\n",
       " (('억제/NNG', '전략/NNP'), 0.004641089108910891),\n",
       " (('수상전/NNP', '센터/NNP'), 0.004201680672268907)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bigrams.items(), key=lambda x:-x[1])[5:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigram tokenizer\n",
    "\n",
    "BigramTokenizer 는 base tokenizer 인 tagger 와 bigrams 를 입력받는 class 입니다. \n",
    "\n",
    "\\_\\_call\\_\\_() 함수를 구현하면 클래스도 함수처럼 호출할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BigramTokenizer:\n",
    "\n",
    "    def __init__(self, bigrams, tokenize=lambda x:x.split()):\n",
    "        self.bigrams = bigrams\n",
    "        self.tokenize = tokenize\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        if not sent:\n",
    "            return []\n",
    "\n",
    "        unigrams = self.tokenize(sent)\n",
    "\n",
    "        bigrams = [(t0, t1) for t0, t1 in zip(unigrams, unigrams[1:])]\n",
    "        bigrams = [bigram for bigram in bigrams if bigram in self.bigrams]\n",
    "        bigrams = ['%s-%s' % (t0, t1) for t0, t1 in bigrams]\n",
    "\n",
    "        return unigrams + bigrams\n",
    "\n",
    "bigram_tokenizer = BigramTokenizer(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플로 하나의 문장을 선택하여 토크나이징을 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"오패산터널/NNP 총격전/NNG 용의자/NNP 검거/NNG 서울/NNP 연합뉴스/NNP 경찰/NNG 관계자/NNG 들/XSN 이/JKS 19/SN 일/NNB 오후/NNG 서울/NNP 강북구/NNP 오/NNP 패사/NNG ㄴ/JX 터널/NNP 인근/NNG 에서/JKB 사제/NNP 총기/NNG 를/JKO 발사/NNG 하/XSV 아/EC 경찰/NNG 을/JKO 살해/NNG 하/XSV ㄴ/ETM 용의자/NNP 성모/NNP 씨/NNB 를/JKO 검거/NNG 하/XSV 고/EC 있/VV 다/EC 성씨/NNP 는/JX 검거/NNG 당시/NNG 서바이벌/NNP 게임/NNG 에서/JKB 쓰/VV 는/ETM 방탄/NNP 조끼/NNP 에/JKB 헬멧/NNP 까지/JX 착용/NNG 하/XSV ㄴ/ETM 상태/NNG 이/VCP 었/EP 다/EC 독자/NNG 제공/NNG 영상/NNP 캡처/NNP 연합뉴스/NNP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram_tokenizer(sent) 로 클래스를 호출합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오패산터널/NNP',\n",
       " '총격전/NNG',\n",
       " '용의자/NNP',\n",
       " '검거/NNG',\n",
       " '서울/NNP',\n",
       " '연합뉴스/NNP',\n",
       " '경찰/NNG',\n",
       " '관계자/NNG',\n",
       " '들/XSN',\n",
       " '이/JKS',\n",
       " '19/SN',\n",
       " '일/NNB',\n",
       " '오후/NNG',\n",
       " '서울/NNP',\n",
       " '강북구/NNP',\n",
       " '오/NNP',\n",
       " '패사/NNG',\n",
       " 'ㄴ/JX',\n",
       " '터널/NNP',\n",
       " '인근/NNG',\n",
       " '에서/JKB',\n",
       " '사제/NNP',\n",
       " '총기/NNG',\n",
       " '를/JKO',\n",
       " '발사/NNG',\n",
       " '하/XSV',\n",
       " '아/EC',\n",
       " '경찰/NNG',\n",
       " '을/JKO',\n",
       " '살해/NNG',\n",
       " '하/XSV',\n",
       " 'ㄴ/ETM',\n",
       " '용의자/NNP',\n",
       " '성모/NNP',\n",
       " '씨/NNB',\n",
       " '를/JKO',\n",
       " '검거/NNG',\n",
       " '하/XSV',\n",
       " '고/EC',\n",
       " '있/VV',\n",
       " '다/EC',\n",
       " '성씨/NNP',\n",
       " '는/JX',\n",
       " '검거/NNG',\n",
       " '당시/NNG',\n",
       " '서바이벌/NNP',\n",
       " '게임/NNG',\n",
       " '에서/JKB',\n",
       " '쓰/VV',\n",
       " '는/ETM',\n",
       " '방탄/NNP',\n",
       " '조끼/NNP',\n",
       " '에/JKB',\n",
       " '헬멧/NNP',\n",
       " '까지/JX',\n",
       " '착용/NNG',\n",
       " '하/XSV',\n",
       " 'ㄴ/ETM',\n",
       " '상태/NNG',\n",
       " '이/VCP',\n",
       " '었/EP',\n",
       " '다/EC',\n",
       " '독자/NNG',\n",
       " '제공/NNG',\n",
       " '영상/NNP',\n",
       " '캡처/NNP',\n",
       " '연합뉴스/NNP',\n",
       " '서울/NNP-연합뉴스/NNP',\n",
       " '일/NNB-오후/NNG',\n",
       " '오/NNP-패사/NNG',\n",
       " '패사/NNG-ㄴ/JX',\n",
       " '인근/NNG-에서/JKB',\n",
       " '발사/NNG-하/XSV',\n",
       " '성씨/NNP-는/JX',\n",
       " '상태/NNG-이/VCP']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tokenizer(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 CountVectorizer 의 tokenizer 로 입력합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus.iter_sent=False\n",
    "vectorizer = CountVectorizer(tokenizer=bigram_tokenizer)\n",
    "x = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 5,317 개의 uni + bigram 으로 term frequency vector 가 만들어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 5317)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. All bigrams\n",
    "\n",
    "sklearn.feature_extraction.text.CountVectorizer 의 ngram_range=(1, 2) 로 설정한다면, 가능한 모든 종류의 bigram 을 term 으로 이용하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def all_bigram_tokenizer(sent):\n",
    "    if not sent:\n",
    "        return []\n",
    "    return sent.split()\n",
    "\n",
    "vectorizer_ = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    tokenizer=all_bigram_tokenizer\n",
    ")\n",
    "\n",
    "x_ = vectorizer_.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25,748 개의 uni + bigram 을 이용하여 term frequency vector 를 만듭니다. ngram_range 를 이용할 때에는 min_df, max_df 를 이용하여 infrequent bigram 을 제거해야 적절한 크기의 차원을 유지할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 25748)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
