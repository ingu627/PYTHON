{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 종류의 classifiers 의 사용법을 익히기 위하여 앞서 다뤘던 영화 평점 분류 데이터를 이용합니다. Support Vector Machine 계열들은 데이터를 그대로 외우기 때문에 데이터의 크기가 커지면 모델의 크기도 커집니다. 그렇기 때문에 이번 실습에서는 데이터의 개수가 10k 인 작은 데이터를 이용합니다. 긍정 (label = 1), 부정 (label = -1) 의 리뷰 5k 씩으로 구성되어 있는 balanced dataset 입니다.\n",
    "\n",
    "`dataset_dir` 폴더에는 `lovit_textmining_dataset` 패키지가 있고, 그 아래 하위 패키지로 `navermovie_comments` 가 존재합니다. `navermovie_comments` 의 위치를 sys.path 에 추가하면 간략하게 데이터를 로딩할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset version\n",
      "[navermovie_comments.data] is latest (0.0.1)\n",
      "[navermovie_comments.models] is latest (0.0.1)\n",
      "[navernews_10days.data] is latest (0.0.1)\n",
      "[navernews_10days.models] is latest (0.0.1)\n",
      "(10000, 4808)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from config import dataset_dir\n",
    "sys.path.append('{}/lovit_textmining_dataset/'.format(dataset_dir))\n",
    "\n",
    "from navermovie_comments import load_sentiment_dataset\n",
    "\n",
    "texts, x, y, idx_to_vocab = load_sentiment_dataset(data_name='10k', tokenize='komoran')\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize\n",
    "\n",
    "학습용 데이터 x는 row normalize를 미리 해두겠습니다. 기본은 norm='l2' 입니다. 벡터를 확률의 형식으로 (합이 1 이 되도록) 만들고 싶다면 norm 을 'l1' 으로 변경하면 됩니다.\n",
    "\n",
    "```\n",
    "x_norm_l1 = normalize(x, norm='l1')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "x_norm = normalize(x, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "모델의 사용법을 알기 위함이니 학습과 검증 데이터를 따로 분류하지는 않겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.7 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(x_norm, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트는 여러 classifier 를 이용하여 반복할테니 함수로 만들어 둡니다. idxs 입력받아 이들의 true label 과 predicted label 을 확인합니다.\n",
    "\n",
    "test samples 는 부정 리뷰 10 개와 긍정 리뷰 10 개 입니다. 모든 리뷰를 제대로 분류할 수 있는 것은 아닙니다. Unigram 의 한계이거나, 모델 구조의 한계일 수 있습니다. 또한 10k 개의 데이터셋 사이즈의 문제일 수도 있습니다. 각 모델별로 이와 같은 결과가 나오는 이유에 대하여 이해하는 것이 중요합니다.\n",
    "\n",
    "Logistic regression 은 특정 단어가 존재하느냐에 영향을 가장 많이 받습니다. 영화 데이터에서는 10/SN 이 `10점준다` 라는 맥락에서 자주 이용되는 단어입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neg -> pos] 재미없/VA 다/EC 이상/NNG 10/SN 자/NNB\n",
      "[neg -> pos] 다크/NNP 나이트/NNP 라/NNG 이즈/NNP 에/JKB 이/NP 은/JX 놀라/NNP ㄴ/JX 의/JKG 거품/NNG 이/JKS 확실히/MAG 드러나/VV ㄴ/ETM 영화/NNG 시각/NNG 효과/NNG 빼/VV 곤/EC 아무것/NNG 도/JX 보/VV ㄹ게/EC 없/VA 는/ETM 망/NNG 작/NNG\n",
      "[neg -> neg] 시사회/NNP 보고/JKB 꿀/NNG 잠/NNG 불면증/NNP 치료/NNP 에/JKB\n",
      "[neg -> neg] 알/VV 고/EC 보/VX 니/EC 지구/NNG 이/VCP 라는/ETM 설정/NNG 은/JX 혹성탈출/NNP 하/VV 고/EC 똑같/VA 지/EC 않/VX 어/EC 실망/NNP\n",
      "[neg -> pos] 놀라/VV ㄴ/ETM 각본/NNP 은/JX 천재/NNP 이자/NNP 놀라/NNP ㄴ/JX 의/JKG 동생/NNG 조나단/NNP 놀라/VV ㄴ/ETM 영원/NNG 하/XSV ㄴ/ETM 거장/NNG 스티븐 스필버그/NNP 감독/NNG 제작/NNG 참여/NNG 한스/NNP 짐머/NNP 까지/JX OST/SL 하/XSV 아/EC 주/VX ㄴ다면/EC 길/VA ㄴ/ETM 말/NNG 필요/NNG 없이/MAG 일단/MAG 서/VV ㄴ/ETM 감상/NNP 천재/NNG 성/XSN 이/JKS 다분/XR 하/XSA ㄴ/ETM 놀라/VV ㄴ/ETM 형제/NNP 는/JX 지금껏/MAG 실망/NNG 시키/XSV ㄴ/ETM 적/XSN 이/JKS 없/VA 기/ETN 에/JKB 그만큼/MAG 사람/NNG 들/XSN 이/JKS 믿/VV 고/EC 보/VX 는/ETM 것/NNB 같/VA 은/ETM 소리/NNP 하네/NNP 핵/NNG 노/NNP 재/XPN 므/NNP\n",
      "[neg -> neg] ㅅㅂ/NA 이런/MM 영화/NNG 제일/NNG 싫/VA 은데/EC 몰입/NNP 도도/NNP 없/VA 고/EC 흥행/NNG 도/JX 패션왕/NNP 한테/JKB OO/SL 영화/NNG 다/JX 뻔하/VA ㄴ/ETM 영화/NNP 돈/NNG 낭비/NNG ㅉ/NA\n",
      "[neg -> neg] 야/NNG 배급사/NNP 어디/NP 냐/EC 나오/VV 기/ETN 전/NNG 부터/JX 빠/NNG 내/NNB\n",
      "[neg -> neg] 정말/NNP 시간/NNP 만/JX 길/VA ㄴ/ETM 재/VV ㅁ/ETN 없/VA 는/ETM 영화/NNG 능성/NNP 이/JKS 높/VA 음/ETN\n",
      "[neg -> neg] 스필버그/NA 신/NNG 께서/JKS 이런/MM 쓰레기/NNP 영화 제작/NNP 에/JKB 참여/NNG 하/VV 다니/EC ㅜㅜ/NA A/SL I/SL 능가/NNG 하/XSV ㄹ/ETM 작품/NNG 은/JX 없/VA 습니다/EC\n",
      "[neg -> pos] 제가/NNP 시사회/NNP 에서/JKB 보/VV 고/EC 오/VX 았/EP 는데/EC 진짜/MAG 노/NNG 재/VV ㅁ/ETN 이/JKS 더라고요/EC 에/NNG 휴/NNP 놀라/NNP ㄴ/JX 도/JX ㄴ/ETM 거/NNB 에/JKB 놀라/VV ㅂ니다/EC\n",
      "[pos -> pos] 크리스토퍼/NNP 놀라/VV ㄴ/ETM 에게/JKB 우리/NP 는/JX 놀라/VV ㄴ/ETM 다/MAG\n",
      "[pos -> pos] 인셉션/NNP 정말/MAG 흥미진진/XR 하/XSA 게/EC 보/VV 았었/EP 고/EC 크리스토퍼/NNP 놀라/VV ㄴ/ETM 감독/NNG 님/XSN 신작/NNP 인터스텔라/NNP 도/JX 이번/NNG 주/NNP 일요일/NNG 에/JKB 보/VV 러/EC 가/VX ㅂ니다/EC 완전/NNG 기대/NNG 중/NNB\n",
      "[pos -> pos] 놀라/VV ㄴ/ETM 이면/NNP 무조건/MAG 보/VV 아야/EC 되/VV ㄴ다/EC 왜냐하면/MAJ 모든/MM 작품/NNG 을/JKO 다/MAG 히트/NNP 치/VV 었/EP 으니깐/EC\n",
      "[pos -> pos] 나/NP 는/JX 감탄/NNG 하/XSV ㄹ/ETM 준비/NNG 가/JKS 되/VV 어/EC 있/VX 다/EC\n",
      "[pos -> neg] 얘/NP 들/XSN 아/JKV 오늘/NNG 나오/VV 는/ETM 거지/NNG 밤/NNG 에/JKB ㅋㅋ/NA 오늘/NNG\n",
      "[pos -> neg] 이제/MAG 죽/VV 어도/EC 이/JKS 없/VA 다/EC\n",
      "[pos -> pos] 기대감/NNG 에/JKB 잠도/NNP 안/MAG 오/VV ㄴ다/EC\n",
      "[pos -> pos] 나/NP 랑/JKB 같이/MAG 보/VV 아/EC 주/VX ㄹ까/EC ㅎ/NA\n",
      "[pos -> pos] 우선/MAG 명/NNB 감독/NNP 들/XSN 이/JKS 모이/VV 었/EP 고/EC 미국/NNP 에서/JKB 도/JX 극찬/NNG 을/JKO 받/VV 았/EP 더군/EC\n",
      "[pos -> pos] 드디어/MAG 내일/NNG 꼭/MAG 보/VV ㄴ다/EC\n"
     ]
    }
   ],
   "source": [
    "def test(classifier, x, y, texts, idxs):\n",
    "    predicts = classifier.predict(x[idxs])\n",
    "    texts_ = [texts[idx] for idx in idxs]\n",
    "    trues = y[idxs]    \n",
    "    for text, true, pred in zip(texts_, trues, predicts):\n",
    "        text_strf = ' '.join(text)\n",
    "        true_strf = 'pos' if true == 1 else 'neg'\n",
    "        pred_strf = 'pos' if pred == 1 else 'neg'\n",
    "        print('[{} -> {}] {}'.format(true_strf, pred_strf, text_strf))\n",
    "\n",
    "test_samples = np.asarray(list(range(0,10)) + list(range(5000, 5010)))\n",
    "test(logistic_regression, x, y, texts, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression의 coefficient를 살펴보는 것은 이전에 다뤘습니다. 긍정, 부정을 의미하는 단어들을 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관람객/NNG : 7.07\n",
      "놀라/VV : 4.77\n",
      "최고/NNG : 4.43\n",
      "10/SN : 3.81\n",
      "인생/NNP : 3.46\n",
      "필요/NNG : 3.3\n",
      "롭/XSA : 3.04\n",
      "재미있/VA : 3.02\n",
      "우주/NNG : 2.96\n",
      "인터스텔라/NNP : 2.87\n"
     ]
    }
   ],
   "source": [
    "# Coefficients를 array로 만듭니다. \n",
    "coefficients = logistic_regression.coef_.reshape(-1)\n",
    "\n",
    "# Coefficient를 (index, coef)로 만든 뒤, coefficient 의 크기로 내림차순 정렬합니다. \n",
    "sorted_coefficients = sorted(enumerate(coefficients), key=lambda x:-x[1])\n",
    "\n",
    "# 긍정적인 리뷰에 자주 등장하는 단어들을 다시 살펴봅니다.\n",
    "for j, coef in sorted_coefficients[:10]:\n",
    "    print('{} : {:.3}'.format(idx_to_vocab[j], coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron Classifier\n",
    "\n",
    "Feedforward neural network 의 sklearn 의 이름이며, NN 은 parameter 를 해석하기가 조금 복잡합니다. 하지만 학습 방법은 Logistic Regression 과 동일합니다. \n",
    "\n",
    "Neural network 의 경우, hidden laber 의 층 수와, 각 층의 노드 수를 정해줄 수 있습니다. 아래 예시처럼 hidden layer 를 정하면, 2 개의 layers 에 각각 20, 5 개의 nodes 를 이용한다는 의미입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neg -> neg] 재미없/VA 다/EC 이상/NNG 10/SN 자/NNB\n",
      "[neg -> neg] 다크/NNP 나이트/NNP 라/NNG 이즈/NNP 에/JKB 이/NP 은/JX 놀라/NNP ㄴ/JX 의/JKG 거품/NNG 이/JKS 확실히/MAG 드러나/VV ㄴ/ETM 영화/NNG 시각/NNG 효과/NNG 빼/VV 곤/EC 아무것/NNG 도/JX 보/VV ㄹ게/EC 없/VA 는/ETM 망/NNG 작/NNG\n",
      "[neg -> neg] 시사회/NNP 보고/JKB 꿀/NNG 잠/NNG 불면증/NNP 치료/NNP 에/JKB\n",
      "[neg -> neg] 알/VV 고/EC 보/VX 니/EC 지구/NNG 이/VCP 라는/ETM 설정/NNG 은/JX 혹성탈출/NNP 하/VV 고/EC 똑같/VA 지/EC 않/VX 어/EC 실망/NNP\n",
      "[neg -> neg] 놀라/VV ㄴ/ETM 각본/NNP 은/JX 천재/NNP 이자/NNP 놀라/NNP ㄴ/JX 의/JKG 동생/NNG 조나단/NNP 놀라/VV ㄴ/ETM 영원/NNG 하/XSV ㄴ/ETM 거장/NNG 스티븐 스필버그/NNP 감독/NNG 제작/NNG 참여/NNG 한스/NNP 짐머/NNP 까지/JX OST/SL 하/XSV 아/EC 주/VX ㄴ다면/EC 길/VA ㄴ/ETM 말/NNG 필요/NNG 없이/MAG 일단/MAG 서/VV ㄴ/ETM 감상/NNP 천재/NNG 성/XSN 이/JKS 다분/XR 하/XSA ㄴ/ETM 놀라/VV ㄴ/ETM 형제/NNP 는/JX 지금껏/MAG 실망/NNG 시키/XSV ㄴ/ETM 적/XSN 이/JKS 없/VA 기/ETN 에/JKB 그만큼/MAG 사람/NNG 들/XSN 이/JKS 믿/VV 고/EC 보/VX 는/ETM 것/NNB 같/VA 은/ETM 소리/NNP 하네/NNP 핵/NNG 노/NNP 재/XPN 므/NNP\n",
      "[neg -> neg] ㅅㅂ/NA 이런/MM 영화/NNG 제일/NNG 싫/VA 은데/EC 몰입/NNP 도도/NNP 없/VA 고/EC 흥행/NNG 도/JX 패션왕/NNP 한테/JKB OO/SL 영화/NNG 다/JX 뻔하/VA ㄴ/ETM 영화/NNP 돈/NNG 낭비/NNG ㅉ/NA\n",
      "[neg -> neg] 야/NNG 배급사/NNP 어디/NP 냐/EC 나오/VV 기/ETN 전/NNG 부터/JX 빠/NNG 내/NNB\n",
      "[neg -> neg] 정말/NNP 시간/NNP 만/JX 길/VA ㄴ/ETM 재/VV ㅁ/ETN 없/VA 는/ETM 영화/NNG 능성/NNP 이/JKS 높/VA 음/ETN\n",
      "[neg -> neg] 스필버그/NA 신/NNG 께서/JKS 이런/MM 쓰레기/NNP 영화 제작/NNP 에/JKB 참여/NNG 하/VV 다니/EC ㅜㅜ/NA A/SL I/SL 능가/NNG 하/XSV ㄹ/ETM 작품/NNG 은/JX 없/VA 습니다/EC\n",
      "[neg -> neg] 제가/NNP 시사회/NNP 에서/JKB 보/VV 고/EC 오/VX 았/EP 는데/EC 진짜/MAG 노/NNG 재/VV ㅁ/ETN 이/JKS 더라고요/EC 에/NNG 휴/NNP 놀라/NNP ㄴ/JX 도/JX ㄴ/ETM 거/NNB 에/JKB 놀라/VV ㅂ니다/EC\n",
      "[pos -> pos] 크리스토퍼/NNP 놀라/VV ㄴ/ETM 에게/JKB 우리/NP 는/JX 놀라/VV ㄴ/ETM 다/MAG\n",
      "[pos -> pos] 인셉션/NNP 정말/MAG 흥미진진/XR 하/XSA 게/EC 보/VV 았었/EP 고/EC 크리스토퍼/NNP 놀라/VV ㄴ/ETM 감독/NNG 님/XSN 신작/NNP 인터스텔라/NNP 도/JX 이번/NNG 주/NNP 일요일/NNG 에/JKB 보/VV 러/EC 가/VX ㅂ니다/EC 완전/NNG 기대/NNG 중/NNB\n",
      "[pos -> pos] 놀라/VV ㄴ/ETM 이면/NNP 무조건/MAG 보/VV 아야/EC 되/VV ㄴ다/EC 왜냐하면/MAJ 모든/MM 작품/NNG 을/JKO 다/MAG 히트/NNP 치/VV 었/EP 으니깐/EC\n",
      "[pos -> pos] 나/NP 는/JX 감탄/NNG 하/XSV ㄹ/ETM 준비/NNG 가/JKS 되/VV 어/EC 있/VX 다/EC\n",
      "[pos -> pos] 얘/NP 들/XSN 아/JKV 오늘/NNG 나오/VV 는/ETM 거지/NNG 밤/NNG 에/JKB ㅋㅋ/NA 오늘/NNG\n",
      "[pos -> pos] 이제/MAG 죽/VV 어도/EC 이/JKS 없/VA 다/EC\n",
      "[pos -> pos] 기대감/NNG 에/JKB 잠도/NNP 안/MAG 오/VV ㄴ다/EC\n",
      "[pos -> pos] 나/NP 랑/JKB 같이/MAG 보/VV 아/EC 주/VX ㄹ까/EC ㅎ/NA\n",
      "[pos -> pos] 우선/MAG 명/NNB 감독/NNP 들/XSN 이/JKS 모이/VV 었/EP 고/EC 미국/NNP 에서/JKB 도/JX 극찬/NNG 을/JKO 받/VV 았/EP 더군/EC\n",
      "[pos -> pos] 드디어/MAG 내일/NNG 꼭/MAG 보/VV ㄴ다/EC\n",
      "CPU times: user 1min 8s, sys: 3min 8s, total: 4min 16s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "neural_network = MLPClassifier(hidden_layer_sizes=(20,5))\n",
    "neural_network.fit(x_norm, y)\n",
    "\n",
    "test(neural_network, x, y, texts, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients 는 coefs_ 에 list 형식으로 저장됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(type(neural_network.coefs_))\n",
    "print(len(neural_network.coefs_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1 : (4808, 20)\n",
      "layer 2 : (20, 5)\n",
      "layer 3 : (5, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, coef_ in enumerate(neural_network.coefs_):\n",
    "    print('layer {} : {}'.format(i+1, coef_.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intercept 는 $h_{t-1} \\times \\beta_t + intercept_t$ 처럼 이전 layer 의 output 값과 $\\beta$ 의 내적에 더해지는 값입니다. Logistic regression 에서의 평행이동과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.0815515 , 0.05732039, 0.43561834, 0.12969709, 0.12500567,\n",
       "        0.12931301, 0.2722803 , 0.08082177, 0.4050362 , 0.06004531,\n",
       "        0.07852185, 0.10111362, 0.19697818, 0.10313602, 0.06761073,\n",
       "        0.09362888, 0.18652071, 0.31463787, 0.50370473, 0.09673633]),\n",
       " array([0.34643458, 0.06569037, 0.00279967, 0.376795  , 0.29684621]),\n",
       " array([0.31228303])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Classifier)\n",
    "\n",
    "SVM은 kernel을 정할 수 있습니다. default는 linear입니다. \n",
    "\n",
    "Linear kernel이 속도도 훨씬 빠릅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neg -> neg] 재미없/VA 다/EC 이상/NNG 10/SN 자/NNB\n",
      "[neg -> pos] 다크/NNP 나이트/NNP 라/NNG 이즈/NNP 에/JKB 이/NP 은/JX 놀라/NNP ㄴ/JX 의/JKG 거품/NNG 이/JKS 확실히/MAG 드러나/VV ㄴ/ETM 영화/NNG 시각/NNG 효과/NNG 빼/VV 곤/EC 아무것/NNG 도/JX 보/VV ㄹ게/EC 없/VA 는/ETM 망/NNG 작/NNG\n",
      "[neg -> neg] 시사회/NNP 보고/JKB 꿀/NNG 잠/NNG 불면증/NNP 치료/NNP 에/JKB\n",
      "[neg -> neg] 알/VV 고/EC 보/VX 니/EC 지구/NNG 이/VCP 라는/ETM 설정/NNG 은/JX 혹성탈출/NNP 하/VV 고/EC 똑같/VA 지/EC 않/VX 어/EC 실망/NNP\n",
      "[neg -> pos] 놀라/VV ㄴ/ETM 각본/NNP 은/JX 천재/NNP 이자/NNP 놀라/NNP ㄴ/JX 의/JKG 동생/NNG 조나단/NNP 놀라/VV ㄴ/ETM 영원/NNG 하/XSV ㄴ/ETM 거장/NNG 스티븐 스필버그/NNP 감독/NNG 제작/NNG 참여/NNG 한스/NNP 짐머/NNP 까지/JX OST/SL 하/XSV 아/EC 주/VX ㄴ다면/EC 길/VA ㄴ/ETM 말/NNG 필요/NNG 없이/MAG 일단/MAG 서/VV ㄴ/ETM 감상/NNP 천재/NNG 성/XSN 이/JKS 다분/XR 하/XSA ㄴ/ETM 놀라/VV ㄴ/ETM 형제/NNP 는/JX 지금껏/MAG 실망/NNG 시키/XSV ㄴ/ETM 적/XSN 이/JKS 없/VA 기/ETN 에/JKB 그만큼/MAG 사람/NNG 들/XSN 이/JKS 믿/VV 고/EC 보/VX 는/ETM 것/NNB 같/VA 은/ETM 소리/NNP 하네/NNP 핵/NNG 노/NNP 재/XPN 므/NNP\n",
      "[neg -> neg] ㅅㅂ/NA 이런/MM 영화/NNG 제일/NNG 싫/VA 은데/EC 몰입/NNP 도도/NNP 없/VA 고/EC 흥행/NNG 도/JX 패션왕/NNP 한테/JKB OO/SL 영화/NNG 다/JX 뻔하/VA ㄴ/ETM 영화/NNP 돈/NNG 낭비/NNG ㅉ/NA\n",
      "[neg -> neg] 야/NNG 배급사/NNP 어디/NP 냐/EC 나오/VV 기/ETN 전/NNG 부터/JX 빠/NNG 내/NNB\n",
      "[neg -> neg] 정말/NNP 시간/NNP 만/JX 길/VA ㄴ/ETM 재/VV ㅁ/ETN 없/VA 는/ETM 영화/NNG 능성/NNP 이/JKS 높/VA 음/ETN\n",
      "[neg -> neg] 스필버그/NA 신/NNG 께서/JKS 이런/MM 쓰레기/NNP 영화 제작/NNP 에/JKB 참여/NNG 하/VV 다니/EC ㅜㅜ/NA A/SL I/SL 능가/NNG 하/XSV ㄹ/ETM 작품/NNG 은/JX 없/VA 습니다/EC\n",
      "[neg -> pos] 제가/NNP 시사회/NNP 에서/JKB 보/VV 고/EC 오/VX 았/EP 는데/EC 진짜/MAG 노/NNG 재/VV ㅁ/ETN 이/JKS 더라고요/EC 에/NNG 휴/NNP 놀라/NNP ㄴ/JX 도/JX ㄴ/ETM 거/NNB 에/JKB 놀라/VV ㅂ니다/EC\n",
      "[pos -> pos] 크리스토퍼/NNP 놀라/VV ㄴ/ETM 에게/JKB 우리/NP 는/JX 놀라/VV ㄴ/ETM 다/MAG\n",
      "[pos -> pos] 인셉션/NNP 정말/MAG 흥미진진/XR 하/XSA 게/EC 보/VV 았었/EP 고/EC 크리스토퍼/NNP 놀라/VV ㄴ/ETM 감독/NNG 님/XSN 신작/NNP 인터스텔라/NNP 도/JX 이번/NNG 주/NNP 일요일/NNG 에/JKB 보/VV 러/EC 가/VX ㅂ니다/EC 완전/NNG 기대/NNG 중/NNB\n",
      "[pos -> pos] 놀라/VV ㄴ/ETM 이면/NNP 무조건/MAG 보/VV 아야/EC 되/VV ㄴ다/EC 왜냐하면/MAJ 모든/MM 작품/NNG 을/JKO 다/MAG 히트/NNP 치/VV 었/EP 으니깐/EC\n",
      "[pos -> neg] 나/NP 는/JX 감탄/NNG 하/XSV ㄹ/ETM 준비/NNG 가/JKS 되/VV 어/EC 있/VX 다/EC\n",
      "[pos -> neg] 얘/NP 들/XSN 아/JKV 오늘/NNG 나오/VV 는/ETM 거지/NNG 밤/NNG 에/JKB ㅋㅋ/NA 오늘/NNG\n",
      "[pos -> neg] 이제/MAG 죽/VV 어도/EC 이/JKS 없/VA 다/EC\n",
      "[pos -> neg] 기대감/NNG 에/JKB 잠도/NNP 안/MAG 오/VV ㄴ다/EC\n",
      "[pos -> neg] 나/NP 랑/JKB 같이/MAG 보/VV 아/EC 주/VX ㄹ까/EC ㅎ/NA\n",
      "[pos -> neg] 우선/MAG 명/NNB 감독/NNP 들/XSN 이/JKS 모이/VV 었/EP 고/EC 미국/NNP 에서/JKB 도/JX 극찬/NNG 을/JKO 받/VV 았/EP 더군/EC\n",
      "[pos -> neg] 드디어/MAG 내일/NNG 꼭/MAG 보/VV ㄴ다/EC\n",
      "CPU times: user 13.4 s, sys: 148 ms, total: 13.5 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rbf_SVM = SVC(C=1.0, kernel='rbf')\n",
    "rbf_SVM.fit(x_norm, y)\n",
    "test(rbf_SVM, x, y, texts, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rbf_SVM.support_vectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 경우는 저도 자주 보지 못합니다만, 데이터를 모두 외워버렸습니다. k-nearest neighbor 와 다를 게 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4808)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_SVM.support_vectors_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neg -> pos] 재미없/VA 다/EC 이상/NNG 10/SN 자/NNB\n",
      "[neg -> neg] 다크/NNP 나이트/NNP 라/NNG 이즈/NNP 에/JKB 이/NP 은/JX 놀라/NNP ㄴ/JX 의/JKG 거품/NNG 이/JKS 확실히/MAG 드러나/VV ㄴ/ETM 영화/NNG 시각/NNG 효과/NNG 빼/VV 곤/EC 아무것/NNG 도/JX 보/VV ㄹ게/EC 없/VA 는/ETM 망/NNG 작/NNG\n",
      "[neg -> neg] 시사회/NNP 보고/JKB 꿀/NNG 잠/NNG 불면증/NNP 치료/NNP 에/JKB\n",
      "[neg -> neg] 알/VV 고/EC 보/VX 니/EC 지구/NNG 이/VCP 라는/ETM 설정/NNG 은/JX 혹성탈출/NNP 하/VV 고/EC 똑같/VA 지/EC 않/VX 어/EC 실망/NNP\n",
      "[neg -> pos] 놀라/VV ㄴ/ETM 각본/NNP 은/JX 천재/NNP 이자/NNP 놀라/NNP ㄴ/JX 의/JKG 동생/NNG 조나단/NNP 놀라/VV ㄴ/ETM 영원/NNG 하/XSV ㄴ/ETM 거장/NNG 스티븐 스필버그/NNP 감독/NNG 제작/NNG 참여/NNG 한스/NNP 짐머/NNP 까지/JX OST/SL 하/XSV 아/EC 주/VX ㄴ다면/EC 길/VA ㄴ/ETM 말/NNG 필요/NNG 없이/MAG 일단/MAG 서/VV ㄴ/ETM 감상/NNP 천재/NNG 성/XSN 이/JKS 다분/XR 하/XSA ㄴ/ETM 놀라/VV ㄴ/ETM 형제/NNP 는/JX 지금껏/MAG 실망/NNG 시키/XSV ㄴ/ETM 적/XSN 이/JKS 없/VA 기/ETN 에/JKB 그만큼/MAG 사람/NNG 들/XSN 이/JKS 믿/VV 고/EC 보/VX 는/ETM 것/NNB 같/VA 은/ETM 소리/NNP 하네/NNP 핵/NNG 노/NNP 재/XPN 므/NNP\n",
      "[neg -> neg] ㅅㅂ/NA 이런/MM 영화/NNG 제일/NNG 싫/VA 은데/EC 몰입/NNP 도도/NNP 없/VA 고/EC 흥행/NNG 도/JX 패션왕/NNP 한테/JKB OO/SL 영화/NNG 다/JX 뻔하/VA ㄴ/ETM 영화/NNP 돈/NNG 낭비/NNG ㅉ/NA\n",
      "[neg -> neg] 야/NNG 배급사/NNP 어디/NP 냐/EC 나오/VV 기/ETN 전/NNG 부터/JX 빠/NNG 내/NNB\n",
      "[neg -> neg] 정말/NNP 시간/NNP 만/JX 길/VA ㄴ/ETM 재/VV ㅁ/ETN 없/VA 는/ETM 영화/NNG 능성/NNP 이/JKS 높/VA 음/ETN\n",
      "[neg -> neg] 스필버그/NA 신/NNG 께서/JKS 이런/MM 쓰레기/NNP 영화 제작/NNP 에/JKB 참여/NNG 하/VV 다니/EC ㅜㅜ/NA A/SL I/SL 능가/NNG 하/XSV ㄹ/ETM 작품/NNG 은/JX 없/VA 습니다/EC\n",
      "[neg -> pos] 제가/NNP 시사회/NNP 에서/JKB 보/VV 고/EC 오/VX 았/EP 는데/EC 진짜/MAG 노/NNG 재/VV ㅁ/ETN 이/JKS 더라고요/EC 에/NNG 휴/NNP 놀라/NNP ㄴ/JX 도/JX ㄴ/ETM 거/NNB 에/JKB 놀라/VV ㅂ니다/EC\n",
      "[pos -> pos] 크리스토퍼/NNP 놀라/VV ㄴ/ETM 에게/JKB 우리/NP 는/JX 놀라/VV ㄴ/ETM 다/MAG\n",
      "[pos -> pos] 인셉션/NNP 정말/MAG 흥미진진/XR 하/XSA 게/EC 보/VV 았었/EP 고/EC 크리스토퍼/NNP 놀라/VV ㄴ/ETM 감독/NNG 님/XSN 신작/NNP 인터스텔라/NNP 도/JX 이번/NNG 주/NNP 일요일/NNG 에/JKB 보/VV 러/EC 가/VX ㅂ니다/EC 완전/NNG 기대/NNG 중/NNB\n",
      "[pos -> pos] 놀라/VV ㄴ/ETM 이면/NNP 무조건/MAG 보/VV 아야/EC 되/VV ㄴ다/EC 왜냐하면/MAJ 모든/MM 작품/NNG 을/JKO 다/MAG 히트/NNP 치/VV 었/EP 으니깐/EC\n",
      "[pos -> pos] 나/NP 는/JX 감탄/NNG 하/XSV ㄹ/ETM 준비/NNG 가/JKS 되/VV 어/EC 있/VX 다/EC\n",
      "[pos -> pos] 얘/NP 들/XSN 아/JKV 오늘/NNG 나오/VV 는/ETM 거지/NNG 밤/NNG 에/JKB ㅋㅋ/NA 오늘/NNG\n",
      "[pos -> pos] 이제/MAG 죽/VV 어도/EC 이/JKS 없/VA 다/EC\n",
      "[pos -> pos] 기대감/NNG 에/JKB 잠도/NNP 안/MAG 오/VV ㄴ다/EC\n",
      "[pos -> pos] 나/NP 랑/JKB 같이/MAG 보/VV 아/EC 주/VX ㄹ까/EC ㅎ/NA\n",
      "[pos -> pos] 우선/MAG 명/NNB 감독/NNP 들/XSN 이/JKS 모이/VV 었/EP 고/EC 미국/NNP 에서/JKB 도/JX 극찬/NNG 을/JKO 받/VV 았/EP 더군/EC\n",
      "[pos -> pos] 드디어/MAG 내일/NNG 꼭/MAG 보/VV ㄴ다/EC\n",
      "CPU times: user 5.8 s, sys: 64 ms, total: 5.86 s\n",
      "Wall time: 5.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "linear_SVM = SVC(C=1.0, kernel='linear')\n",
    "linear_SVM.fit(x_norm, y)\n",
    "test(linear_SVM, x, y, texts, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 은 support vector를 직접 확인할 수가 있습니다. \n",
    "\n",
    "linear SVM 의 support vector들을 확인해 보겠습니다. (4167, 4808) 은 4,808 차원 (단어 개수) 의 4,167 개의 support vectors 가 있다는 의미입니다. 이 숫자도 매우 큽니다. 이는 features 가 반복되지 않는 (각 리뷰별로 다양한 단어들이 이용되는) 데이터셋이기 때문입니다.\n",
    "\n",
    "명사 추출기 때 살펴볼 (L, R) 구조의 데이터셋에 linear SVM 을 적용하면 support vectors 의 개수가 매우 작음을 볼 수 있습니다. 이는 이후에 명사 추출기 예시에서 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4167, 4808)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_SVM.support_vectors_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 데이터 X 에서의 각 support vectors 의 index 가 support_ 에 저장되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 9989, 9994, 9996], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_SVM.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC.dual\\_coef\\_에는 alpha가 학습되어 있습니다. 이 값이 negative이면 negative class 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4167 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4167 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_SVM.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hist, bin_edges = np.histogram(\n",
    "    linear_SVM.dual_coef_.data,\n",
    "    bins=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 support vectors 의 weight $\\alpha$ 의 값이 -1 아니면 1 에 가깝습니다. 사실상 대부분의 리뷰들이 비슷한 중요도를 지닙니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.0 ~ -0.9) : 1637\n",
      "(-0.9 ~ -0.8) : 56\n",
      "(-0.8 ~ -0.7) : 45\n",
      "(-0.7 ~ -0.6) : 61\n",
      "(-0.6 ~ -0.5) : 43\n",
      "(-0.5 ~ -0.4) : 64\n",
      "(-0.4 ~ -0.3) : 59\n",
      "(-0.3 ~ -0.2) : 52\n",
      "(-0.2 ~ -0.1) : 56\n",
      "(-0.1 ~ 0.0) : 54\n",
      "(0.0 ~ 0.1) : 33\n",
      "(0.1 ~ 0.2) : 34\n",
      "(0.2 ~ 0.3) : 33\n",
      "(0.3 ~ 0.4) : 47\n",
      "(0.4 ~ 0.5) : 45\n",
      "(0.5 ~ 0.6) : 35\n",
      "(0.6 ~ 0.7) : 31\n",
      "(0.7 ~ 0.8) : 39\n",
      "(0.8 ~ 0.9) : 48\n",
      "(0.9 ~ 1.0) : 1695\n"
     ]
    }
   ],
   "source": [
    "for b, e, hist_i in zip(bin_edges, bin_edges[1:], hist):\n",
    "    print('({:.3} ~ {:.3}) : {}'.format(b, e, hist_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "naive bayes 역시 학습 속도가 매우 빠릅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neg -> pos] 재미없/VA 다/EC 이상/NNG 10/SN 자/NNB\n",
      "[neg -> neg] 다크/NNP 나이트/NNP 라/NNG 이즈/NNP 에/JKB 이/NP 은/JX 놀라/NNP ㄴ/JX 의/JKG 거품/NNG 이/JKS 확실히/MAG 드러나/VV ㄴ/ETM 영화/NNG 시각/NNG 효과/NNG 빼/VV 곤/EC 아무것/NNG 도/JX 보/VV ㄹ게/EC 없/VA 는/ETM 망/NNG 작/NNG\n",
      "[neg -> pos] 시사회/NNP 보고/JKB 꿀/NNG 잠/NNG 불면증/NNP 치료/NNP 에/JKB\n",
      "[neg -> neg] 알/VV 고/EC 보/VX 니/EC 지구/NNG 이/VCP 라는/ETM 설정/NNG 은/JX 혹성탈출/NNP 하/VV 고/EC 똑같/VA 지/EC 않/VX 어/EC 실망/NNP\n",
      "[neg -> pos] 놀라/VV ㄴ/ETM 각본/NNP 은/JX 천재/NNP 이자/NNP 놀라/NNP ㄴ/JX 의/JKG 동생/NNG 조나단/NNP 놀라/VV ㄴ/ETM 영원/NNG 하/XSV ㄴ/ETM 거장/NNG 스티븐 스필버그/NNP 감독/NNG 제작/NNG 참여/NNG 한스/NNP 짐머/NNP 까지/JX OST/SL 하/XSV 아/EC 주/VX ㄴ다면/EC 길/VA ㄴ/ETM 말/NNG 필요/NNG 없이/MAG 일단/MAG 서/VV ㄴ/ETM 감상/NNP 천재/NNG 성/XSN 이/JKS 다분/XR 하/XSA ㄴ/ETM 놀라/VV ㄴ/ETM 형제/NNP 는/JX 지금껏/MAG 실망/NNG 시키/XSV ㄴ/ETM 적/XSN 이/JKS 없/VA 기/ETN 에/JKB 그만큼/MAG 사람/NNG 들/XSN 이/JKS 믿/VV 고/EC 보/VX 는/ETM 것/NNB 같/VA 은/ETM 소리/NNP 하네/NNP 핵/NNG 노/NNP 재/XPN 므/NNP\n",
      "[neg -> neg] ㅅㅂ/NA 이런/MM 영화/NNG 제일/NNG 싫/VA 은데/EC 몰입/NNP 도도/NNP 없/VA 고/EC 흥행/NNG 도/JX 패션왕/NNP 한테/JKB OO/SL 영화/NNG 다/JX 뻔하/VA ㄴ/ETM 영화/NNP 돈/NNG 낭비/NNG ㅉ/NA\n",
      "[neg -> neg] 야/NNG 배급사/NNP 어디/NP 냐/EC 나오/VV 기/ETN 전/NNG 부터/JX 빠/NNG 내/NNB\n",
      "[neg -> neg] 정말/NNP 시간/NNP 만/JX 길/VA ㄴ/ETM 재/VV ㅁ/ETN 없/VA 는/ETM 영화/NNG 능성/NNP 이/JKS 높/VA 음/ETN\n",
      "[neg -> pos] 스필버그/NA 신/NNG 께서/JKS 이런/MM 쓰레기/NNP 영화 제작/NNP 에/JKB 참여/NNG 하/VV 다니/EC ㅜㅜ/NA A/SL I/SL 능가/NNG 하/XSV ㄹ/ETM 작품/NNG 은/JX 없/VA 습니다/EC\n",
      "[neg -> neg] 제가/NNP 시사회/NNP 에서/JKB 보/VV 고/EC 오/VX 았/EP 는데/EC 진짜/MAG 노/NNG 재/VV ㅁ/ETN 이/JKS 더라고요/EC 에/NNG 휴/NNP 놀라/NNP ㄴ/JX 도/JX ㄴ/ETM 거/NNB 에/JKB 놀라/VV ㅂ니다/EC\n",
      "[pos -> pos] 크리스토퍼/NNP 놀라/VV ㄴ/ETM 에게/JKB 우리/NP 는/JX 놀라/VV ㄴ/ETM 다/MAG\n",
      "[pos -> pos] 인셉션/NNP 정말/MAG 흥미진진/XR 하/XSA 게/EC 보/VV 았었/EP 고/EC 크리스토퍼/NNP 놀라/VV ㄴ/ETM 감독/NNG 님/XSN 신작/NNP 인터스텔라/NNP 도/JX 이번/NNG 주/NNP 일요일/NNG 에/JKB 보/VV 러/EC 가/VX ㅂ니다/EC 완전/NNG 기대/NNG 중/NNB\n",
      "[pos -> pos] 놀라/VV ㄴ/ETM 이면/NNP 무조건/MAG 보/VV 아야/EC 되/VV ㄴ다/EC 왜냐하면/MAJ 모든/MM 작품/NNG 을/JKO 다/MAG 히트/NNP 치/VV 었/EP 으니깐/EC\n",
      "[pos -> pos] 나/NP 는/JX 감탄/NNG 하/XSV ㄹ/ETM 준비/NNG 가/JKS 되/VV 어/EC 있/VX 다/EC\n",
      "[pos -> neg] 얘/NP 들/XSN 아/JKV 오늘/NNG 나오/VV 는/ETM 거지/NNG 밤/NNG 에/JKB ㅋㅋ/NA 오늘/NNG\n",
      "[pos -> neg] 이제/MAG 죽/VV 어도/EC 이/JKS 없/VA 다/EC\n",
      "[pos -> neg] 기대감/NNG 에/JKB 잠도/NNP 안/MAG 오/VV ㄴ다/EC\n",
      "[pos -> pos] 나/NP 랑/JKB 같이/MAG 보/VV 아/EC 주/VX ㄹ까/EC ㅎ/NA\n",
      "[pos -> neg] 우선/MAG 명/NNB 감독/NNP 들/XSN 이/JKS 모이/VV 었/EP 고/EC 미국/NNP 에서/JKB 도/JX 극찬/NNG 을/JKO 받/VV 았/EP 더군/EC\n",
      "[pos -> pos] 드디어/MAG 내일/NNG 꼭/MAG 보/VV ㄴ다/EC\n",
      "CPU times: user 16 ms, sys: 4 ms, total: 20 ms\n",
      "Wall time: 18.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bernoulli_naive_bayes = BernoulliNB()\n",
    "bernoulli_naive_bayes.fit(x_norm, y)\n",
    "test(bernoulli_naive_bayes, x, y, texts, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes 역시 binary classification 에서는 (1, n_terms) 의 coefficient 가 저장되어 있습니다. 숫자가 클수록 긍정 리뷰에 발생하는 단어라는 의미입니다.\n",
    "\n",
    "일반적으로 확률의 곲을 이용하는 모델들은 계산의 편의성을 위하여 log 확률을 이용합니다. 확률의 누적곲에 log 를 취하면 덧샘이 되기 때문입니다. Naive Bayes 의 coefficient 의 값은 log 가 취해진 확률값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.57168296, -8.51759311, -4.32793837, ..., -5.95264375,\n",
       "        -7.13129875, -5.33953928]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_naive_bayes.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 Naive Bayes 는 단어 빈도수 정보까지 coefficient 에 저장합니다. Classsification 에 불필요한 단어가 많이 포함되어 있다면 이들에 의해 classification 의 성능이 저하될 수도 있습니다. 즉 Naive Bayes 를 잘 이용하기 위해서는 정보력이 적은 (frequent) 단어들은 제거해 주면 도움이 됩니다.\n",
    "\n",
    "그리고 판별에 명확한 힌트가 되는 단어들로만 구성된 데이터셋이라면 coefficient 의 해석도 자연스럽습니다. 이 역시 명사 추출기에서 함께 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관람객/NNG : -0.901\n",
      "ㄴ/ETM : -1.07\n",
      "보/VV : -1.13\n",
      "영화/NNG : -1.16\n",
      "이/VCP : -1.3\n",
      "다/EC : -1.35\n",
      "하/XSV : -1.44\n",
      "는/ETM : -1.45\n",
      "고/EC : -1.51\n",
      "이/JKS : -1.52\n"
     ]
    }
   ],
   "source": [
    "# Coefficients를 array로 만듭니다. \n",
    "coefficients = bernoulli_naive_bayes.coef_.reshape(-1)\n",
    "\n",
    "# Coefficient를 (index, coef)로 만든 뒤, coefficient 의 크기로 내림차순 정렬합니다. \n",
    "sorted_coefficients = sorted(enumerate(coefficients), key=lambda x:-x[1])\n",
    "\n",
    "for j, coef in sorted_coefficients[:10]:\n",
    "    print('{} : {:.3}'.format(idx_to_vocab[j], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "적/XSN : -2.85\n",
      "들/XSN : -2.85\n",
      "이/MM : -2.88\n",
      "좋/VA : -2.93\n",
      "지만/EC : -2.97\n",
      "같/VA : -3.04\n",
      "되/XSV : -3.04\n",
      "다/JX : -3.05\n",
      "내/NP : -3.06\n",
      "되/VV : -3.08\n",
      "생각/NNG : -3.09\n",
      "음/ETN : -3.09\n",
      "을/ETM : -3.1\n",
      "싶/VX : -3.11\n",
      "우주/NNG : -3.11\n",
      "과/JC : -3.11\n",
      "기/ETN : -3.12\n",
      "필요/NNG : -3.12\n",
      "만들/VV : -3.14\n",
      "네요/EC : -3.15\n",
      "감독/NNG : -3.16\n",
      "않/VX : -3.17\n",
      "진짜/MAG : -3.18\n",
      "롭/XSA : -3.19\n",
      "다시/MAG : -3.19\n",
      "아서/EC : -3.2\n",
      "잘/MAG : -3.2\n",
      "보/VX : -3.21\n",
      "재미있/VA : -3.22\n",
      "인터스텔라/NNP : -3.22\n",
      "감동/NNG : -3.23\n",
      "하/VX : -3.23\n",
      "더/MAG : -3.24\n",
      "느끼/VV : -3.25\n",
      "너무/MAG : -3.28\n",
      "있/VX : -3.3\n",
      "거/NNB : -3.34\n",
      "이런/MM : -3.34\n",
      "면/EC : -3.36\n",
      "대단/XR : -3.37\n",
      "나오/VV : -3.38\n",
      "던/ETM : -3.42\n",
      "점/NNB : -3.42\n",
      "겠/EP : -3.42\n",
      "까지/JX : -3.43\n",
      "시간/NNG : -3.43\n",
      "지/VX : -3.43\n",
      "이해/NNG : -3.44\n",
      "그/MM : -3.45\n",
      "그냥/MAG : -3.47\n"
     ]
    }
   ],
   "source": [
    "for j, coef in sorted_coefficients[50:100]:\n",
    "    print('{} : {:.3}'.format(idx_to_vocab[j], coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neg -> neg] 재미없/VA 다/EC 이상/NNG 10/SN 자/NNB\n",
      "[neg -> neg] 다크/NNP 나이트/NNP 라/NNG 이즈/NNP 에/JKB 이/NP 은/JX 놀라/NNP ㄴ/JX 의/JKG 거품/NNG 이/JKS 확실히/MAG 드러나/VV ㄴ/ETM 영화/NNG 시각/NNG 효과/NNG 빼/VV 곤/EC 아무것/NNG 도/JX 보/VV ㄹ게/EC 없/VA 는/ETM 망/NNG 작/NNG\n",
      "[neg -> neg] 시사회/NNP 보고/JKB 꿀/NNG 잠/NNG 불면증/NNP 치료/NNP 에/JKB\n",
      "[neg -> neg] 알/VV 고/EC 보/VX 니/EC 지구/NNG 이/VCP 라는/ETM 설정/NNG 은/JX 혹성탈출/NNP 하/VV 고/EC 똑같/VA 지/EC 않/VX 어/EC 실망/NNP\n",
      "[neg -> pos] 놀라/VV ㄴ/ETM 각본/NNP 은/JX 천재/NNP 이자/NNP 놀라/NNP ㄴ/JX 의/JKG 동생/NNG 조나단/NNP 놀라/VV ㄴ/ETM 영원/NNG 하/XSV ㄴ/ETM 거장/NNG 스티븐 스필버그/NNP 감독/NNG 제작/NNG 참여/NNG 한스/NNP 짐머/NNP 까지/JX OST/SL 하/XSV 아/EC 주/VX ㄴ다면/EC 길/VA ㄴ/ETM 말/NNG 필요/NNG 없이/MAG 일단/MAG 서/VV ㄴ/ETM 감상/NNP 천재/NNG 성/XSN 이/JKS 다분/XR 하/XSA ㄴ/ETM 놀라/VV ㄴ/ETM 형제/NNP 는/JX 지금껏/MAG 실망/NNG 시키/XSV ㄴ/ETM 적/XSN 이/JKS 없/VA 기/ETN 에/JKB 그만큼/MAG 사람/NNG 들/XSN 이/JKS 믿/VV 고/EC 보/VX 는/ETM 것/NNB 같/VA 은/ETM 소리/NNP 하네/NNP 핵/NNG 노/NNP 재/XPN 므/NNP\n",
      "[neg -> neg] ㅅㅂ/NA 이런/MM 영화/NNG 제일/NNG 싫/VA 은데/EC 몰입/NNP 도도/NNP 없/VA 고/EC 흥행/NNG 도/JX 패션왕/NNP 한테/JKB OO/SL 영화/NNG 다/JX 뻔하/VA ㄴ/ETM 영화/NNP 돈/NNG 낭비/NNG ㅉ/NA\n",
      "[neg -> neg] 야/NNG 배급사/NNP 어디/NP 냐/EC 나오/VV 기/ETN 전/NNG 부터/JX 빠/NNG 내/NNB\n",
      "[neg -> neg] 정말/NNP 시간/NNP 만/JX 길/VA ㄴ/ETM 재/VV ㅁ/ETN 없/VA 는/ETM 영화/NNG 능성/NNP 이/JKS 높/VA 음/ETN\n",
      "[neg -> neg] 스필버그/NA 신/NNG 께서/JKS 이런/MM 쓰레기/NNP 영화 제작/NNP 에/JKB 참여/NNG 하/VV 다니/EC ㅜㅜ/NA A/SL I/SL 능가/NNG 하/XSV ㄹ/ETM 작품/NNG 은/JX 없/VA 습니다/EC\n",
      "[neg -> neg] 제가/NNP 시사회/NNP 에서/JKB 보/VV 고/EC 오/VX 았/EP 는데/EC 진짜/MAG 노/NNG 재/VV ㅁ/ETN 이/JKS 더라고요/EC 에/NNG 휴/NNP 놀라/NNP ㄴ/JX 도/JX ㄴ/ETM 거/NNB 에/JKB 놀라/VV ㅂ니다/EC\n",
      "[pos -> pos] 크리스토퍼/NNP 놀라/VV ㄴ/ETM 에게/JKB 우리/NP 는/JX 놀라/VV ㄴ/ETM 다/MAG\n",
      "[pos -> pos] 인셉션/NNP 정말/MAG 흥미진진/XR 하/XSA 게/EC 보/VV 았었/EP 고/EC 크리스토퍼/NNP 놀라/VV ㄴ/ETM 감독/NNG 님/XSN 신작/NNP 인터스텔라/NNP 도/JX 이번/NNG 주/NNP 일요일/NNG 에/JKB 보/VV 러/EC 가/VX ㅂ니다/EC 완전/NNG 기대/NNG 중/NNB\n",
      "[pos -> pos] 놀라/VV ㄴ/ETM 이면/NNP 무조건/MAG 보/VV 아야/EC 되/VV ㄴ다/EC 왜냐하면/MAJ 모든/MM 작품/NNG 을/JKO 다/MAG 히트/NNP 치/VV 었/EP 으니깐/EC\n",
      "[pos -> pos] 나/NP 는/JX 감탄/NNG 하/XSV ㄹ/ETM 준비/NNG 가/JKS 되/VV 어/EC 있/VX 다/EC\n",
      "[pos -> neg] 얘/NP 들/XSN 아/JKV 오늘/NNG 나오/VV 는/ETM 거지/NNG 밤/NNG 에/JKB ㅋㅋ/NA 오늘/NNG\n",
      "[pos -> neg] 이제/MAG 죽/VV 어도/EC 이/JKS 없/VA 다/EC\n",
      "[pos -> pos] 기대감/NNG 에/JKB 잠도/NNP 안/MAG 오/VV ㄴ다/EC\n",
      "[pos -> neg] 나/NP 랑/JKB 같이/MAG 보/VV 아/EC 주/VX ㄹ까/EC ㅎ/NA\n",
      "[pos -> pos] 우선/MAG 명/NNB 감독/NNP 들/XSN 이/JKS 모이/VV 었/EP 고/EC 미국/NNP 에서/JKB 도/JX 극찬/NNG 을/JKO 받/VV 았/EP 더군/EC\n",
      "[pos -> pos] 드디어/MAG 내일/NNG 꼭/MAG 보/VV ㄴ다/EC\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(\n",
    "    max_depth=50,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    class_weight=None\n",
    ")\n",
    "\n",
    "decision_tree.fit(x_norm, y)\n",
    "test(decision_tree, x, y, texts, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neg -> neg] 재미없/VA 다/EC 이상/NNG 10/SN 자/NNB\n",
      "[neg -> neg] 다크/NNP 나이트/NNP 라/NNG 이즈/NNP 에/JKB 이/NP 은/JX 놀라/NNP ㄴ/JX 의/JKG 거품/NNG 이/JKS 확실히/MAG 드러나/VV ㄴ/ETM 영화/NNG 시각/NNG 효과/NNG 빼/VV 곤/EC 아무것/NNG 도/JX 보/VV ㄹ게/EC 없/VA 는/ETM 망/NNG 작/NNG\n",
      "[neg -> neg] 시사회/NNP 보고/JKB 꿀/NNG 잠/NNG 불면증/NNP 치료/NNP 에/JKB\n",
      "[neg -> neg] 알/VV 고/EC 보/VX 니/EC 지구/NNG 이/VCP 라는/ETM 설정/NNG 은/JX 혹성탈출/NNP 하/VV 고/EC 똑같/VA 지/EC 않/VX 어/EC 실망/NNP\n",
      "[neg -> pos] 놀라/VV ㄴ/ETM 각본/NNP 은/JX 천재/NNP 이자/NNP 놀라/NNP ㄴ/JX 의/JKG 동생/NNG 조나단/NNP 놀라/VV ㄴ/ETM 영원/NNG 하/XSV ㄴ/ETM 거장/NNG 스티븐 스필버그/NNP 감독/NNG 제작/NNG 참여/NNG 한스/NNP 짐머/NNP 까지/JX OST/SL 하/XSV 아/EC 주/VX ㄴ다면/EC 길/VA ㄴ/ETM 말/NNG 필요/NNG 없이/MAG 일단/MAG 서/VV ㄴ/ETM 감상/NNP 천재/NNG 성/XSN 이/JKS 다분/XR 하/XSA ㄴ/ETM 놀라/VV ㄴ/ETM 형제/NNP 는/JX 지금껏/MAG 실망/NNG 시키/XSV ㄴ/ETM 적/XSN 이/JKS 없/VA 기/ETN 에/JKB 그만큼/MAG 사람/NNG 들/XSN 이/JKS 믿/VV 고/EC 보/VX 는/ETM 것/NNB 같/VA 은/ETM 소리/NNP 하네/NNP 핵/NNG 노/NNP 재/XPN 므/NNP\n",
      "[neg -> neg] ㅅㅂ/NA 이런/MM 영화/NNG 제일/NNG 싫/VA 은데/EC 몰입/NNP 도도/NNP 없/VA 고/EC 흥행/NNG 도/JX 패션왕/NNP 한테/JKB OO/SL 영화/NNG 다/JX 뻔하/VA ㄴ/ETM 영화/NNP 돈/NNG 낭비/NNG ㅉ/NA\n",
      "[neg -> neg] 야/NNG 배급사/NNP 어디/NP 냐/EC 나오/VV 기/ETN 전/NNG 부터/JX 빠/NNG 내/NNB\n",
      "[neg -> neg] 정말/NNP 시간/NNP 만/JX 길/VA ㄴ/ETM 재/VV ㅁ/ETN 없/VA 는/ETM 영화/NNG 능성/NNP 이/JKS 높/VA 음/ETN\n",
      "[neg -> neg] 스필버그/NA 신/NNG 께서/JKS 이런/MM 쓰레기/NNP 영화 제작/NNP 에/JKB 참여/NNG 하/VV 다니/EC ㅜㅜ/NA A/SL I/SL 능가/NNG 하/XSV ㄹ/ETM 작품/NNG 은/JX 없/VA 습니다/EC\n",
      "[neg -> neg] 제가/NNP 시사회/NNP 에서/JKB 보/VV 고/EC 오/VX 았/EP 는데/EC 진짜/MAG 노/NNG 재/VV ㅁ/ETN 이/JKS 더라고요/EC 에/NNG 휴/NNP 놀라/NNP ㄴ/JX 도/JX ㄴ/ETM 거/NNB 에/JKB 놀라/VV ㅂ니다/EC\n",
      "[pos -> pos] 크리스토퍼/NNP 놀라/VV ㄴ/ETM 에게/JKB 우리/NP 는/JX 놀라/VV ㄴ/ETM 다/MAG\n",
      "[pos -> pos] 인셉션/NNP 정말/MAG 흥미진진/XR 하/XSA 게/EC 보/VV 았었/EP 고/EC 크리스토퍼/NNP 놀라/VV ㄴ/ETM 감독/NNG 님/XSN 신작/NNP 인터스텔라/NNP 도/JX 이번/NNG 주/NNP 일요일/NNG 에/JKB 보/VV 러/EC 가/VX ㅂ니다/EC 완전/NNG 기대/NNG 중/NNB\n",
      "[pos -> pos] 놀라/VV ㄴ/ETM 이면/NNP 무조건/MAG 보/VV 아야/EC 되/VV ㄴ다/EC 왜냐하면/MAJ 모든/MM 작품/NNG 을/JKO 다/MAG 히트/NNP 치/VV 었/EP 으니깐/EC\n",
      "[pos -> neg] 나/NP 는/JX 감탄/NNG 하/XSV ㄹ/ETM 준비/NNG 가/JKS 되/VV 어/EC 있/VX 다/EC\n",
      "[pos -> neg] 얘/NP 들/XSN 아/JKV 오늘/NNG 나오/VV 는/ETM 거지/NNG 밤/NNG 에/JKB ㅋㅋ/NA 오늘/NNG\n",
      "[pos -> neg] 이제/MAG 죽/VV 어도/EC 이/JKS 없/VA 다/EC\n",
      "[pos -> neg] 기대감/NNG 에/JKB 잠도/NNP 안/MAG 오/VV ㄴ다/EC\n",
      "[pos -> neg] 나/NP 랑/JKB 같이/MAG 보/VV 아/EC 주/VX ㄹ까/EC ㅎ/NA\n",
      "[pos -> neg] 우선/MAG 명/NNB 감독/NNP 들/XSN 이/JKS 모이/VV 었/EP 고/EC 미국/NNP 에서/JKB 도/JX 극찬/NNG 을/JKO 받/VV 았/EP 더군/EC\n",
      "[pos -> neg] 드디어/MAG 내일/NNG 꼭/MAG 보/VV ㄴ다/EC\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    class_weight=None\n",
    ")\n",
    "\n",
    "decision_tree.fit(x_norm, y)\n",
    "test(decision_tree, x, y, texts, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,  161,  162,  178,  179,  184,  206,  257,  336,  480,  494,\n",
       "        559,  585,  842,  877,  882,  931,  985, 1006, 1018, 1031, 1032,\n",
       "       1074, 1111, 1152, 1255, 1299, 1406, 1557, 1591, 1662, 2078, 2114,\n",
       "       2512, 2601, 2716, 2734, 2741, 2808, 2851, 2983, 2996, 3098, 3359,\n",
       "       3368, 3375, 3408, 3435, 3551, 3697, 3706, 3876, 3947, 3973, 3998,\n",
       "       4000, 4008, 4176, 4213, 4456, 4511, 4517, 4518, 4558])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(decision_tree.feature_importances_ > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4808,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇 개의 단어만을 이용하여 분류를 하다보니 분류가 제대로 이뤄지지 않기도 합니다. 게다가 분류에 유용한 features 가 아닌 경우도 많습니다. 또한 `크리스토퍼 놀란` 때문에 `놀라/VV`, `놀라/NNP` 와 같은 잘못된 features 들도 생성되었음을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('관람객/NNG', 0.4510845086787639),\n",
       " ('놀라/VV', 0.10623425004513344),\n",
       " ('최고/NNG', 0.08842707720332754),\n",
       " ('번/NNB', 0.03825325790437422),\n",
       " ('경이/NNG', 0.03468372593892105),\n",
       " ('필요/NNG', 0.029884436449725816),\n",
       " ('수/NNB', 0.02891829972680556),\n",
       " ('놀라/NNP', 0.027174462181816735),\n",
       " ('아깝/VA', 0.023680759018578437),\n",
       " ('인터스텔라/NNP', 0.02090671738172154),\n",
       " ('재미없/VA', 0.015089595722997775),\n",
       " ('들/XSN', 0.01022028341450091),\n",
       " ('지루/XR', 0.009356473444929085),\n",
       " ('평점/NNG', 0.008016104496944376),\n",
       " ('노/NNG', 0.006661564598202253),\n",
       " ('지/EC', 0.006367177178232638),\n",
       " ('1/SN', 0.005127239183416747),\n",
       " ('졸/VV', 0.004864729250959874),\n",
       " ('ㄴ/ETM', 0.004777572451144816),\n",
       " ('을/JKO', 0.00454902543036219),\n",
       " ('쓰레기/NNP', 0.004385307060655613),\n",
       " ('ㄹ/ETM', 0.004208895713729889),\n",
       " ('나/NP', 0.0040954016849899496),\n",
       " ('ㅁ/ETN', 0.0036113313444510104),\n",
       " ('없/VA', 0.0029579094762203235),\n",
       " ('음/ETN', 0.002657335600262841),\n",
       " ('난해/XR', 0.0026438404474042445),\n",
       " ('지루/NNG', 0.0026313753816456517),\n",
       " ('ㄴ지/EC', 0.0026216086307932697),\n",
       " ('나/EC', 0.0025552774485123127),\n",
       " ('영화/NNP', 0.0024122566739635588),\n",
       " ('시간/NNP', 0.002198923183181407),\n",
       " ('재밌/VA', 0.0021329551392987043),\n",
       " ('만들/VV', 0.0020492837695076285),\n",
       " ('한/MM', 0.002025704852633403),\n",
       " ('너무/MAG', 0.0019852029081508745),\n",
       " ('도/JX', 0.0018529240974557723),\n",
       " ('보/VV', 0.0017048983019519297),\n",
       " ('더/MAG', 0.0016778109519742183),\n",
       " ('아니/VCN', 0.0015596347785097066),\n",
       " ('ㄴ/JX', 0.001467671577099962),\n",
       " ('과/JC', 0.0014499136282291976),\n",
       " ('았/EP', 0.0014330010936749553),\n",
       " ('지만/EC', 0.0014215568495955712),\n",
       " ('고/EC', 0.0013696415934483417),\n",
       " ('는/ETM', 0.0013416897241942945),\n",
       " ('이다/NNP', 0.001321040301203567),\n",
       " ('네/EC', 0.0012770056244967934),\n",
       " ('다시/MAG', 0.0011949178914269871),\n",
       " ('감독/NNG', 0.0011595546274203216),\n",
       " ('꼭/MAG', 0.0011011736399152803),\n",
       " ('안/MAG', 0.0010770976252684307),\n",
       " ('려/EC', 0.0010320690186109952),\n",
       " ('를/JKO', 0.0009864680524241038),\n",
       " ('천재/NNG', 0.0009132548346628975),\n",
       " ('가/JKS', 0.0008804838815025055),\n",
       " ('ㄴ듯/EC', 0.0008730165398015853),\n",
       " ('하/XSA', 0.0008455929118392356),\n",
       " ('다/EC', 0.0007583114768921091),\n",
       " ('중/NNB', 0.000648038677864864),\n",
       " ('은/JX', 0.0005303872821871997),\n",
       " ('하/XSV', 0.00030381368845679813),\n",
       " ('에/JKB', 0.0002624369946630048),\n",
       " ('이/JKS', 0.00010672531897000088)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(idx_to_vocab[idx], importancy) for idx, importancy in \n",
    " sorted(\n",
    "     filter(lambda x:x[1]>0,\n",
    "            enumerate(decision_tree.feature_importances_)),\n",
    "     key=lambda x:-x[1]\n",
    " )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
