{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch: optim\n----------------\n\n$y=\\sin(x)$ \uc744 \uc608\uce21\ud560 \uc218 \uc788\ub3c4\ub85d, $-\\pi$ \ubd80\ud130 $pi$ \uae4c\uc9c0\n\uc720\ud074\ub9ac\ub4dc \uac70\ub9ac(Euclidean distance)\ub97c \ucd5c\uc18c\ud654\ud558\ub3c4\ub85d 3\ucc28 \ub2e4\ud56d\uc2dd\uc744 \ud559\uc2b5\ud569\ub2c8\ub2e4.\n\n\uc774\ubc88\uc5d0\ub294 PyTorch\uc758 nn \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e0\uacbd\ub9dd\uc744 \uad6c\uc131\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uc9c0\uae08\uae4c\uc9c0 \ud574\uc654\ub358 \uac83\ucc98\ub7fc \uc9c1\uc811 \ubaa8\ub378\uc758 \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud558\ub294 \ub300\uc2e0, optim \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud560\n\uc635\ud2f0\ub9c8\uc774\uc800(Optimizer)\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. optim \ud328\ud0a4\uc9c0\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ub525\ub7ec\ub2dd\uc5d0 \uc0ac\uc6a9\ud558\ub294 SGD+momentum, \nRMSProp, Adam \ub4f1\uacfc \uac19\uc740 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654(optimization) \uc54c\uace0\ub9ac\uc998\uc744 \uc815\uc758\ud569\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport math\n\n# \uc785\ub825\uac12\uacfc \ucd9c\ub825\uac12\uc744 \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# \uc785\ub825 \ud150\uc11c (x, x^2, x^3)\ub97c \uc900\ube44\ud569\ub2c8\ub2e4.\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# nn \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uacfc \uc190\uc2e4 \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n# optim \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud560 optimizer\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n# \uc5ec\uae30\uc11c\ub294 RMSprop\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4; optim \ud328\ud0a4\uc9c0\ub294 \ub2e4\ub978 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n# RMSprop \uc0dd\uc131\uc790\uc758 \uccab\ubc88\uc9f8 \uc778\uc790\ub294 \uc5b4\ub5a4 \ud150\uc11c\uac00 \uac31\uc2e0\ub418\uc5b4\uc57c \ud558\ub294\uc9c0\ub97c \uc54c\ub824\uc90d\ub2c8\ub2e4.\nlearning_rate = 1e-3\noptimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\nfor t in range(2000):\n    # \uc21c\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc5d0 x\ub97c \uc804\ub2ec\ud558\uc5ec \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n    y_pred = model(xx)\n\n    # \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc804\uc5d0, optimizer \uac1d\uccb4\ub97c \uc0ac\uc6a9\ud558\uc5ec (\ubaa8\ub378\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \uac00\uc911\uce58\uc778) \uac31\uc2e0\ud560\n    # \ubcc0\uc218\ub4e4\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \ubcc0\ud654\ub3c4(gradient)\ub97c 0\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\ub294 \uc774\uc720\ub294 \uae30\ubcf8\uc801\uc73c\ub85c \n    # .backward()\ub97c \ud638\ucd9c\ud560 \ub54c\ub9c8\ub2e4 \ubcc0\ud654\ub3c4\uac00 \ubc84\ud37c(buffer)\uc5d0 (\ub36e\uc5b4\uc4f0\uc9c0 \uc54a\uace0) \ub204\uc801\ub418\uae30\n    # \ub54c\ubb38\uc785\ub2c8\ub2e4. \ub354 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 torch.autograd.backward\uc5d0 \ub300\ud55c \ubb38\uc11c\ub97c \ucc38\uc870\ud558\uc138\uc694.\n    optimizer.zero_grad()\n\n    # \uc5ed\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ub4e4\uc5d0 \ub300\ud55c \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n    loss.backward()\n\n    # optimizer\uc758 step \ud568\uc218\ub97c \ud638\ucd9c\ud558\uba74 \ub9e4\uac1c\ubcc0\uc218\uac00 \uac31\uc2e0\ub429\ub2c8\ub2e4.\n    optimizer.step()\n\n\nlinear_layer = model[0]\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}