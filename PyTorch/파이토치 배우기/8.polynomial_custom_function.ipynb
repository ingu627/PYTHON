{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch: \uc0c8 autograd Function \uc815\uc758\ud558\uae30\n----------------------------------------\n\n$y=\\sin(x)$ \uc744 \uc608\uce21\ud560 \uc218 \uc788\ub3c4\ub85d, $-\\pi$ \ubd80\ud130 $pi$ \uae4c\uc9c0\n\uc720\ud074\ub9ac\ub4dc \uac70\ub9ac(Euclidean distance)\ub97c \ucd5c\uc18c\ud654\ud558\ub3c4\ub85d 3\ucc28 \ub2e4\ud56d\uc2dd\uc744 \ud559\uc2b5\ud569\ub2c8\ub2e4.\n\ub2e4\ud56d\uc2dd\uc744 $y=a+bx+cx^2+dx^3$ \ub77c\uace0 \uc4f0\ub294 \ub300\uc2e0 $y=a+b P_3(c+dx)$ \ub85c \ub2e4\ud56d\uc2dd\uc744 \uc801\uaca0\uc2b5\ub2c8\ub2e4.\n\uc5ec\uae30\uc11c $P_3(x)=\frac{1}{2}\\left(5x^3-3x\right)$ \uc740 3\ucc28\n`\ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd(Legendre polynomial)`_ \uc785\ub2c8\ub2e4.\n\n    https://en.wikipedia.org/wiki/Legendre_polynomials\n\n\uc774 \uad6c\ud604\uc740 PyTorch \ud150\uc11c \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc21c\uc804\ud30c \ub2e8\uacc4\ub97c \uacc4\uc0b0\ud558\uace0, PyTorch autograd\ub97c \uc0ac\uc6a9\ud558\uc5ec\n\ubcc0\ud654\ub3c4(gradient)\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\n\uc544\ub798 \uad6c\ud604\uc5d0\uc11c\ub294 $P_3'(x)$ \uc744 \uc218\ud589\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\uc790 \uc815\uc758 autograd Function\ub97c \uad6c\ud604\ud569\ub2c8\ub2e4.\n\uc218\ud559\uc801\uc73c\ub85c\ub294 $P_3'(x)=\frac{3}{2}\\left(5x^2-1\right)$ \uc785\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport math\n\n\nclass LegendrePolynomial3(torch.autograd.Function):\n    \"\"\"\n    torch.autograd.Function\uc744 \uc0c1\uc18d\ubc1b\uc544 \uc0ac\uc6a9\uc790 \uc815\uc758 autograd Function\uc744 \uad6c\ud604\ud558\uace0,\n    \ud150\uc11c \uc5f0\uc0b0\uc744 \ud558\ub294 \uc21c\uc804\ud30c \ub2e8\uacc4\uc640 \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        \uc21c\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c\ub294 \uc785\ub825\uc744 \uac16\ub294 \ud150\uc11c\ub97c \ubc1b\uc544 \ucd9c\ub825\uc744 \uac16\ub294 \ud150\uc11c\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.\n        ctx\ub294 \ucee8\ud14d\uc2a4\ud2b8 \uac1d\uccb4(context object)\ub85c \uc5ed\uc804\ud30c \uc5f0\uc0b0\uc744 \uc704\ud55c \uc815\ubcf4 \uc800\uc7a5\uc5d0 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n        ctx.save_for_backward \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc5b4\ub5a4 \uac1d\uccb4\ub3c4\n        \uc800\uc7a5(cache)\ud574 \ub458 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return 0.5 * (5 * input ** 3 - 3 * input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        \uc5ed\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c\ub294 \ucd9c\ub825\uc5d0 \ub300\ud55c \uc190\uc2e4(loss)\uc758 \ubcc0\ud654\ub3c4(gradient)\ub97c \uac16\ub294 \ud150\uc11c\ub97c \ubc1b\uace0,\n        \uc785\ub825\uc5d0 \ub300\ud55c \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud574\uc57c \ud569\ub2c8\ub2e4.\n        \"\"\"\n        input, = ctx.saved_tensors\n        return grad_output * 1.5 * (5 * input ** 2 - 1)\n\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\") # GPU\uc5d0\uc11c \uc2e4\ud589\ud558\ub824\uba74 \uc774 \uc8fc\uc11d\uc744 \uc81c\uac70\ud558\uc138\uc694\n\n# \uc785\ub825\uac12\uacfc \ucd9c\ub825\uac12\uc744 \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.\n# requires_grad=False\uac00 \uae30\ubcf8\uac12\uc73c\ub85c \uc124\uc815\ub418\uc5b4 \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc911\uc5d0 \uc774 \ud150\uc11c\ub4e4\uc5d0 \ub300\ud55c \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud560\n# \ud544\uc694\uac00 \uc5c6\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# \uac00\uc911\uce58\ub97c \uac16\ub294 \uc784\uc758\uc758 \ud150\uc11c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. 3\ucc28 \ub2e4\ud56d\uc2dd\uc774\ubbc0\ub85c 4\uac1c\uc758 \uac00\uc911\uce58\uac00 \ud544\uc694\ud569\ub2c8\ub2e4:\n# y = a + b * P3(c + d * x) \n# \uc774 \uac00\uc911\uce58\ub4e4\uc774 \uc218\ub834(convergence)\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc815\ub2f5\uc73c\ub85c\ubd80\ud130 \ub108\ubb34 \uba40\ub9ac \ub5a8\uc5b4\uc9c0\uc9c0 \uc54a\uc740 \uac12\uc73c\ub85c\n# \ucd08\uae30\ud654\uac00 \ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4. \n# requires_grad=True\ub85c \uc124\uc815\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc911\uc5d0 \uc774 \ud150\uc11c\ub4e4\uc5d0 \ub300\ud55c \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud560 \ud544\uc694\uac00\n# \uc788\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \na = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nb = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\nc = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nd = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n\nlearning_rate = 5e-6\nfor t in range(2000):\n    # \uc0ac\uc6a9\uc790 \uc815\uc758 Function\uc744 \uc801\uc6a9\ud558\uae30 \uc704\ud574 Function.apply \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n    # \uc5ec\uae30\uc5d0 'P3'\ub77c\uace0 \uc774\ub984\uc744 \ubd99\uc600\uc2b5\ub2c8\ub2e4.\n    P3 = LegendrePolynomial3.apply\n\n    # \uc21c\uc804\ud30c \ub2e8\uacc4: \uc5f0\uc0b0\uc744 \ud558\uc5ec \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4; \n    # \uc0ac\uc6a9\uc790 \uc815\uc758 autograd \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud558\uc5ec P3\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n    y_pred = a + b * P3(c + d * x)\n\n    # \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4.\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # autograd\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n    loss.backward()\n\n    # \uacbd\uc0ac\ud558\uac15\ubc95(gradient descent)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud569\ub2c8\ub2e4.\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # \uac00\uc911\uce58 \uac31\uc2e0 \ud6c4\uc5d0\ub294 \ubcc0\ud654\ub3c4\ub97c \uc9c1\uc811 0\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}