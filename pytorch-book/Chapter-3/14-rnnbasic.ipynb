{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 순환 신경망\n",
    "\n",
    "> 3.2.4 장에 해당하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 3-35\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=True):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        # 선형결합에 사용할 학습가능한 매개변수를 생성한다.\n",
    "        self.weight_xh, self.weight_hh, self.bias = \\\n",
    "            self.init_weight(input_size, hidden_size)\n",
    "        # 필요한 정보를 저장한다\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        rnn_cell 을 구동하기 위해 inputs 의 크기 (T, B, E) 형태가 되어야 한다.\n",
    "         - T: 시퀀스 총 길이\n",
    "         - B: 미니배치크기\n",
    "         - E: 입력층 크기 \n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            # 첫번째 차원이 미니배치 크기인 경우 전치연산으로 바꿔준다.\n",
    "            inputs = inputs.transpose(0, 1)\n",
    "        seqlen, batch_size, _ = inputs.size()\n",
    "        # 0 time-step 에서 은닉층 값을 0으로 초기화 시킨다\n",
    "        hidden = self.init_hidden(batch_size, self.hidden_size)\n",
    "        # output에 은닉층의 출력값을 저장한다.\n",
    "        output = []\n",
    "        # 시퀀스의 총 길이만큼 순방향전파를 진행한다.\n",
    "        for i in range(seqlen):\n",
    "            hidden = self.rnn_cell(inputs[i], hidden)\n",
    "            output.append(hidden)\n",
    "        output = torch.stack(output)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        # 모든 타임스텝의 은닉층 출력값과 마지막 타임 스텝의 은닉층 출력값을 각각 반환한다.\n",
    "        return output, hidden\n",
    "    \n",
    "    def rnn_cell(self, x, h):\n",
    "        \"\"\"RNN Cell\"\"\"\n",
    "        h = x.mm(self.weight_xh.t()) + h.mm(self.weight_hh.t()) + self.bias\n",
    "        return torch.tanh(h)\n",
    "    \n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        \"\"\"0 타임스텝에서 은닉층의 초기화\"\"\"\n",
    "        return torch.zeros(batch_size, hidden_size)\n",
    "    \n",
    "    def init_weight(self, input_size, hidden_size):\n",
    "        \"\"\"rnn_cell 의 선형결합을 위한 초기값\"\"\"\n",
    "        weight_xh = torch.randn(hidden_size, input_size).requires_grad_()\n",
    "        weight_hh = torch.randn(hidden_size, hidden_size).requires_grad_()\n",
    "        bias = torch.zeros(1, hidden_size).requires_grad_()\n",
    "        return weight_xh, weight_hh, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(5, 10, batch_first=True)\n",
      "LSTM(5, 10, batch_first=True)\n",
      "GRU(5, 10, batch_first=True)\n"
     ]
    }
   ],
   "source": [
    "# 코드 3-36\n",
    "\n",
    "# RNN, LSTM, GRU 호출 방법\n",
    "\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=10, batch_first=True)\n",
    "print(rnn_layer)\n",
    "rnn_layer = nn.LSTM(input_size=5, hidden_size=10, batch_first=True)\n",
    "print(rnn_layer)\n",
    "rnn_layer = nn.GRU(input_size=5, hidden_size=10, batch_first=True)\n",
    "print(rnn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/151] 2.3034 \n",
      "We Avengers watch are going watch watch Avengers\n",
      "\n",
      "[31/151] 1.9172 \n",
      "We are Avengers going are Avengers End Game\n",
      "\n",
      "[61/151] 1.5057 \n",
      "We are going going watch Avengers End Game\n",
      "\n",
      "[91/151] 1.0342 \n",
      "We are going to watch Avengers End Game\n",
      "\n",
      "[121/151] 0.6438 \n",
      "We are going to watch Avengers End Game\n",
      "\n",
      "[151/151] 0.3932 \n",
      "We are going to watch Avengers End Game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 코드 3-37\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(70)\n",
    "\n",
    "## 노트북 14-word embedding 에서 사용한 예시 사용\n",
    "sentence = \"We are going to watch Avengers End Game\".split()\n",
    "vocab = {tkn: i for i, tkn in enumerate(sentence, 1)}  # 단어장 생성\n",
    "vocab['<unk>'] = 0\n",
    "# 수치화된 데이터를 단어로 바꾸기 위한 사전\n",
    "rev_vocab = {v: k for k, v in vocab.items()}\n",
    "# 수치화된 데이터를 단어로 전환하는 함수\n",
    "decode = lambda y: [rev_vocab.get(x) for x in y]  \n",
    "\n",
    "def construct_data(sentence, vocab):\n",
    "    \"\"\"\n",
    "    (input, target) 쌍으로 데이터를 생성한다.\n",
    "    - 최종 형태(수치화된 단어 쌍):\n",
    "     [(We,are) ,(are,going), (going,to), (to,watch), \n",
    "        (watch,Avengers), (Avengers,End), (End,Game)]\n",
    "    \"\"\"\n",
    "    numericalize = lambda x: vocab.get(x) if vocab.get(x) is not None else 0\n",
    "    totensor = lambda x: torch.LongTensor(x)\n",
    "    idxes = [numericalize(token) for token in sentence]\n",
    "    x, t = idxes[:-1], idxes[1:]\n",
    "    return totensor(x).unsqueeze(0), totensor(t).unsqueeze(0)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"예제 문장을 출력하는 모델\"\"\"\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
    "        super(Net, self).__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: 단어장의 크기\n",
    "        input_size: 임베딩 크기 = RNN의 입력층 크기\n",
    "        hidden_size: RNN의 은닉층 크기\n",
    "        \"\"\"\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                            embedding_dim=input_size)\n",
    "        self.rnn_layer = nn.RNN(input_size, hidden_size, \n",
    "                                batch_first=batch_first)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        텐서 크기 변화 위한 문자 설명 \n",
    "         - V: 단어장 크기\n",
    "         - T: 시퀀스 총 길이\n",
    "         - B: 미니배치 크기\n",
    "         - E: 임베딩 크기 = RNN의 입력층 크기 \n",
    "         - D: RNN 은닉층 크기\n",
    "        \"\"\"\n",
    "        # 1. embedding 층을 총과해서 분산 표상 방식으로 단어를 표현\n",
    "        # 크기변화: (B, T) > (B, T, D)\n",
    "        output = self.embedding_layer(x)\n",
    "        # 2. RNN 층\n",
    "        # 크기변화: (B, T, D) > output (B, T, D), hidden (1, B, D)\n",
    "        output, hidden = self.rnn_layer(output)\n",
    "        # 3. 최종 출력층\n",
    "        # 크기변화: (B, T, D) > (B, T, V)\n",
    "        output = self.linear(output)\n",
    "        # 4. 모델 출력: \n",
    "        # 크기변화: (B, T, V) > (B*T, V)\n",
    "        return output.view(-1, output.size(2))\n",
    "        \n",
    "# -----------------------------------------\n",
    "# 데이터 생성\n",
    "x, t = construct_data(sentence, vocab)\n",
    "\n",
    "# 모델 생성을 위한 하이퍼 파라미터 설정\n",
    "vocab_size = len(vocab)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다\n",
    "input_size = 5  # 임베딩된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
    "hidden_size = 20  # RNN의 은닉층 크기\n",
    "\n",
    "# 모델 생성\n",
    "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
    "# 손실함수 정의\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.Adam(params=model.parameters())\n",
    "\n",
    "# 훈련 시작\n",
    "for step in range(151):\n",
    "    # 경사 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 순방향 전파\n",
    "    output = model(x)\n",
    "    # 손실값 계산\n",
    "    loss = loss_function(output, t.view(-1))\n",
    "    # 역방향 전파\n",
    "    loss.backward()\n",
    "    # 매개변수 업데이트\n",
    "    optimizer.step()\n",
    "    # 기록\n",
    "    if step % 30 == 0:\n",
    "        print(\"[{:02d}/151] {:.4f} \".format(step+1, loss))\n",
    "        pred = output.softmax(-1).argmax(-1).tolist()\n",
    "        print(\" \".join([\"We\"] + decode(pred)))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
