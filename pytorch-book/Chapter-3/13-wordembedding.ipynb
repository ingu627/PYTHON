{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 임베딩\n",
    "\n",
    "> 3.2.3 장에 해당하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7396)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.FloatTensor([1, 2, 3, 4])\n",
    "x2 = torch.FloatTensor([1, 4, 2, 1])\n",
    "print(torch.cosine_similarity(x1, x2, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "word1 = torch.FloatTensor([0, 1, 0, 0, 0])\n",
    "word2 = torch.FloatTensor([0, 0, 0, 1, 0])\n",
    "print(torch.cosine_similarity(word1, word2, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  5.,  7.],\n",
      "        [ 2.,  1.,  8.],\n",
      "        [ 6.,  1., 10.]])\n",
      "tensor([[ 1.,  5.,  7.],\n",
      "        [ 2.,  1.,  8.],\n",
      "        [ 6.,  1., 10.]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 코드 3-31\n",
    "\n",
    "tokens = \"We are going to watch Avengers End Game\".split()  # 원래 문장\n",
    "new_sent = \"We are Avengers\".split()  # 새로운 문장\n",
    "vocab = {tkn: i for i, tkn in enumerate(tokens)}  # 단어장 생성\n",
    "\n",
    "## 임베딩 층을 사용하지 않는 임베딩 기법 구현\n",
    "\n",
    "embedding = torch.FloatTensor([[ 1,  5,  7], \n",
    "                               [ 2,  1,  8], \n",
    "                               [ 1,  4,  5], \n",
    "                               [ 4,  1,  1], \n",
    "                               [ 1,  8,  9], \n",
    "                               [ 6,  1, 10], \n",
    "                               [ 3,  2,  2], \n",
    "                               [ 1,  5,  4]])\n",
    "\n",
    "# 새로운 문장 토큰들이 단어장에 해당하는 인덱스, 인덱스는 항상 LongTensor 타입이어야 한다.\n",
    "idxes = torch.LongTensor([vocab[tkn] for tkn in new_sent])\n",
    "print(embedding[idxes, :])\n",
    "\n",
    "## 파이토치에서 사용하는 방법\n",
    "import torch.nn as nn\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab), \n",
    "                               embedding_dim=3, \n",
    "                               _weight=embedding)\n",
    "# 해당하는 index 조회\n",
    "print(embedding_layer(idxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "\n",
    "```\n",
    "$ conda activate torchenv\n",
    "(torchenv)$ conda install -c conda-forge gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30382, 100)\n"
     ]
    }
   ],
   "source": [
    "# 코드 3-32\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# paramiko 설치하라는 오류시: 터미널에서 pip install paramiko 실행해준다.\n",
    "\n",
    "# 품사정보를 \"(단어)/(품사정보)\" 처럼 함께 저장하기위해 tokenizer 함수를 정의 한다.\n",
    "mecab = Mecab()\n",
    "tokenizer = lambda x: [\"/\".join((tkn.lower(), pos.lower())) for (tkn, pos) in mecab.pos(x)]\n",
    "\n",
    "with open(\"./data/nsmc/ratings.txt\") as file:\n",
    "    # 행단위로 데이터를 분리한다. 첫 행은 header라 제외한다.\n",
    "    raw_data = file.read().splitlines()[1:]\n",
    "    # 텍스트 데이터만 사용한다\n",
    "    data = [line.split(\"\\t\")[1] for line in raw_data]\n",
    "    # 토큰화를 진행한다.\n",
    "    data = [tokenizer(sent) for sent in data]\n",
    "\n",
    "model = Word2Vec(sentences=data, size=100, window=5, min_count=3, sg=1)\n",
    "# 훈련 완료후 불필요한 메모리 제거\n",
    "model.init_sims(replace=True)\n",
    "# 단어 임베딩 행렬의 크기\n",
    "print(model.wv.vectors.shape)\n",
    "# 모델 저장: 파일의 첫번째 줄에는 임베딩 행렬의 크기가 적혀있다. \n",
    "model.wv.save_word2vec_format(\"./word2vec.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(여배우, 배우) = 0.71\n"
     ]
    }
   ],
   "source": [
    "# 코드 3-33\n",
    "\n",
    "# 1. 단어\"여배우\" 와 \"배우\"의 유사도\n",
    "sim1 = model.wv.similarity(*tokenizer(\"여배우 배우\"))\n",
    "print(\"similarity(여배우, 배우) = {:.2f}\".format(sim1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "줄거리/nng = 0.79\n",
      "시나리오/nng = 0.75\n",
      "전개/nng = 0.73\n",
      "내용/nng = 0.72\n",
      "cg/sl = 0.69\n"
     ]
    }
   ],
   "source": [
    "# 코드 3-33\n",
    "\n",
    "# 2. \"스토리\"와 가장 유사한 단어 Top 5\n",
    "sim2 = model.wv.most_similar(tokenizer(\"스토리\"), topn=5)\n",
    "for t, s in sim2:\n",
    "    print(\"{} = {:.2f}\".format(t, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('연기자/nng', 0.7952105402946472)]\n"
     ]
    }
   ],
   "source": [
    "# 코드 3-33\n",
    "\n",
    "# 3 벡터 연산 \"남자배우\" - \"남자\" = \"연기자\"\n",
    "sim3 = model.wv.most_similar(positive=tokenizer(\"남자배우\"), \n",
    "                             negative=tokenizer(\"남자\"), \n",
    "                             topn=1)\n",
    "print(sim3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Embedding Vector 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30369, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0727, -0.1427, -0.0380,  0.2419, -0.1037, -0.0136, -0.1420, -0.0085,\n",
       "          0.0271,  0.1225, -0.1727, -0.0869,  0.0120,  0.0461, -0.1488, -0.2509,\n",
       "          0.0759, -0.0287, -0.0843,  0.0947, -0.0327, -0.0707, -0.0113,  0.0780,\n",
       "         -0.0384,  0.0003, -0.0969, -0.0776, -0.0129,  0.0865, -0.1255, -0.0956,\n",
       "          0.2097,  0.0579,  0.1535, -0.0118,  0.0315, -0.1047,  0.1097,  0.0450,\n",
       "          0.1604, -0.0862, -0.0209,  0.0061,  0.0504,  0.1044, -0.0141,  0.0858,\n",
       "         -0.0034,  0.1797,  0.0646,  0.0590,  0.0500, -0.0884,  0.0130, -0.0702,\n",
       "         -0.1109, -0.0096,  0.1315,  0.0465,  0.0948, -0.0620,  0.0690,  0.0086,\n",
       "         -0.0218,  0.0484, -0.1597, -0.2013,  0.1190, -0.1094,  0.0949, -0.1127,\n",
       "          0.0617, -0.1034,  0.1461, -0.1071, -0.0323, -0.0345, -0.0851, -0.0720,\n",
       "          0.1612, -0.0669,  0.0030, -0.1015, -0.0137, -0.0183, -0.0536,  0.0441,\n",
       "         -0.0224, -0.0768,  0.1640, -0.1327, -0.2432,  0.1285,  0.0319,  0.0384,\n",
       "          0.1391,  0.1593, -0.0353,  0.0440]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코드 3-34\n",
    "\n",
    "# esc + 0, 0 으로 노트북 재시작\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator\n",
    "from torchtext.vocab import Vectors\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "tokenizer = lambda x:  [\"/\".join((tkn.lower(), pos.lower())) \\\n",
    "                        for (tkn, pos) in mecab.pos(x)]\n",
    "\n",
    "# 필드 정의\n",
    "TEXT = Field(sequential=True,\n",
    "             use_vocab=True,\n",
    "             tokenize=tokenizer,  \n",
    "             lower=True, \n",
    "             batch_first=True)  \n",
    "LABEL = Field(sequential=False,  \n",
    "              use_vocab=False,   \n",
    "              preprocessing = lambda x: int(x),  \n",
    "              batch_first=True, \n",
    "              is_target=True)\n",
    "\n",
    "# 각 댓글에 해당하는 id, 사용하지 않지만 기본필드로 정의 해준다.\n",
    "ID = Field(sequential=False,  \n",
    "           use_vocab=False,   \n",
    "           is_target=False)\n",
    "\n",
    "dataset = TabularDataset(path='./data/nsmc/ratings.txt', \n",
    "                         format='tsv', \n",
    "                         fields=[('id', ID), ('text', TEXT), ('label', LABEL)],\n",
    "                         skip_header=True)\n",
    "\n",
    "# 훈련된 임베딩 행렬을 사용하기 위해 Vectors 객체를 만들어서 TEXT 필드로 전달한다.\n",
    "# 파일 내에 첫번째 행은 임베딩 행렬의 크기기 때문에 무시하게 된다.\n",
    "vectors = Vectors(name=\"word2vec.pt\")\n",
    "# 생성한 vector 객체를 단어장을 만드면서 필드로 전달한다\n",
    "TEXT.build_vocab(dataset, min_freq=3, vectors=vectors)\n",
    "# 필드에 <unk> 과 <pad> 토큰을 포함한 임베딩 행렬로 구성되어 있다.\n",
    "print(TEXT.vocab.vectors.size())\n",
    "\n",
    "# 임베딩 층을 필드에 내장된 임베딩 행렬로 초기화 시킨다. \n",
    "# freeze 를 비활성해야 계속 학습이 가능하다.\n",
    "embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)\n",
    "embedding(torch.LongTensor([2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
