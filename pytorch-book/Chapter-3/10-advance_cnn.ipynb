{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study\n",
    "\n",
    "> 3.1.5 장에 해당하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "# 2.2.8 장에서 정의한 훈련 및 테스트 함수를 불러온다.\n",
    "from train_utils import train, test, main\n",
    "\n",
    "# 데이터 셋 불러오기\n",
    "train_dataset = datasets.CIFAR10('./data', train=True, download=True, \n",
    "                                 transform=transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10('./data', train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "# 환경 변수 설정\n",
    "BATCH = 128  # 미니배치크기\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # 디바이스\n",
    "STEP = 30  # 총 반복스텝\n",
    "PRINT_STEP = 180  # 경과 print 기간\n",
    "\n",
    "# 데이터로더 선언\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 3-13\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, n_in=32, output_size=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \"\"\"\n",
    "        config:\n",
    "        - kernel: c_out, k, s, p\n",
    "        - pool: maxpool, (kernel_size=3, stride=2, padding=0)\n",
    "        \"\"\"\n",
    "        self.n_in = n_in\n",
    "        config = [(96, 11, 4, 0), \"M\", (256, 5, 1, 2), \"M\", \n",
    "                  (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), \"M\"]\n",
    "        \n",
    "        self.convs = self._make_layers(config)\n",
    "        self.fc = nn.Sequential(\n",
    "                    nn.Linear(self._get_fc_input(config), 4096),\n",
    "                    nn.Linear(4096, 4096),\n",
    "                    nn.Linear(4096, output_size),\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convs(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, config):\n",
    "        layers = []\n",
    "        ch_in = 3\n",
    "        for x in config:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=3, stride=2)]\n",
    "            else:\n",
    "                ch_out, k, s, p = x\n",
    "                layers += [nn.Conv2d(ch_in, ch_out, \n",
    "                                     kernel_size=k, stride=s, padding=p),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                ch_in = ch_out\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _get_fc_input(self, config):\n",
    "        n_in = self.n_in\n",
    "        for x in config:\n",
    "            if x == \"M\":\n",
    "                pool_k, pool_s = 3, 2\n",
    "                pool_n_out = (n_in - pool_k)/pool_s + 1\n",
    "                n_in = pool_n_out\n",
    "            else:\n",
    "                ch_out, k, s, p = x\n",
    "            \n",
    "                conv_n_out = (n_in + 2*p - k)/s + 1\n",
    "                n_in = conv_n_out\n",
    "            \n",
    "        fc_input = int(ch_out*n_in*n_in)\n",
    "        return fc_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 3-14\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, n_in=32, output_size=10):\n",
    "        super(VGG, self).__init__()\n",
    "        \"\"\"reference: https://github.com/kuangliu/pytorch-cifar\"\"\"\n",
    "        self.n_in = n_in\n",
    "        config = {\n",
    "            'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "            'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "            'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "            'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "        }\n",
    "        \n",
    "        self.convs = self._make_layers(config[vgg_name])\n",
    "        self.fc = nn.Linear(self._get_fc_input(config[vgg_name]), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convs(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, config):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in config:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _get_fc_input(self, config):\n",
    "        n_in = self.n_in\n",
    "        for x in config:\n",
    "            if x == \"M\":\n",
    "                pool_k, pool_s = 2, 2\n",
    "                pool_n_out = (n_in - pool_k)/pool_s + 1\n",
    "                n_in = pool_n_out\n",
    "            else:\n",
    "                ch_out = x\n",
    "                k, s, p = 3, 1, 1\n",
    "                conv_n_out = (n_in + 2*p - k)/s + 1\n",
    "                n_in = conv_n_out\n",
    "            \n",
    "        fc_input = int(ch_out*n_in*n_in)\n",
    "        return fc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 1 (00.00%)  \tLoss: 2.5153\n",
      "Train Step: 1 (46.08%)  \tLoss: 1.3414\n",
      "Train Step: 1 (92.16%)  \tLoss: 1.0604\n",
      "Test set: Average loss: 1.3962, Accuracy: 5255/10000 (52.55%)\n",
      "\n",
      "Train Step: 2 (00.00%)  \tLoss: 0.9636\n",
      "Train Step: 2 (46.08%)  \tLoss: 1.0470\n",
      "Train Step: 2 (92.16%)  \tLoss: 0.8943\n",
      "Test set: Average loss: 0.9317, Accuracy: 6677/10000 (66.77%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 3 (00.00%)  \tLoss: 0.7937\n",
      "Train Step: 3 (46.08%)  \tLoss: 0.7134\n",
      "Train Step: 3 (92.16%)  \tLoss: 0.5986\n",
      "Test set: Average loss: 0.7099, Accuracy: 7616/10000 (76.16%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 4 (00.00%)  \tLoss: 0.5773\n",
      "Train Step: 4 (46.08%)  \tLoss: 0.4496\n",
      "Train Step: 4 (92.16%)  \tLoss: 0.4923\n",
      "Test set: Average loss: 0.7049, Accuracy: 7674/10000 (76.74%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 5 (00.00%)  \tLoss: 0.3897\n",
      "Train Step: 5 (46.08%)  \tLoss: 0.4485\n",
      "Train Step: 5 (92.16%)  \tLoss: 0.3601\n",
      "Test set: Average loss: 0.6414, Accuracy: 7919/10000 (79.19%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 6 (00.00%)  \tLoss: 0.3157\n",
      "Train Step: 6 (46.08%)  \tLoss: 0.3221\n",
      "Train Step: 6 (92.16%)  \tLoss: 0.3421\n",
      "Test set: Average loss: 0.6211, Accuracy: 8076/10000 (80.76%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 7 (00.00%)  \tLoss: 0.2762\n",
      "Train Step: 7 (46.08%)  \tLoss: 0.2630\n",
      "Train Step: 7 (92.16%)  \tLoss: 0.3182\n",
      "Test set: Average loss: 0.7441, Accuracy: 7751/10000 (77.51%)\n",
      "\n",
      "Train Step: 8 (00.00%)  \tLoss: 0.1759\n",
      "Train Step: 8 (46.08%)  \tLoss: 0.1877\n",
      "Train Step: 8 (92.16%)  \tLoss: 0.1947\n",
      "Test set: Average loss: 0.6162, Accuracy: 8095/10000 (80.95%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 9 (00.00%)  \tLoss: 0.1004\n",
      "Train Step: 9 (46.08%)  \tLoss: 0.1884\n",
      "Train Step: 9 (92.16%)  \tLoss: 0.1819\n",
      "Test set: Average loss: 0.6818, Accuracy: 8060/10000 (80.60%)\n",
      "\n",
      "Train Step: 10 (00.00%)  \tLoss: 0.0806\n",
      "Train Step: 10 (46.08%)  \tLoss: 0.0970\n",
      "Train Step: 10 (92.16%)  \tLoss: 0.1713\n",
      "Test set: Average loss: 0.6704, Accuracy: 8197/10000 (81.97%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 11 (00.00%)  \tLoss: 0.0990\n",
      "Train Step: 11 (46.08%)  \tLoss: 0.1351\n",
      "Train Step: 11 (92.16%)  \tLoss: 0.1338\n",
      "Test set: Average loss: 0.6814, Accuracy: 8271/10000 (82.71%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 12 (00.00%)  \tLoss: 0.0999\n",
      "Train Step: 12 (46.08%)  \tLoss: 0.0977\n",
      "Train Step: 12 (92.16%)  \tLoss: 0.0921\n",
      "Test set: Average loss: 0.6344, Accuracy: 8311/10000 (83.11%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 13 (00.00%)  \tLoss: 0.0739\n",
      "Train Step: 13 (46.08%)  \tLoss: 0.1630\n",
      "Train Step: 13 (92.16%)  \tLoss: 0.2030\n",
      "Test set: Average loss: 0.6366, Accuracy: 8419/10000 (84.19%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 14 (00.00%)  \tLoss: 0.0985\n",
      "Train Step: 14 (46.08%)  \tLoss: 0.1240\n",
      "Train Step: 14 (92.16%)  \tLoss: 0.0436\n",
      "Test set: Average loss: 0.7387, Accuracy: 8288/10000 (82.88%)\n",
      "\n",
      "Train Step: 15 (00.00%)  \tLoss: 0.0679\n",
      "Train Step: 15 (46.08%)  \tLoss: 0.0813\n",
      "Train Step: 15 (92.16%)  \tLoss: 0.0272\n",
      "Test set: Average loss: 0.7531, Accuracy: 8239/10000 (82.39%)\n",
      "\n",
      "Train Step: 16 (00.00%)  \tLoss: 0.1156\n",
      "Train Step: 16 (46.08%)  \tLoss: 0.0425\n",
      "Train Step: 16 (92.16%)  \tLoss: 0.0556\n",
      "Test set: Average loss: 0.7818, Accuracy: 8282/10000 (82.82%)\n",
      "\n",
      "Train Step: 17 (00.00%)  \tLoss: 0.0424\n",
      "Train Step: 17 (46.08%)  \tLoss: 0.0322\n",
      "Train Step: 17 (92.16%)  \tLoss: 0.0303\n",
      "Test set: Average loss: 0.7920, Accuracy: 8208/10000 (82.08%)\n",
      "\n",
      "Train Step: 18 (00.00%)  \tLoss: 0.1928\n",
      "Train Step: 18 (46.08%)  \tLoss: 0.0574\n",
      "Train Step: 18 (92.16%)  \tLoss: 0.1234\n",
      "Test set: Average loss: 0.6908, Accuracy: 8438/10000 (84.38%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 19 (00.00%)  \tLoss: 0.0398\n",
      "Train Step: 19 (46.08%)  \tLoss: 0.0270\n",
      "Train Step: 19 (92.16%)  \tLoss: 0.0293\n",
      "Test set: Average loss: 0.6794, Accuracy: 8482/10000 (84.82%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 20 (00.00%)  \tLoss: 0.0128\n",
      "Train Step: 20 (46.08%)  \tLoss: 0.0858\n",
      "Train Step: 20 (92.16%)  \tLoss: 0.0454\n",
      "Test set: Average loss: 0.7007, Accuracy: 8392/10000 (83.92%)\n",
      "\n",
      "Train Step: 21 (00.00%)  \tLoss: 0.0781\n",
      "Train Step: 21 (46.08%)  \tLoss: 0.0483\n",
      "Train Step: 21 (92.16%)  \tLoss: 0.0698\n",
      "Test set: Average loss: 0.6745, Accuracy: 8529/10000 (85.29%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 22 (00.00%)  \tLoss: 0.0188\n",
      "Train Step: 22 (46.08%)  \tLoss: 0.0280\n",
      "Train Step: 22 (92.16%)  \tLoss: 0.0113\n",
      "Test set: Average loss: 0.6887, Accuracy: 8586/10000 (85.86%)\n",
      "discard previous state, best model state saved!\n",
      "\n",
      "Train Step: 23 (00.00%)  \tLoss: 0.0694\n",
      "Train Step: 23 (46.08%)  \tLoss: 0.0178\n",
      "Train Step: 23 (92.16%)  \tLoss: 0.0364\n",
      "Test set: Average loss: 0.6866, Accuracy: 8497/10000 (84.97%)\n",
      "\n",
      "Train Step: 24 (00.00%)  \tLoss: 0.0052\n",
      "Train Step: 24 (46.08%)  \tLoss: 0.0370\n",
      "Train Step: 24 (92.16%)  \tLoss: 0.0296\n",
      "Test set: Average loss: 0.8724, Accuracy: 8265/10000 (82.65%)\n",
      "\n",
      "Train Step: 25 (00.00%)  \tLoss: 0.0389\n",
      "Train Step: 25 (46.08%)  \tLoss: 0.0458\n",
      "Train Step: 25 (92.16%)  \tLoss: 0.1147\n",
      "Test set: Average loss: 0.7109, Accuracy: 8483/10000 (84.83%)\n",
      "\n",
      "Train Step: 26 (00.00%)  \tLoss: 0.0311\n",
      "Train Step: 26 (46.08%)  \tLoss: 0.0037\n",
      "Train Step: 26 (92.16%)  \tLoss: 0.0779\n",
      "Test set: Average loss: 0.7301, Accuracy: 8449/10000 (84.49%)\n",
      "\n",
      "Train Step: 27 (00.00%)  \tLoss: 0.0688\n",
      "Train Step: 27 (46.08%)  \tLoss: 0.0248\n",
      "Train Step: 27 (92.16%)  \tLoss: 0.0294\n",
      "Test set: Average loss: 0.7315, Accuracy: 8492/10000 (84.92%)\n",
      "\n",
      "Train Step: 28 (00.00%)  \tLoss: 0.0124\n",
      "Train Step: 28 (46.08%)  \tLoss: 0.0102\n",
      "Train Step: 28 (92.16%)  \tLoss: 0.0264\n",
      "Test set: Average loss: 0.7695, Accuracy: 8431/10000 (84.31%)\n",
      "\n",
      "Train Step: 29 (00.00%)  \tLoss: 0.0031\n",
      "Train Step: 29 (46.08%)  \tLoss: 0.0211\n",
      "Train Step: 29 (92.16%)  \tLoss: 0.0443\n",
      "Test set: Average loss: 0.7433, Accuracy: 8536/10000 (85.36%)\n",
      "\n",
      "Train Step: 30 (00.00%)  \tLoss: 0.0370\n",
      "Train Step: 30 (46.08%)  \tLoss: 0.0310\n",
      "Train Step: 30 (92.16%)  \tLoss: 0.0062\n",
      "Test set: Average loss: 0.7347, Accuracy: 8545/10000 (85.45%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 선언\n",
    "torch.manual_seed(70)\n",
    "model = VGG(vgg_name=\"VGG16\", n_in=32, output_size=10).to(DEVICE)\n",
    "# 손실함수 선언\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# 옵티마이저 선언\n",
    "optimizer = optim.Adam(model.parameters()) \n",
    "\n",
    "main(model=model, \n",
    "     train_loader=train_loader, \n",
    "     test_loader=test_loader, \n",
    "     loss_func=loss_function, \n",
    "     optimizer=optimizer, \n",
    "     n_step=STEP,\n",
    "     device=DEVICE,\n",
    "     save_path=\"cifar10_vgg16.pt\", \n",
    "     print_step=PRINT_STEP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
