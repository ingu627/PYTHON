{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손실함수\n",
    "\n",
    "> 2.2.5 장에 해당하는 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 출력값을 확률로 표현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42035338282585144\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-7\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(70)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Network, self).__init__()\n",
    "        # 층을 구성\n",
    "        # input layer > hidden layer \n",
    "        self.linear_ih = nn.Linear(in_features=input_size, \n",
    "                                   out_features=hidden_size)\n",
    "        # hidden layer > output layer\n",
    "        self.linear_ho = nn.Linear(in_features=hidden_size, \n",
    "                                   out_features=output_size)\n",
    "        # activation layer\n",
    "        self.activation_layer = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.linear_ih(x)\n",
    "        a1 = self.activation_layer(z1)\n",
    "        z2 = self.linear_ho(a1)\n",
    "        y = self.activation_layer(z2)\n",
    "        return y\n",
    "\n",
    "# 입력텐서 생성    \n",
    "x = torch.Tensor([[0, 1]])\n",
    "\n",
    "# 커스텀 모듈 호출\n",
    "net = Network(input_size=2, hidden_size=2, output_size=1)\n",
    "y = net(x)\n",
    "print(y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률론적 접근\n",
    "\n",
    "### 엔트로피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.673\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-8\n",
    "\n",
    "P = torch.Tensor([0.4, 0.6])\n",
    "Q = torch.Tensor([0.0, 1.0])\n",
    "\n",
    "def self_information(x):\n",
    "    return -torch.log(x)\n",
    "\n",
    "def entropy(x):\n",
    "    # log(0) = NaN 값이 나옴으로 아주 작은 수를 더해서 이를 방지한다.\n",
    "    e = 1e-30\n",
    "    return torch.sum((x+e)*self_information(x+e))\n",
    "\n",
    "\n",
    "# 앞면이 40%, 뒷면이 60% 확률의 동전\n",
    "print(entropy(P).numpy().round(4))\n",
    "# 뒷면만 100% 나오는 확실한 동전\n",
    "print(entropy(Q).numpy().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0204)\n",
      "tensor(33.8456)\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-9\n",
    "\n",
    "def KL_divergence(q, p):\n",
    "    \"\"\"\n",
    "    q: predict prob\n",
    "    p: target prob\n",
    "    \"\"\"\n",
    "    # log(0) = NaN 값이 나옴으로 아주 작은 수(e)를 더해서 이를 방지한다.\n",
    "    e = 1e-30\n",
    "    return torch.sum((p+e)*torch.log(p+e) - (p+e)*torch.log(q+e))\n",
    "\n",
    "U = torch.Tensor([0.5, 0.5])\n",
    "# 확률분포 P 와 U 의 KL-divergence\n",
    "print(KL_divergence(P, U))\n",
    "# 확률분포 Q 와 U 의 KL-divergence\n",
    "print(KL_divergence(Q, U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0204)\n",
      "tensor(33.8456)\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-10\n",
    "\n",
    "loss_function = nn.KLDivLoss(reduction=\"sum\")\n",
    "e = 1e-30\n",
    "## pytorch: y*(log(y) - x) = log(y)*y - x*y\n",
    "# 확률분포 P 와 U 의 KL-divergence\n",
    "print(loss_function(torch.log(P+e), U+e))\n",
    "# 확률분포 Q 와 U 의 KL-divergence\n",
    "print(loss_function(torch.log(Q+e), U+e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.]])\n",
      "tensor([[0.5796, 0.4204]], grad_fn=<CatBackward>)\n",
      "tensor(0.8667, grad_fn=<KlDivBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-11\n",
    "\n",
    "torch.manual_seed(70)\n",
    "\n",
    "# 입력과 타겟텐서 생성\n",
    "x = torch.Tensor([[0, 1]])\n",
    "t = torch.Tensor([1])\n",
    "# 이전에 만든 XOR 네트워크 호출\n",
    "net = Network(input_size=2, hidden_size=2, output_size=1)\n",
    "y = net(x)\n",
    "# 타겟값의 원-핫 인코딩 생성\n",
    "one_hot = torch.eye(2)\n",
    "prob_t = one_hot.index_select(dim=0, index=t.long())\n",
    "# 예측값도 확률 분포로 만들어준다. y=1이 될 확률분포이다.\n",
    "prob_y = torch.cat([1-y, y], dim=1)\n",
    "# t=1 에 해당하는 t의 확률분포와 예측값 y의 확률분포\n",
    "print(prob_t)\n",
    "print(prob_y)\n",
    "# KL-divergence 구하기\n",
    "loss_function = nn.KLDivLoss(reduction=\"sum\")\n",
    "print(loss_function(torch.log(prob_y), prob_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8667, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-12\n",
    "\n",
    "torch.manual_seed(70)\n",
    "\n",
    "# 입력과 타겟텐서 생성\n",
    "x = torch.Tensor([[0, 1]])\n",
    "t = torch.Tensor([1])\n",
    "# 이전에 만든 XOR 네트워크 호출\n",
    "net = Network(input_size=2, hidden_size=2, output_size=1)\n",
    "y = net(x)\n",
    "# BCE 구하기\n",
    "loss_function = nn.BCELoss(reduction=\"sum\")\n",
    "# 예측 값: y=1 확률 / 타겟 값: t=1 의 확률은 1\n",
    "print(loss_function(y.squeeze(1), t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0986, 0.0914, 0.0897, 0.0831, 0.1241, 0.0729, 0.1315, 0.1243, 0.0711,\n",
      "         0.1133]])\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-13\n",
    "\n",
    "torch.manual_seed(70)\n",
    "# 선형결합값: 임의의 크기가 (1,10)인 벡터 텐서 생성\n",
    "z = torch.rand(1, 10)\n",
    "# Softmax \n",
    "y = torch.softmax(z, dim=1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3930)\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-14\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "# 타겟값 또한 torch.LongTensor로 만들어야 한다.\n",
    "print(loss_function(z, t.long()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
