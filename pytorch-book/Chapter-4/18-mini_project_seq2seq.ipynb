{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: 한영 번역기 만들기\n",
    "\n",
    "> 4.5 장에 해당하는 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패키지 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 4-4\n",
    "\n",
    "from encdec import EncoderDecoder\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import Field, TabularDataset, Iterator\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 알고리즘으로 토큰 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 4-5\n",
    "\n",
    "# 영어 토큰화 함수로 SpaCy 사용\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "# 한글 토큰화 함수로 MeCab 사용\n",
    "mecab = Mecab()\n",
    "\n",
    "# 각 언어에 해당하는 tokenizer 함수를 정의 한다.\n",
    "tokenizer_ko = lambda x: [tkn.lower() for tkn in Mecab().morphs(x)]\n",
    "tokenizer_en = lambda x: [tkn.text.lower() for tkn in spacy_en.tokenizer(x)]\n",
    "\n",
    "with open(\"./data/eng-kor.tsv\") as file:\n",
    "    # 데이터를 불러와서 tab 을 기준으로 분리한다.\n",
    "    raw_data = file.read().splitlines()\n",
    "    eng, kor = zip(*[line.split(\"\\t\") for line in raw_data])\n",
    "    # 토큰화를 진행한다.\n",
    "    eng = [tokenizer_en(sent) for sent in eng]\n",
    "    kor = [tokenizer_ko(sent) for sent in kor]\n",
    "    \n",
    "\n",
    "# 영어 Word2Vec 훈련\n",
    "model_eng = Word2Vec(sentences=eng, size=100, window=3, min_count=1, sg=1)\n",
    "model_eng.init_sims(replace=True)  # 훈련 완료후 불필요한 메모리 제거\n",
    "model_eng.wv.save_word2vec_format(\"./word2vec_eng.pt\")  # 모델 저장    \n",
    "\n",
    "# 한글 Word2Vec 훈련\n",
    "model_kor = Word2Vec(sentences=kor, size=100, window=3, min_count=1, sg=1)\n",
    "model_kor.init_sims(replace=True)  # 훈련 완료후 불필요한 메모리 제거\n",
    "model_kor.wv.save_word2vec_format(\"./word2vec_kor.pt\")  # 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필드정의 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocab Size: 1173\n",
      "Target Vocab Size: 1549\n"
     ]
    }
   ],
   "source": [
    "# 코드 4-6\n",
    "\n",
    "# 필드 정의\n",
    "SRC = Field(sequential=True,\n",
    "            use_vocab=True,\n",
    "            tokenize=tokenizer_en,  \n",
    "            lower=True, \n",
    "            batch_first=True)  \n",
    "TRG = Field(sequential=True,  \n",
    "            use_vocab=True, \n",
    "            tokenize=tokenizer_ko,\n",
    "            init_token=\"<s>\",\n",
    "            eos_token=\"</s>\",\n",
    "            batch_first=True)\n",
    "\n",
    "\n",
    "# TabularDataset를 사용해 훈련 세트를 만든다.\n",
    "train_data = TabularDataset(path=\"./data/eng-kor.tsv\", format='tsv',\n",
    "                            fields=[('eng', SRC), ('kor', TRG)])\n",
    "\n",
    "# 학습한 Word2Vector를 Vectors 객체를 통해 불러온다.\n",
    "vectors_kor = Vectors(name=\"./word2vec_kor.pt\")\n",
    "vectors_eng = Vectors(name=\"./word2vec_eng.pt\")\n",
    "\n",
    "# 단어장 생성\n",
    "SRC.build_vocab(train_data.eng, min_freq=1, vectors=vectors_eng)\n",
    "TRG.build_vocab(train_data.kor, min_freq=1, vectors=vectors_kor)\n",
    "# 데이터 개수 및 단어장 크기 확인\n",
    "print(\"Source Vocab Size: {}\".format(len(SRC.vocab)))\n",
    "print(\"Target Vocab Size: {}\".format(len(TRG.vocab)))\n",
    "\n",
    "# 환경 변수 설정\n",
    "BATCH = 32  # 미니배치크기\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # 디바이스\n",
    "STEP = 200  # 총 반복스텝\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = Iterator(dataset=train_data, batch_size=BATCH, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'you', 'eaten', 'dinner', 'yet', '?']\n",
      "['아직', '저녁', '을', '안', '드셨', '나요', '?']\n"
     ]
    }
   ],
   "source": [
    "idx = torch.randint(0, len(train_data), (1,))\n",
    "\n",
    "print(train_data.examples[idx].eng)\n",
    "print(train_data.examples[idx].kor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 4-7\n",
    "\n",
    "# 모델 선언에 필요한 인자 설정\n",
    "enc_vocab_size = len(SRC.vocab)\n",
    "dec_vocab_size = len(TRG.vocab)  \n",
    "embed_size = 100  # E: 임베딩 크기\n",
    "hidden_size = 600  # D: 은닉층 크기\n",
    "num_layers = 2  # RNN 층의 개수\n",
    "batch_first = True  # RNN 입력의 첫번째 차원이 미니배치 크기인 경우 활성화\n",
    "bidirec = True  # Encoder의 양방향 순환 신경망 사용 여부\n",
    "\n",
    "# 모델 선언\n",
    "model = EncoderDecoder(\n",
    "    enc_vocab_size=enc_vocab_size, \n",
    "    dec_vocab_size=dec_vocab_size, \n",
    "    embed_size=embed_size, \n",
    "    hidden_size=hidden_size, \n",
    "    num_layers=num_layers, \n",
    "    batch_first=True, \n",
    "    bidirec=bidirec, \n",
    "    sos_idx=TRG.vocab.stoi.get(\"<s>\")).to(DEVICE)\n",
    "\n",
    "# 각 encoder 와 decoder 에서 embedding 층을 미리 학습된 벡터로 교체해준다.\n",
    "model.encoder.embedding = nn.Embedding.from_pretrained(SRC.vocab.vectors, freeze=False).to(DEVICE)\n",
    "model.decoder.embedding = nn.Embedding.from_pretrained(TRG.vocab.vectors, freeze=False).to(DEVICE)\n",
    "\n",
    "# 손실함수와 옵티마이저 선언\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# StepLR 스케쥴러는 Learing Rate 를 조절 해주는 함수\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [1] Loss: 148.8715\n",
      "Step [11] Loss: 89.9731\n",
      "Step [21] Loss: 43.4289\n",
      "Model Saved at step: 21, best_loss: 43.4289\n",
      "Step [31] Loss: 19.7057\n",
      "Model Saved at step: 31, best_loss: 19.7057\n",
      "Step [41] Loss: 25.5950\n",
      "Model Saved at step: 36, best_loss: 14.8238\n",
      "Step [51] Loss: 5.8047\n",
      "Model Saved at step: 51, best_loss: 5.8047\n",
      "Step [61] Loss: 3.1677\n",
      "Model Saved at step: 61, best_loss: 3.1677\n",
      "Step [71] Loss: 4.2973\n",
      "Model Saved at step: 62, best_loss: 3.1487\n",
      "Step [81] Loss: 4.6207\n",
      "Model Saved at step: 62, best_loss: 3.1487\n",
      "Step [91] Loss: 4.0227\n",
      "Model Saved at step: 62, best_loss: 3.1487\n",
      "Step [101] Loss: 2.9694\n",
      "Model Saved at step: 101, best_loss: 2.9694\n",
      "Step [111] Loss: 2.3598\n",
      "Model Saved at step: 111, best_loss: 2.3598\n",
      "Step [121] Loss: 2.2942\n",
      "Model Saved at step: 121, best_loss: 2.2942\n",
      "Step [131] Loss: 2.3475\n",
      "Model Saved at step: 126, best_loss: 2.2230\n",
      "Step [141] Loss: 2.5317\n",
      "Model Saved at step: 126, best_loss: 2.2230\n",
      "Step [151] Loss: 2.2445\n",
      "Model Saved at step: 126, best_loss: 2.2230\n",
      "Step [161] Loss: 2.1155\n",
      "Model Saved at step: 156, best_loss: 2.0268\n",
      "Step [171] Loss: 2.2585\n",
      "Model Saved at step: 156, best_loss: 2.0268\n",
      "Step [181] Loss: 2.3781\n",
      "Model Saved at step: 175, best_loss: 2.0058\n",
      "Step [191] Loss: 2.0790\n",
      "Model Saved at step: 175, best_loss: 2.0058\n"
     ]
    }
   ],
   "source": [
    "# 코드 4-8\n",
    "\n",
    "best_loss = 50\n",
    "best_step = None \n",
    "for step in range(STEP):\n",
    "    scheduler.step()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        src_data, trg_data = batch.eng, batch.kor\n",
    "        # 경사 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 순방향전파\n",
    "        scores = model(src_data, maxlen=trg_data.size(1))\n",
    "        # 손실값 계산\n",
    "        train_loss = loss_function(scores.view(-1, scores.size(-1)), \n",
    "                                   trg_data[:, 1:].contiguous().view(-1))\n",
    "        \n",
    "        # 역방향 전파\n",
    "        train_loss.backward()\n",
    "        # 매개변수 업데이트\n",
    "        optimizer.step()\n",
    "        # 기록용\n",
    "        total_loss += train_loss.item()\n",
    "    \n",
    "    # 모델 저장 여부 판단\n",
    "    if total_loss <= best_loss:\n",
    "        torch.save(model.state_dict(), \"./encdec.pt\")\n",
    "        best_loss = total_loss\n",
    "        best_step = step\n",
    "        \n",
    "    # 중간 과정 print    \n",
    "    if step % 10 == 0:\n",
    "        print(\"Step [{}] Loss: {:.4f}\".format(step+1, total_loss))\n",
    "        if best_step is not None:\n",
    "            print(\"Model Saved at step: {}, best_loss: {:.4f}\".format(best_step+1, best_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm shocked.\n",
      "충격 이 야 .\n"
     ]
    }
   ],
   "source": [
    "# 코드 4-9\n",
    "\n",
    "model.eval()\n",
    "test_sent = \"I'm shocked.\".lower()\n",
    "# 테스트할 문장을 텐서로 변환한다.\n",
    "src_tensor = SRC.process([tokenizer_en(test_sent)]).to(DEVICE)\n",
    "# Encoder 로 소스 문장을 압축한다.\n",
    "enc_outputs = model.encoder(src_tensor)\n",
    "# Decoder 로 타겟 문장을 예측한다.\n",
    "scores = model.decoder(enc_outputs, maxlen=50, eos_idx=TRG.vocab.stoi[\"</s>\"])\n",
    "pred = scores.argmax(-1).squeeze()[:-1].cpu().tolist()\n",
    "decode = lambda li: [TRG.vocab.itos[idx] for idx in li] \n",
    "# 예측\n",
    "print(test_sent)\n",
    "print(\" \".join(decode(pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
